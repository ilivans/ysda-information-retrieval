Human Memory Search as Initial-Visit Emitting Random Walk

Kwang-Sung Jun, Xiaojin Zhu, Timothy Rogers Wisconsin Institute for Discovery, Department of Computer Sciences, Department of Psychology
University of Wisconsin-Madison
kjun@discovery.wisc.edu, jerryzhu@cs.wisc.edu, ttrogers@wisc.edu

Zhuoran Yang Department of Mathematical Sciences
Tsinghua University yzr11@mails.tsinghua.edu.cn

Ming Yuan Department of Statistics University of Wisconsin-Madison myuan@stat.wisc.edu

Abstract
Imagine a random walk that outputs a state only when visiting it for the first time. The observed output is therefore a repeat-censored version of the underlying walk, and consists of a permutation of the states or a prefix of it. We call this model initial-visit emitting random walk (INVITE). Prior work has shown that the random walks with such a repeat-censoring mechanism explain well human behavior in memory search tasks, which is of great interest in both the study of human cognition and various clinical applications. However, parameter estimation in INVITE is challenging, because naive likelihood computation by marginalizing over infinitely many hidden random walk trajectories is intractable. In this paper, we propose the first efficient maximum likelihood estimate (MLE) for INVITE by decomposing the censored output into a series of absorbing random walks. We also prove theoretical properties of the MLE including identifiability and consistency. We show that INVITE outperforms several existing methods on real-world human response data from memory search tasks.
1 Human Memory Search as a Random Walk
A key goal for cognitive science has been to understand the mental structures and processes that underlie human semantic memory search. Semantic fluency has provided the central paradigm for this work: given a category label as a cue (e.g. animals, vehicles, etc.) participants must generate as many example words as possible in 60 seconds without repetition. The task is useful because, while exceedingly easy to administer, it yields rich information about human semantic memory. Participants do not generate responses in random order but produce "bursts" of related items, beginning with the highly frequent and prototypical, then moving to subclusters of related items. This ordinal structure sheds light on associative structures in memory: retrieval of a given item promotes retrieval of a related item, and so on, so that the temporal proximity of items in generated lists reflects the degree to which the two items are related in memory [14, 5]. The task also places demands on other important cognitive contributors to memory search: for instance, participants must retain a mental trace of previously-generated items and use it to refrain from repetition, so that the task draws upon working memory and cognitive control in addition to semantic processes. For these reasons the task is a central tool in all commonly-used metrics for diagnosing cognitive dysfunction (see e.g. [6]). Performance is generally sensitive to a variety of neurological disorders [19], but different syndromes also give rise to different patterns of impairment, making it useful for diagnosis [17]. For these reasons the task has been widely employed both in basic science and applied health research.
Nevertheless, the representations and processes that support category fluency remain poorly understood. Beyond the general observation that responses tend to be clustered by semantic relatedness,

1

it is not clear what ordinal structure in produced responses reveals about the structure of human semantic memory, in either healthy or disordered populations. In the past few years researchers in cognitive science have begun to fill this gap by considering how search models from other domains of science might explain patterns of responses observed in fluency tasks [12, 13, 15]. We review related works in Section 4.
In the current work we build on these advances by considering, not how search might operate on a pre-specified semantic representation, but rather how the representation itself can be learned from data (i.e., human-produced semantic fluency lists) given a specified model of the list-generation process. Specifically, we model search as a random walk on a set of states (e.g. words) where the transition probability indicates the strength of association in memory, and with the further constraint that node labels are only generated when the node is first visited. Thus, repeated visits are censored in the output. We refer to this generative process as the initial-visit emitting (INVITE) random walk. The repeat-censoring mechanism of INVITE was first employed in Abbott et al. [1]. However, their work did not provide a tractable method to compute the likelihood nor to estimate the transition matrix from the fluency responses. The problem of estimating the underlying Markov chain from the lists so produced is nontrivial because once the first two items in a list have been produced there may exist infinitely many pathways that lead to production of the next item. For instance, consider the produced sequence "dog"  "cat"  "goat" where the underlying graph is fully connected. Suppose a random walk visits "dog" then "cat". The walk can then visit "dog" and "cat" arbitrarily many times before visiting "goat"; there exist infinitely many walks that outputs the given sequence. How can the transition probabilities of the underlying random walk be learned?
A solution to this problem would represent a significant advance from prior works that estimate parameters from a separate source such as a standard text corpus [13]. First, one reason for verbal fluency's enduring appeal has been that the task appears to reveal important semantic structure that may not be discoverable by other means. It is not clear that methods for estimating semantic structure based on another corpus do a very good job at modelling the structure of human semantic representations generally [10], or that they would reveal the same structures that govern behavior specifically in this widely-used fluency task. Second, the representational structures employed can vary depending upon the fluency category. For instance, the probability of producing "chicken" after "goat" will differ depending on whether the task involves listing "animals", "mammals", or "farm animals". Simply estimating a single structure from the same corpus will not capture these task-based effects. Third, special populations, including neurological patients and developing children, may generate lists from quite different underlying mental representations, which cannot be independently estimated from a standard corpus.
In this work, we make two important contributions on the INVITE random walk. First, we propose a tractable way to compute the INVITE likelihood. Our key insight in computing the likelihood is to turn INVITE into a series of absorbing random walks. This formulation allows us to leverage the fundamental matrix [7] and compute the likelihood in polynomial time. Second, we show that the MLE of INVITE is consistent, which is non-trivial given that the convergence of the log likelihood function is not uniform. We formally define INVITE and present the two main contributions as well as an efficient optimization method to estimate the parameters in Section 2. In Section 3, we apply INVITE to both toy data and real-world fluency data. On toy data our experiments empirically confirm the consistency result. On actual human responses from verbal fluency INVITE outperforms off-the-shelf baselines. The results suggest that INVITE may provide a useful tool for investigating human cognitive functions.
2 The INVITE Random Walk
INVITE is a probabilistic model with the following generative story. Consider a random walk on a set of n states S with an initial distribution  > 0 (entry-wise) and an arbitrary transition matrix P where Pij is the probability of jumping from state i to j. A surfer starts from a random initial state drawn from . She outputs a state if it is the first time she visits that state. Upon arriving at an already visited state, however, she does not output the state. The random walk continues indefinitely. Therefore, the output consists of states in the order of their first-visit; the underlying entire walk trajectory is hidden. We further assume that the time step of each output is unobserved. For example, consider the random walk over four states in Figure 1(a). If the underlying random walk takes the trajectory (1, 2, 1, 3, 1, 2, 1, 4, 1, . . .), the observation is (1, 2, 3, 4).
2

2

1

1 3

1 3

1

1

3

1

1 3

4

T 2
1 W1 3

1
1 1 14

T1 21
1 W1 1
3 14

Log likelihood

-1.44 -1.46 -1.48
-1.5 -1.52
0 0.5 1 

(a) (b) (c)

(d)

Figure 1: (a-c) Example Markov chains (d) Example nonconvexity of the INVITE log likelihood

We say that the observation produced by INVITE is a censored list since non-initial visits are censored. It is easy to see that a censored list is a permutation of the n states or a prefix thereof (more on this later). We denote a censored list by a = (a1, a2, . . . , aM ) where M  n. A censored list is not Markovian since the probability of a transition in censored list depends on the whole history rather than just the current state. It is worth noting that INVITE is distinct from Broder's algorithm for generating random spanning trees [4], or the self-avoiding random walk [9], or cascade models of infection. We discuss the technical difference to related works in Section 4.
We characterize the type of output INVITE is capable of producing, given that the underlying uncensored random walk continues indefinitely. A state s is said to be transient if a random walk starting from s has nonzero probability of not returning to itself in finite time and recurrent if such probability is zero. A set of states A is closed if a walk cannot exit A; i.e., if i  A and j  A, then a random walk from i cannot reach j. A set of states B is irreducible if there exists a path between every pair of states in B; i.e., if i, j  B, then a random walk from i can reach j. Define [M ] = {1, 2, . . . , M }. We use a1:M as a shorthand for a1, . . . , aM . Theorem 1 states that a finite state Markov chain can be uniquely decomposed into disjoint sets, and Theorem 2 states what a censored list should look like. All proofs are in the supplementary material.
Theorem 1. [8] If the state space S is finite, then S can be written as a disjoint union T  W1  . . .  WK, where T is a set of transient states that is possibly empty and each Wk, k  [K], is a nonempty closed irreducible set of recurrent states.
Theorem 2. Consider a Markov chain P with the decomposition S = T  W1  . . .  WK as in Theorem 1. A censored list a = (a1:M ) generated by INVITE on P has zero or more transient states, followed by all states in one and only one closed irreducible set. That is,   [M ] s.t. {a1: -1}  T and {a :M } = Wk for some k  [K].
As an example, when the graph is fully connected INVITE is capable of producing all n! permutations of the n states as the censored lists. As another example, in Figure 1 (b) and (c), both chains have two transient states T = {1, 2} and two recurrent states W1 = {3, 4}. (b) has no path that visits both 1 and 2, and thus every censored list must be a prefix of a permutation. However, (c) has a path that visits both 1 and 2, thus can generate (1,2,3,4), a full permutation.
In general, each INVITE run generates a permutation of n states, or a prefix of a permutation. Let Sym(n) be the symmetric group on [n]. Then, the data space D of censored lists is D  {(a1:k) | a  Sym(n), k  [n]}.

2.1 Computing the INVITE likelihood

Learning and inference under the INVITE model is challenging due to its likelihood function. A naive method to compute the probability of a censored list a given  and P is to sum over all uncensored random walk trajectories x which produces a: P(a; , P) = x produces a P(x; , P). This naive computation is intractable since the summation can be over an infinite number of trajectories x's that might have produced the censored list a. For example, consider the censored list a = (1, 2, 3, 4) generated from Figure 1(a). There are infinite uncensored trajectories to produce a by visiting states 1 and 2 arbitrarily many times before visiting state 3, and later state 4.

The likelihood of  and P on a censored list a is

P(a; , P) =

a1 0

M -1 k=1

P(ak+1

|

a1:k; P)

if a cannot be extended otherwise.

(1)

3

Note we assign zero probability to a censored list that is not completed yet, since the underlying random walk must run forever. We say a censored list a is valid (invalid) under  and P if P(a; , P) > 0 (= 0).

We first review the fundamental matrix in the absorbing random walk. A state that transits to itself

with probability 1 is called an absorbing state. Given a Markov chain P with absorbing states, we

can rearrange the states into P =

Q 0

R I

, where Q is the transition between the nonabsorbing

states, R is the transition from the nonabsorbing states to absorbing states, and the rest trivially

represent the absorbing states. Theorem 3 presents the fundamental matrix, the essential tool for the

tractable computation of the INVITE likelihood.

Theorem 3. [7] The fundamental matrix of the Markov chain P is N = (I - Q)-1. Nij is
the expected number of times that a chain visits state j before absorption when starting from i. Furthermore, define B = (I - Q)-1R. Then, Bik is the probability of a chain starting from i being
absorbed by k. In other words, Bi* is the absorption distribution of a chain starting from i.

As a tractable way to compute the likelihood, we propose a novel formulation that turns an INVITE
random walk into a series of absorbing random walks. Although INVITE itself is not an absorbing
random walk, each segment that produces the next item in the censored list can be modeled as one. That is, for each k = 1 . . . M - 1 consider the segment of the uncensored random walk starting
from the previous output ak until the next output ak+1. For this segment, we construct an absorbing random walk by keeping a1:k nonabsorbing and turning the rest into the absorbing states. A random walk starting from ak is eventually absorbed by a state in S \ {a1:k}. The probability of being absorbed by ak+1 is exactly the probability of outputting ak+1 after outputting a1:k in INVITE. Formally, we construct an absorbing random walk P(k):

P(k) = Q(k) R(k) , 0I

(2)

where the states are ordered as a1:M . Corollary 1 summarizes our computation of the INVITE likelihood.

Corollary 1. The k-th step INVITE likelihood for k  [M - 1] is

P(ak+1 | a1:k, P) =

[(I - Q(k))-1R(k)]k1 0

if (I - Q(k))-1 exists otherwise

(3)

Suppose we observe m independent realizations of INVITE: Dm

=

a(11), ..., a(M1)1 , ..., a(1m), ..., a(Mmm) , where Mi is the length of the i-th censored list.

Then, the INVITE log likelihood is (, P; Dm) =

m i=1

log

P(a(i);

,

P).

2.2 Consistency of the MLE

Identifiability is an essential property for a model to be consistent. Theorem 4 shows that allowing self-transitions in P cause INVITE to be unidentifiable. Then, Theorem 5 presents a remedy. The proof for both theorems are presented in our supplementary material. Let diag(q) be a diagonal matrix whose i-th diagonal entry is qi.
Theorem 4. Let P be an n x n transition matrix without any self-transition (Pii = 0, i), and q  [0, 1)n. Define P = diag(q) + (I - diag(q))P, a scaled transition matrix with self-transition probabilities q. Then, P(a; , P) = P(a; , P ), for every censored list a.
For example, consider a censored list a = (1, j) where j = 1. Using the fundamental matrix, P(a2 | a1; P) = (1 - P11)-1P1j = ( j =1 P1j )-1P1j = ( j =1 cP1j )-1cP1j , c. This implies that multiplying a constant c to P1j for all j = 1 and renormalizing the first row P1* to sum to 1 does not change the likelihood.

Theorem 5. Assume the initial distribution  > 0 elementwise. In the space of transition matrices

P without self-transitions, INVITE is identifiable.

Let n-1 = {p  Rn | pi  0, i, i pi = 1} be the probability simplex. For brevity, we pack

the parameters of INVITE into one vector  as follows:    = {( , P1*, . . . , Pn*) | , Pi* 

n-1, Pii = censored lists

0, i}. Let  Dm generated

= ( from ,

, P1*, . . . , Pn*) the average log

  be likelihood

the true function

model. Given a and its pointwise

set of m limit are

4

Qm()

=

1 m

m

log P(a(i)); )

and

Q() =

P(a; ) log P(a; ).

i=1 aD

(4)

For brevity, we assume that the true model  is strongly connected; the analysis can be easily extended to remove it. Under the Assumption A1, Theorem 6 states the consistency result.

Assumption A1. Furthermore, P

Let  = is strongly

( , P1*, connected.

.

.

.

,

Pn*)

  be the true model.  has no zero entries.

Theorem 6. Assume A1. The MLE of INVITE m  max Qm() is consistent.

We provide a sketch here. The proof relies on Lemma 6 and Lemma 2 that are presented in our

supplementary material. Since  is compact, the sequence {m} has a convergent subsequence

{mj }. Let  = limj mj . Since Qmj ()  Qmj (mj ),

Q()

=

lim
j

Qmj ()



lim
j

Qmj (mj

)

=

Q(

),

where the last equality is due to Lemma 6. By Lemma 2,  is the unique maximizer of Q, which implies  = . Note that the subsequence was chosen arbitrarily. Since every convergent subsequence converges to , m converges to .

2.3 Parameter Estimation via Regularized Maximum Likelihood

We present a regularized MLE (RegMLE) of INVITE. We first extend the censored lists that we

consider. Now we allow the underlying walk to terminate after finite steps because in real-world

applications the observed censored lists are often truncated. That is, the underlying random walk

can be stopped before exhausting every state the walk could visit. For example, in verbal fluency,

participants have limited time to produce a list. Consequently, we use the prefix likelihood

M -1

L(a; , P) = a1

P(ak+1 | a1:k; P).

(5)

k=1

We find the RegMLE by maximizing the prefix log likelihood plus a regularization term on , P.

Note that,  and P can be separately optimized. For , we place a Dirichlet prior and find the

maximum a posteriori (MAP) estimator  by j 

m i=1

1a(1i) =j

+

C ,

j.

Directly computing the RegMLE of P requires solving a constrained optimization problem, because

the transition matrix P must be row stochastic. We re-parametrize P which leads to a more conve-

nient unconstrained optimization problem. Let   Rnxn. We exponentiate  and row-normalize it

to derive P: Pij = eij /

n j =1

eij

, i, j.

We

fix

the

diagonal

entries

of



to

-

to

disallow

self-

transitions. We place squared 2 norm regularizer on  to prevent overfitting. The unconstrained

optimization problem is:

min


-

m i=1

Mi -1 k=1

log

P(a(ki+) 1

|

a(1i:)k; )

+

1 2

C

i=j i2j

,

(6)

where C > 0 is a regularization parameter. We provide the derivative of the prefix log likelihood w.r.t.  in our supplementary material. We point out that the objective function of (6) is not convex
in  in general. Let n = 5 and suppose we observe two censored lists (5, 4, 3, 1, 2) and (3, 4, 5, 1, 2). We found with random starts two different local optima (1) and (2) of (6). We plot the prefix log likelihood of (1 - )(1) + (2), where   [0, 1] in Figure 1(d). Nonconvexity of this 1D slice

implies nonconvexity of the prefix log likelihood surface in general.

Efficient Optimization using Averaged Stochastic Gradient Descent Given a censored list a of length M , computing the derivative of P(ak+1 | a1:k) w.r.t.  takes O(k3) time for matrix inversion. There are n2 entries in , so the time complexity per item is O(k3 + n2). This computation needs to
be done for k = 1, ..., (M - 1) in a list and for m censored lists, which makes the overall time complexity O(mM (M 3 +n2)). In the worst case, M is as large as n, which makes it O(mn4). Even the

state-of-the-art batch optimization method such as LBFGS takes a very long time to find the solution for a moderate problem size such as n  500. For a faster computation of the RegMLE (6), we turn

to averaged stochastic gradient descent (ASGD) [20, 18]. ASGD processes the lists sequentially by updating the parameters after every list. The per-round objective function for  on the i-th list is

f (a(i); )



Mi -1
- log

P(ak(i+) 1

|

a(1i:)k; )

+

C 2m

i2j .

k=1

i=j

5

error(P) error(P) error(P)

Ring, n=25

3

INVITE RW

FE

2

Star, n=25 5 4 3

Grid, n=25 3
INVITE RW 2 FE

1
0 102
The number of lists (m)

2

INVITE RW

1 FE

102 The number of lists (m)

1
0 102
The number of lists (m)

(a) (b) (c)

Figure 2: Toy experiment results where the error is measured with the Frobenius norm.

We randomly initialize 0. At round t, we update the solution t with t  t-1 - tf (a(i); )

and the average estimate t with t



t-1 t

t-1

+

1 t

t.

Let

t

=

0(1 + 0at)-c.

We use

a = C/m and c = 3/4 following [3] and pick 0 by running the algorithm on a small subsample

of the train set. We run ASGD for a fixed number of epochs and take the final t as the solution.

3 Experiments

We compare INVITE against two popular estimators of P: naive random walk (RW) and First-Edge

(FE). RW is the regularized MLE of the naive random walk, pretending the censored lists are the

underlying uncensored walk trajectory: Pr(cRW ) 

m i=1

1Mi-1

j=1

(a(ji) =r)(a(ji+) 1 =c)

+ CRW .

Though simple and popular, RW is a biased estimator due to the model mismatch. FE was proposed

in [2] for graph structure recovery in cascade model. FE uses only the first two items in each

censored list: Pr(cF E) 

1m
i=1 (a(1i)=r)(a(2i)=c)

+CF E. Because the first transition in a censored

list is always the same as the first transition in its underlying trajectory, FE is a consistent estimator

of P (assuming  has no zero entries). In fact, FE is equivalent to the RegMLE of the length

two prefix likelihood of the INVITE model. However, we expect FE to waste information since it

discards the rest of the censored lists. Furthermore, FE cannot estimate the transition probabilities

from an item that does not appear as the first item in the lists, which is common in real-world data.

3.1 Toy Experiments
Here we compare the three estimators INVITE, RW, and FE on toy datasets, where the observations are indeed generated by an initial-visit emitting random walk. We construct three undirected, unweighted graphs of n = 25 nodes each: (i) Ring,a ring graph, (ii) Star, n - 1 nodes each connected to a "hub" node, and (iii) Grid, a 2-dimensional n x n lattice.
The initial distribution  is uniform, and the transition matrix P at each node has an equal transition probability to its neighbors. For each graph, we generate datasets with m  {10, 20, 40, 80, 160, 320, 640} censored lists. Each censored list has length n. We note that, in the star graph a censored list contains many apparent transitions between leaf nodes, although such transitions are not allowed in its underlying uncensored random walk. This will mislead RW. This effect is less severe in the grid graph and the ring graph.

For each estimator, we perform 5-fold cross validation (CV) for finding the best smoothing parameters C, CRW , CF E on the grid 10-2, 10-1.5, . . . , 101, respectively, with which we compute each

estimator. Then, we evaluate the three estimators using the Frobenius norm between P and the true

transition matrix P: error(P) =

i,j(Pij - Pij)2. Note the error must approach 0 as m in-

creases for consistent estimators. We repeat the same experiment 20 times where each time we draw

a new set of censored lists.

Figure 2 shows how error(P) changes as the number of censored lists m increases. The error bars are 95% confidence bounds. We make three observations: (1) INVITE tends towards 0 error. This is expected given the consistency of INVITE in Theorem 6. (2) RW is biased. In all three plots, RW tends towards some positive number, unlike INVITE and FE. This is because RW has the wrong model on the censored lists. (3) INVITE outperforms FE. On the ring and grid graphs INVITE dominates FE for every training set size. On the star graph FE is better than INVITE with a small m, but INVITE eventually achieves lower error. This reflects the fact that, although FE is unbiased, it discards most of the censored lists and therefore has higher variance compared to INVITE.

6

n

m

Min.

Length

Max. Mean

Median

Animal 274 4710 2 36 18.72 19

Food 452 4622 1 47 20.73 21

Animal Food

Model INVITE
RW FE INVITE RW FE

Test set mean neg. loglik. 60.18 (1.75) 69.16 (2.00) 72.12 (2.17) 83.62 (2.32) 94.54 (2.75) 100.27 (2.96)

Table 1: Statistics of the verbal fluency data. Table 2: Verbal fluency test set log likelihood.

3.2 Verbal Fluency

We now turn to the real-world fluency data where we compare INVITE with the baseline models. Since we do not have the ground truth parameter  and P, we compare test set log likelihood of various models. Confirming the empirical performance of INVITE sheds light on using it for practical applications such as the dignosis and classification of the brain-damaged patient.
Data The data used to assess human memory search consists of two verbal fluency datasets from the Wisconsin Longitudinal Survey (WLS). The WLS is a longitudinal assessment of many sociodemographic and health factors that has been administered to a large cohort of Wisconsin residents every five years since the 1950s. Verbal fluency for two semantic categories, animals and foods, was administered in the last two testing rounds (2005 and 2010), yielding a total of 4714 lists for animals and 4624 lists for foods collected from a total of 5674 participants ranging in age from their early-60's to mid-70's. The raw lists included in the WLS were preprocessed by expanding abbreviations ("lab"  "labrador"), removing inflections ("cats"  "cat"), correcting spelling errors, and removing response errors like unintelligible items. Though instructed to not repeat, some human participants did occasionally produce repeated words. We removed the repetitions from the data, which consist of 4% of the word token responses. Finally, the data exhibits a Zipfian behavior with many idiosyncratic, low count words. We removed words appearing in less than 10 lists. In total, the process resulted in removing 5% of the total number of word token responses. The statistics of the data after preprocessing is summarized in Table 1.
Procedure We randomly subsample 10% of the lists as the test set, and use the rest as the training set. We perform 5-fold CV on the training set for each estimator to find the best smoothing parameter C, CRW , CF E  {101, 10.5, 100, 10-.5, 10-1, 10-1.5, 10-2} respectively, where the validation measure is the prefix log likelihood for INVITE and the standard random walk likelihood for RW. For the validation measure of FE we use the INVITE prefix log likelihood since FE is equivalent to the length two prefix likelihood of INVITE. Then, we train the final estimator on the whole training set using the fitted regularization parameter.
Result The experiment result is summarized in Table 2. For each estimator, we measure the average per-list negative prefix log likelihood on the test set for INVITE and FE, and the standard random walk per-list negative log likelihood for RW. The number in the parenthesis is the 95% confidence interval. Boldfaced numbers mean that the corresponding estimator is the best and the difference from the others is statistically significant under a two-tailed paired t-test at 95% significance level. In both animal and food verbal fluency tasks, the result indicates that human-generated fluency lists are better explained by INVITE than by either RW or FE. Furthermore, RW outperforms FE. We believe that FE performs poorly despite being consistent because the number of lists is too small (compared to the number of states) for FE to reach a good estimate.

4 Related Work
Though behavior in semantic fluency tasks has been studied for many years, few computationally explicit models of the task have been advanced. Influential models in the psychological literature, such as the widely-known "clustering and switching" model of Troyer et al. [21], have been articulated only verbally. Efforts to estimate the structure of semantic memory from fluency lists have mainly focused on decomposing the structure apparent in distance matrices that reflect the mean inter-item ordinal distances across many fluency lists [5]--but without an account of the processes that generate list structure it is not clear how the results of such studies are best interpreted. More recently, researchers in cognitive science have begun to focus on explicit model of the processes by which fluency lists are generated. In these works, the structure of semantic memory is first modelled either as a graph or as a continuous multidimensional space estimated from word co-occurrence statistics in large corpora of natural language. Researchers then assess whether structure in fluency data can be understood as resulting from a particular search process operating over the specified semantic

7

structure. Models explored in this vein include simple random walk over a semantic network, with repeated nodes omitted from the sequence produced [12], the PageRank algorithm employed for network search by Google [13], and foraging algorithms designed to explain the behavior of animals searching for food [15]. Each example reports aspects of human behavior that are well-explained by the respective search process, given accompanying assumptions about the nature of the underlying semantic structure. However, these works do not learn their model directly from the fluency lists, which is the key difference from our study.
Broder's algorithm Generate [4] for generating random spanning tree is similar to INVITE's generative process. Given an undirected graph, the algorithm runs a random walk and outputs each transition to an unvisited node. Upon transiting to an already visited node, however, it does not output the transition. The random walk stops after visiting every node in the graph. In the end, we observe an ordered list of transitions. For example, in Figure 1(a) if the random walk trajectory is (2,1,2,1,3,1,4), then the output is (21, 13, 14). Note that if we take the starting node of the first transition and the arriving nodes of each transition, then the output list reduces to a censored list generated from INVITE with the same underlying random walk. Despite the similarity, to the best of our knowledge, the censored list derived from the output of the algorithm Generate has not been studied, and there has been no parameter estimation task discussed in prior works.
Self-avoiding random walk, or non-self-intersecting random walk, performs random walk while avoiding already visited node [9]. For example, in Figure 1(a), if a self-avoiding random walk starts from state 2 then visits 1, then it can only visit states 3 or 4 since 2 is already visited. In not visiting the same node twice, self-avoiding walk is similar to INVITE. However, a key difference is that self-avoiding walk cannot produce a transition i  j if Pij = 0. In contrast, INVITE can appear to have such "transitions" in the censored list. Such behavior is a core property that allows INVITE to switch clusters in modeling human memory search.
INVITE resembles cascade models in many aspects [16, 11]. In a cascade model, the information or disease spreads out from a seed node to the whole graph by infections that occur from an infected node to its neighbors. [11] formulates a graph learning problem where an observation is a list, or so-called trace, that contains infected nodes along with their infection time. Although not discussed in the present paper, it is trivial for INVITE to produce time stamps for each item in its censored list, too. However, there is a fundamental difference in how the infection occurs. A cascade model typically allows multiple infected nodes to infect their neighbors in parallel, so that infection can happen simultaneously in many parts of the graph. On the other hand, INVITE contains a single surfer that is responsible for all the infection via a random walk. Therefore, infection in INVITE is necessarily sequential. This results in INVITE exhibiting clustering behaviors in the censored lists, which is well-known in human memory search tasks [21].
5 Discussion
There are numerous directions to extend INVITE. First, more theoretical investigation is needed. For example, although we know the MLE of INVITE is consistent, the convergence rate is unknown. Second, one can improve the INVITE estimate when data is sparse by assuming certain cluster structures in the transition matrix P, thereby reducing the degrees of freedom. For instance, it is known that verbal fluency tends to exhibit "runs" of semantically related words. One can assume a stochastic block model P with parameter sharing at the block level, where the blocks represent semantic clusters of words. One then estimates the block structure and the shared parameters at the same time. Third, INVITE can be extended to allow repetitions in a list. The basic idea is as follows. In the k-th segment we previously used an absorbing random walk to compute P(ak+1 | a1:k), where a1:k were the nonabsorbing states. For each nonabsorbing state ai, add a "dongle twin" absorbing state ai attached only to ai. Allow a small transition probability from ai to ai. If the walk is absorbed by ai, we output ai in the censored list, which becomes a repeated item in the censored list. Note that the likelihood computation in this augmented model is still polynomial. Such a model with "reluctant repetitions" will be an interesting interpolation between "no repetitions" and "repetitions as in a standard random walk."
Acknowledgments
The authors are thankful to the anonymous reviewers for their comments. This work is supported in part by NSF grants IIS-0953219 and DGE-1545481, NIH Big Data to Knowledge 1U54AI11792401, NSF Grant DMS-1265202, and NIH Grant 1U54AI117924-01.
8

References
[1] J. T. Abbott, J. L. Austerweil, and T. L. Griffiths, "Human memory search as a random walk in a semantic network," in NIPS, 2012, pp. 3050-3058.
[2] B. D. Abrahao, F. Chierichetti, R. Kleinberg, and A. Panconesi, "Trace complexity of network inference." CoRR, vol. abs/1308.2954, 2013.
[3] L. Bottou, "Stochastic gradient tricks," in Neural Networks, Tricks of the Trade, Reloaded, ser. Lecture Notes in Computer Science (LNCS 7700), G. Montavon, G. B. Orr, and K.-R. Muller, Eds. Springer, 2012, pp. 430-445.
[4] A. Z. Broder, "Generating random spanning trees," in FOCS. IEEE Computer Society, 1989, pp. 442-447.
[5] A. S. Chan, N. Butters, J. S. Paulsen, D. P. Salmon, M. R. Swenson, and L. T. Maloney, "An assessment of the semantic network in patients with alzheimer's disease." Journal of Cognitive Neuroscience, vol. 5, no. 2, pp. 254-261, 1993.
[6] J. R. Cockrell and M. F. Folstein, "Mini-mental state examination." Principles and practice of geriatric psychiatry, pp. 140-141, 2002.
[7] P. G. Doyle and J. L. Snell, Random Walks and Electric Networks. Washington, DC: Mathematical Association of America, 1984.
[8] R. Durrett, Essentials of stochastic processes, 2nd ed., ser. Springer texts in statistics. New York: Springer, 2012.
[9] P. Flory, Principles of polymer chemistry. Cornell University Press, 1953.
[10] A. M. Glenberg and S. Mehta, "Optimal foraging in semantic memory," Italian Journal of Linguistics, 2009.
[11] M. Gomez Rodriguez, J. Leskovec, and A. Krause, "Inferring networks of diffusion and influence," Max-Planck-Gesellschaft. New York, NY, USA: ACM Press, July 2010, pp. 1019- 1028.
[12] J. Goi, G. Arrondo, J. Sepulcre, I. Martincorena, N. V. de Mendizbal, B. Corominas-Murtra, B. Bejarano, S. Ardanza-Trevijano, H. Peraita, D. P. Wall, and P. Villoslada, "The semantic organization of the animal category: evidence from semantic verbal fluency and network theory." Cognitive Processing, vol. 12, no. 2, pp. 183-196, 2011.
[13] T. L. Griffiths, M. Steyvers, and A. Firl, "Google and the mind: Predicting fluency with pagerank," Psychological Science, vol. 18, no. 12, pp. 1069-1076, 2007.
[14] N. M. Henley, "A psychological study of the semantics of animal terms." Journal of Verbal Learning and Verbal Behavior, vol. 8, no. 2, pp. 176-184, Apr. 1969.
[15] T. T. Hills, P. M. Todd, and M. N. Jones, "Optimal foraging in semantic memory," Psychological Review, pp. 431-440, 2012.
[16] D. Kempe, J. Kleinberg, and E. Tardos, "Maximizing the spread of influence through a social network," in Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '03. New York, NY, USA: ACM, 2003, pp. 137-146.
[17] F. Pasquier, F. Lebert, L. Grymonprez, and H. Petit, "Verbal fluency in dementia of frontal lobe type and dementia of Alzheimer type." Journal of Neurology, vol. 58, no. 1, pp. 81-84, 1995.
[18] B. T. Polyak and A. B. Juditsky, "Acceleration of stochastic approximation by averaging," SIAM J. Control Optim., vol. 30, no. 4, pp. 838-855, July 1992.
[19] T. T. Rogers, A. Ivanoiu, K. Patterson, and J. R. Hodges, "Semantic memory in Alzheimer's disease and the frontotemporal dementias: a longitudinal study of 236 patients." Neuropsychology, vol. 20, no. 3, pp. 319-335, 2006.
[20] D. Ruppert, "Efficient estimations from a slowly convergent robbins-monro process," Cornell University Operations Research and Industrial Engineering, Tech. Rep., 1988.
[21] A. Troyer, M. Moscovitch, G. Winocur, M. Alexander, and D. Stuss, "Clustering and switching on verbal fluency: The effects of focal fronal- and temporal-lobe lesions," Neuropsychologia, vol. 36, no. 6, 1998.
9

