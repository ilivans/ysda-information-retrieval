Spectral Norm Regularization of Orthonormal Representations for Graph Transduction

Rakesh Shivanna Google Inc.
Mountain View, CA, USA rakeshshivanna@google.com

Bibaswan Chatterjee Dept. of Computer Science & Automation
Indian Institute of Science, Bangalore bibaswan.chatterjee@csa.iisc.ernet.in

Raman Sankaran, Chiranjib Bhattacharyya Dept. of Computer Science & Automation Indian Institute of Science, Bangalore ramans,chiru@csa.iisc.ernet.in

Francis Bach INRIA - Sierra Project-team E cole Normale Superieure, Paris, France
francis.bach@ens.fr

Abstract
Recent literature [1] suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction. However, the choice of optimal embedding and an efficient algorithm to compute the same remains open. In this paper, we show that orthonormal representations, a class of unit-sphere graph embeddings are PAC learnable. Existing PAC-based analysis do not apply as the VC dimension of the function class is infinite. We propose an alternative PAC-based bound, which do not depend on the VC dimension of the underlying function class, but is related to the famous Lovasz  function. The main contribution of the paper is SPORE, a SPectral regularized ORthonormal Embedding for graph transduction, derived from the PAC bound. SPORE is posed as a non-smooth convex function over an elliptope. These problems are usually solved as semi-definite programs (SDPs) with time complexity O(n6). We present, Infeasible Inexact proximal (IIP): an Inexact proximal method which performs subgradient procedure on an approximate projection, not necessarily feasible. IIP is more scalable than SDP, has an O( 1 ) convergence, and is generally applicable whenever a suit-
T
able approximate projection is available. We use IIP to compute SPORE where the approximate projection step is computed by FISTA, an accelerated gradient descent procedure. We show that the method has a convergence rate of O( 1 ).
T
The proposed algorithm easily scales to 1000's of vertices, while the standard SDP computation does not scale beyond few hundred vertices. Furthermore, the analysis presented here easily extends to the multiple graph setting.
1 Introduction
Learning problems on graph-structured data have received significant attention in recent years [11, 17, 20]. We study an instance of graph transduction, the problem of learning labels on vertices of simple graphs1. A typical example is webpage classification [20], where a very small part of the entire web is manually classified. Even for simple graphs, predicting binary labels of the unlabeled vertices is NP-complete [6].
More formally: let G = (V, E), V = [n] be a simple graph with unknown labels y  {1}n. Without loss of generality, let the labels of first m  [n] vertices be observable, let u := n - m.
1A simple graph is an unweighted, undirected graph with no self loops or multiple edges.
1

Let yS and yS be the labels of S = [m] and S = V \S. Given G and yS, the goal is to learn soft

predictions y  Rn, such that erS[y] :=

1
|S| jS

(yj, yj) is small, where

is any loss function. The

following formulation has been extensively used [19, 20]

min
yRn

erS[y] + y

K-1y,

(1)

where K is a graph-dependent kernel and  > 0 is a regularizer constant. Let y be the solution to (1), given G and S  V, |S| = m. [1] proposed the following generalization bound

ESV

erS[y]



c1

inf
yRn

erV [y] + y

K-1y

+ c2

trp(K)

p
,

|S|

(2)

where c1, c2 are dependent on

and trp(K) =

1 n

i[n] Kpii 1/p, p > 0. [1] argued that trp(K)

should be a constant and can be enforced by normalizing the diagonal entries of K to be 1. This

is an important advice in graph transduction, however it is to be noted that the set of normalized

kernels is quite large and (2) gives little insight in choosing the optimal kernel.

Normalizing the diagonal entries of K can be viewed geometrically as embedding the graph on a

unit sphere. Recently, [16] studied a rich class of unit sphere graph embeddings, called orthonormal

representations [13], and find that it is statistically consistent for graph transduction. However, the

choice of the optimal orthonormal embedding is not clear. We study orthonormal representations

for

the

following

equivalent

[19]

kernel

learning

formulation

of

(1),

with

C

=

1 m

,

1

C (K, yS) = max
Rn

i - 2

ijyiyjKij s.t. 0  i  C i  S, j = 0 j / S, (3)

iS i,jS

from a probably approximately correctly (PAC) learning point of view. Note that the final predictions are given by yi = jS Kijjyj i  [n], where  is the optimal solution to (3).
Contributions. We make the following contributions:
- Using (3) we show the class of orthonormal representations are efficiently PAC learnable over a large class of graph families, including power-law and random graphs.
- The above analysis suggests that spectral norm regularization could be beneficial in computing the best embedding. To this end we pose the problem of SPectral norm regularized ORthonormal Embedding (SPORE) for graph Transduction, namely that of minimizing a convex function over an elliptope. One could solve such problems as SDPs which unfortunately do not scale well beyond few hundred vertices.
- We propose an infeasible inexact proximal (IIP) method, a novel projected subgradient descent algorithm, in which the projection is approximated by an inexact proximal method. We suggest a novel approximation criteria which approximates the proximal operator for the support function of the feasible set within a given precision. One could compute an approximation to the projection from the inexact proximal point which may not be feasible hence the name IIP. We provethat IIP converges to the optimal minimum of a non-smooth convex function with rate O(1/ T ) in T iterations.
- The IIP algorithm is then applied to the case when the set of interest is composed of the intersection of two convex sets. The proximal operator for the support function of the set of interest can be obtained using the FISTA algorithm, once we know the proximal operator for the support functions of the individual sets involved.
- Our analysis paves the way for learning labels on multiple graphs by using the embedding by adopting an MKL style approach. We present both algorithmic and generalization results.

Notations. Let * , * F denote the Euclidean and Frobenius norm respectively. Let Sn and

Sn+ denote the set of respectively. Let Rn+

nx be a

n square symmetric and square non-negative orthant. Let Sn-1

symmetric positive = u  Rn+ u 1

semi-definite = 1 denote

matrices the n-1

dimensional simplex. Let [n] := {1, . . . , n}. For any M  Sn, let 1(M)  . . .  n(M) denote its Eigenvalues. We denote the adjacency matrix of a graph G by A. Let G denote the complement

graph of G, with the adjacency matrix A = 11 - I - A; where 1 is a vector of all 1's, and I is the

identity matrix. Let Y = {1}, Y = R be the label and soft-prediction spaces over V . Given y  Y

2

and y  Y, we use 0-1(y, y) = 1[yy < 0], hng(y, y) = (1 - yy)+2 to denote 0-1 and hinge loss respectively. The notations O, o, ,  will denote standard measures in asymptotic analysis [4].
Related work. [1]'s analysis was restricted to Laplacian matrices, and does not give insights in choosing the optimal unit sphere embedding. [2] studied graph transduction using PAC model, however for graph orthonormal embeddings, there is no known sample complexity estimate. [16] showed that working with orthonormal embeddings leads to consistency. However, the choice of optimal embedding and an efficient algorithm to compute the same remains an open issue. Furthermore, we show that [16]'s sample complexity estimate is sub-optimal.
Preliminaries. An orthonormal embedding [13] of a simple graph G = (V, E), V = [n], is defined by a matrix U = [u1, . . . , un]  Rdxn such that ui uj = 0 whenever (i, j) / E and ui = 1 i  [n]. Let Lab(G) denote the set of all possible orthonormal embeddings of the graph G, Lab(G) := U | U is an orthonormal embedding . Recently, [8] showed an interesting connection to the set of graph kernel matrices
K(G) := K  Sn+ | Kii = 1, i  [n]; Kij = 0, (i, j) / E .
Note that K  K(G) is positive semidefinite, and hence there exists U  Rdxn such that K = U U. Note that Kij = ui uj where ui is the i-th column of U. Hence by inspection it is clear that U  Lab(G). Using a similar argument, we can show that for any U  Lab(G), the matrix K = U U  K(G). Thus, the two sets, Lab(G) and K(G) are equivalent.
Furthermore, orthonormal embeddings are associated with an interesting quantity, the Lovasz  function [13, 7]. However, computing  requires solving an SDP, which is impractical.

2 Generalization Bound for Graph Transduction using Orthonormal Embeddings

In this section we derive a generalization bound, used in the sequel for PAC analysis. We derive the following error bound, valid for any orthonormal embedding (supplementary material, Section B).
Theorem 1 (Generalization bound). Let G = (V, E) be a simple graph with unknown binary labels y  Yn on the vertices V . Let K  K(G). Given G, and labels of a randomly drawn subgraph
S, let y  Yn be the predictions learnt by C (K, yS) in (3). Then, for m  n/2, with probability  1 -  over the choice of S  V , such that |S| = m

erS0-1 [y ]



1 m

hng(yi, yi) + 2C 21(K) + O

iS

11 log .
m

(4)

Note that the above is a high-probability bound, in comparison to the expected analysis in (2). Also, the above result suggests that graph embeddings with low spectral norm and empirical error lead to better generalization. [1]'s analysis in (2) suggests that we should embed a graph on a unit sphere, however, does not help to choose the optimal embedding for graph transduction. Exploiting our analysis from (4), we present a spectral norm regularized algorithm in Section 3.
We would also like to study PAC learnability of orthonormal embeddings, where PAC learnability is defined as follows: given G, y; does there exist an m < n, such that w.p.  1 -  over S  V, |S|  m ; the generalization error erS0-1  . The quantity m is termed as labelled sample complexity [2]. Existing analysis [2] do not apply to orthonormal embeddings as discussed in related work, Section 1. Theorem 1 allows us to derive improved statistical estimates (Section 3).

3 SPORE Formulation and PAC Analysis

Theorem 1 suggests that penalizing the spectral norm of K would lead to better generalization. To this end we motivate the following formulation.

C,(G, yS) = min g K
KK(G)

where g(K) = C (K, yS) + 1(K).

(5)

2(a)+ = max(a, 0) a  R

3

(5) gives an optimal orthonormal embedding, the optimal K, which we will refer to as SPORE. In this section we first study the PAC learnability of SPORE, and derive a labelled sample complexity estimate. Next, we study efficient computation of SPORE. Though SPORE can be posed as an SDP, we show in Section 4 that it is possible to exploit the structure, and solve efficiently.
Given G and yS, the function C (K, yS) is convex in K as it is the maximum of affine functions of K. The spectral norm of K, 1(K) is also convex, and hence g(K) is a convex function. Furthermore K(G) is an Elliptope [5], a convex body which can be described by the intersection of a positive semi-definite and affine constraints. It follows that hence (5) is convex. Usually these formulations are posed as SDPs which do not scale beyond few hundred vertices. In Section 4 we derive an efficient first order method which can solve for 1000's of vertices. Let K be the optimal embedding computed from (5). Note that once the kernel is fixed, the predictions are only dependent on C (K, yS). Let  be the solution to C (K, yS) as in (3), then the final predictions of (5) is given by yi = jS Kij jyj , i  [n].
At this point, we derive an interesting graph-dependent error convergence rate. We gather two important results, the proof of which appears in the supplementary material, Section C. Lemma 2. Given a simple graph G = (V, E), maxKK(G) 1(K) = (G).
Lemma 3. Given G and y, for any S  V and C > 0, minKK(G) C (K, yS)  (G)/2.

In the standard PAC setting, there is a complete disconnection between the data distribution and

target hypothesis. However, in the presence of unlabeled nodes, without any assumption on the

data, it is impossible to learn labels. Following existing literature [1, 9], we work with similarity

graphs - where presence of an edge would mean two nodes are similar; and derive the following

(supplementary material, Section C).

Theorem 4. Let G = (V, E), V = [n] be a simple graph with unknown binary labels y  Yn

on the vertices V . Given G, and labels of a randomly drawn subgraph S  V , m = |S|; let y be

1

the predictions learnt by SPORE (5), for parameters C =

(G)
m (G)

2

and 

=

(G)
(G)

.

Then, for

m  n/2, with probability  1 -  over the choice of S  V , such that |S| = m

erS0-1[y] = O

1 m

1 n(G) + log

1
2.



(6)

Proof. (Sketch) Let K be the kernel learnt by SPORE (5). Using Theorem 1 and Lemma 2 for y

erS0-1 [y ]



1 m

hng(yi, yi) + 2C 2 G + O

iS

11 log .
m

From the primal formulation of (3), using Lemma 2 and 3, we get

C

hng(yi, yi)



C (K, yS)



C,(G, yS )



(G) 2

+



G

.

iS

(7)

Plugging back in (7), choosing  such that

 Cm



G

= 2C

2 G and optimizing for C gives

us the choice of parameters as stated. Finally, using (G)(G) = n [13] proves the result.

In the theorem above, G is the complement graph of G. The optimal orthonormal embedding K tend to embed vertices to nearby regions if they have connecting edges, hence, the notion of similarity is implicitly captured in the embedding. From (6), for a fixed n and m, note that the error converges at a faster rate for a dense graph ( is small), than for a sparse graph ( is large). Such connections relating to graph structural properties were previously unavailable [1].

We also estimate the labelled sample complexity, by bounding (6) by > 0, to obtain m =



1
2

(

n

+

log

1 

)

.

This connection helps to reason the intuition that for a sparse graph one

would need a larger number of labelled vertices, than for a dense graph. For constants , , we

1
obtain a fractional labelled sample complexity estimate of m /n =  /n 2 , which is a signif-

1
icant improvement over the recently proposed  /n 3 [16]. The use of stronger machinery of

4

Rademacher averages (supplementary material, Section C), instead of VC-dimension [2], and spe-

cializing to SPORE allows us to improve over existing analysis [1, 16]. The proposed sample

complexity estimate is interesting for  ((G(n, p)) = ( n)) and power-law

= o(n), graphs

(exa=mOpl(es onf))s.uch

graphs

include:

random

graphs

4 Inexact Proximal methods for SPORE

In this section, we propose an efficient algorithm to solve SPORE (see (5)). The optimization problem SPORE can be posed as an SDP. Generic SDP solvers have a runtime complexity of O(n6) and often does not scale well for large graphs. We study first-order methods, such as projected subgradient procedures, as an alternative to SDPs, for minimizing g(K). The main computational challenge in developing such procedures is that it is difficult to compute the projection on the elliptope. One could potentially use the seminal Dykstra's algorithm [3] of finding a feasible point in the intersection of two convex sets. The algorithm asymptotically finds a point in the intersection. This asymptotic convergence is a serious disadvantage in the usage of Dykstra's algorithm as a projection sub-routine. It would be useful to have an algorithm which after a finite number of iterations yield an approximate projection and a subsequent descent algorithm can yield a convergent algorithm.
Motivated by SPORE, we study the problem of minimizing non-smooth convex functions where the projection onto the feasible set can be computed only approximately. Recently there has been increasing interest in studying Inexact proximal methods [15, 18]. In the sequel we design an inexact proximal method which yields an O(1/ T ) algorithm to solve (5). The algorithm is based on approximating the prox function by an iterative procedure which satisfies a suitably designed criterion.

4.1 An Infeasible Inexact Proximal (IIP) algorithm

Let f be a convex function with properly defined sub-differential f (x) at every x  X . Consider

the following optimization problem.

min f (x).
xX Rd

(8)

A subgradient projection iteration of the form

xk+1 = PX (xk - khk), hk  f (xk)

(9)

is often used to arrive at an

accurate

solution

by

running

the

iterations

O(

1
2

)

number

of

times,

where PX (v) is the projection of v  Rd on X

 Rd if PX (v) = argminxX

1 2

v-x

2 F

.

In

many

situations, such as X = K(G), it is not possible to accurately compute the projection in finite amount

of time and one may obtain only an approximate projection. Using the Moreau decomposition
PX (v) + ProxX (v) = v [14], one can compute the projection if one could compute proxX , where A(a) = maxaX x a is the support function of X , and proxX refers to the proximal operator for the function g at v as defined below3.

proxg (v) = argmin pg (z; v) zDom(g )

1 =

v-z

2 + g (z)

.

2

(10)

We assume that one could compute zX (v), not necessarily in X , such that

pX

(zX

(v);

v)



min
zRn

pX

(z;

v)

+

,

and PX (v) = v - zX .

(11)

See that zX is an inexact prox and the resultant estimate of the projection PX can be infeasible but hopefully not too far away. Note that = 0 recovers the exact case. The next theorem confirms that it is possible to converge to the true optimum for a non-zero (supplementary material, Section D.5).

Theorem 5. Consider the optimization problem (8). Starting from any x0 - x  R, where x is a solution of (8), for every k let us assume that we could obtain PX (yk) such that zk = yk - PX (yk)

satisfy (11), where yk = xk - khk, k =

s hk

,

hk

 L,

xk - x

 R, s =

R2 T

+

.

Then the iterates

xk+1 = PX (xk - khk), hk  f (xk)

(12)

3A

more

general

definition

of

the

proximal

operator

is

-

proxg

(v)

=

argminzDom(g

)

1 2

v-z 2+g (z)

5

yield

fT - f   L

R2 +.
T

(13)

Related Work on Inexact Proximal methods: There has been recent interest in deriving inexact proximal methods such as projected gradient descent, see [15, 18] for a comprehensive list of references. To the best of our knowledge, composite functions have been analyzed but no one has explored the case that f is non-smooth. The results presented here are thus complementary to [15, 18]. Note the subtlety in using the proper approximation criteria. Using a distance criterion between the true projection and the approximate projection, or an approximate optimality criteria on the optimal distance would lead to a worse bound; using a dual approximate optimality criterion (here through the proximal operator for the support function) is key (as noted in [15, 18] and references therein).

As an immediate consequence of Theorem 5, note that suppose we have an algorithm to compute proxX which guarantees after S iterations that

pX (zS ; v)

-

min
zRd

pX (z; v)



R2 S2 ,

(14)

for a constant R particular to the set overwhich pX is defined. We can initialize

=

R2 S2

in (13),

that may suggest that one could use S = T iterations to yield

fT

-

f



LR T

where R =

R2 + R2.

(15)

Remarks: Computational efficiency dictates that the number of projection steps shouldbe kept at a minimum. To this end we see that number of projection steps need to be at least S = T with the current choice of stepsizes. Let cp be the cost of one iteration of FISTA step and c0 be the cost of one outer iteration. The total computation cost can be then estimated as T 3/2 * cp + T * c0.

4.2 Applying IIP to compute SPORE

The problem of computing SPORE can be posed as minimizing a non-smooth convex function over an intersection of two sets: K(G) = Sn+  P (G), intersection of positive semi-definite cone Sn+ and a polytope of equality constraints P (G) := {M  Sn|Mii = 1, Mij = 0 (i, j) / E}.
The algorithm described in Theorem 5 readily applies to the new setting if the projection can be computed efficiently. The proximal operator for X can be derived as 4

ProxX (v) = argmin pX (a, b; v)
a,bRd

1 =
2

(a + b) - v

2 + A(a) + B(b)

.

(16)

This means that even if we do not have an efficient procedure for computing ProxX (v) directly, we can devise an algorithm to guarantee the approximation (11) if we can compute ProxA (v) and ProxB (v) efficiently. This can be done through the application of the popular FISTA algorithm for (16), which also guarantees (14). Algorithm 1 (detailed in the supplementary, named IIP F IST A),

computes the following simple steps followed by the usual FISTA variable updates at each iteration

t:

(a) gradient descent step on a and b with respect to the smooth term

1 2

(a + b) - v

2 and (b)

proximal step with respect to A and B using the expressions (14), (21) (supplementary material).

Using the tools discussed above, we design Algorithm 1 to solve the SPORE formulation (5) using IIP. The proposed algorithm readily applies to general convex sets. However, we confine ourselves to specific sets of interest in our problem. The following theorem states the convergence rate of the proposed procedure.
Theorem 6. Consider the optimization problem (8) with X = A B, where A and B are Sn+ and P (G) respectively. Starting from any K0  A the iterates Kt in Algorithm (1) satisfy

min f (Kt) - f (K)  L R2 + R2.

t=0,...,T

T

Proof. Is an immediate extension of Theorem 5 - supplementary material, Section D.6. 4The derivation is presented in supplementary material, Claim 6.

6

Algorithm 1 IIP for SPORE

1: function APPROX-PROJ-SUBG(K0, L, R, R, T )

2: s = L * R2 + R2
T

compute stepsize

3: Initialize t0 = 1.

4: for t = 1, . . . , T do

5: compute ht-1

subgradient of f (K) at Kt-1 see equation (5)

6:

vt = Kt-1 -

s ht-1

ht-1

7: K t = IIP F IST A(vt, T )

 FISTA for T steps. Use Algorithm 1 (supp.)

8: Kt = P rojA(Kt) = Kt - proxA (Kt)

9: Kt needs to be psd for the next SVM call. Use (14) (supp.)

10: end for

11: end function

Equating the problem (8) with the SPORE problem (5), we have f (K) = C (K, yS) + 1(K).

The set of subgradients of f at the iteration t is given by f (Kt) =

-

1 2

Ytt

Y

+

vtvt |t is returned by SVM, and vt is the eigen vector corresponding to 1(Kt) 5, where Y be

a diagonal matrix such that Yii = yi, for i  S, and 0 otherwise. The step size is calculated using

estimates of L, R and R, which can be derived as L = nC2, R = n, R = n2.5 for the SPORE prob-

lem. Check the supplementary material for the derivations.

5 Multiple Graph Transduction

Multiple graph transduction is of recent interest in a multi-view setting, where individual views are expressed by a graph. This includes many practical problems in bioinformatics [17], spam detection [21], etc. We propose an MKL style extension of SPORE, with improved PAC bounds.
Formally, the problem of multiple graph transduction is stated as - let G = {G(1), . . . , G(M)} be a set of simple graphs G(k) = (V, E(k)), defined on a common vertex set V = [n]. Given G and yS as before, the goal is to accurately predict yS. Following the standard technique of taking convex combination of graph kernels [16], we propose the following MKL-SPORE formulation

C,(G, yS) = min

min C

kK(k), yS +  max 1(K(k)) .

K(k)K(G(k)) SM-1

k[M ]

k[M ]

(17)

Similar to Theorem 4, we can show the following (supplementary material, Theorem 8)

erS0-1[y] = O

1 m

1 n(G) + log 

1
2 where (G)  min (G(k)).
k[M ]

(18)

It immediately follows that combining multiple graphs improves the error convergence rate (see (6)), and hence the labelled sample complexity. Also, the bound suggests that the presence of at least one "good" graph is sufficient for MKL-SPORE to learn accurate predictions. This motivates us to use the proposed formulation in the presence of noisy graphs (Section 6). We can also apply the IIP algorithm described in Section 4 to solve for (17) (supplementary material, Section F).

6 Experiments

We conducted experiments on both real world and synthetic graphs, to illustrate our theoretical observations. All experiments were run on CPU with 2 Xeon Quad-Core processors (2.66GHz, 12MB L2 Cache) and 16GB memory running CentOS 5.3.

5t = argmaxRn+,

 C 

1

-

1 2



YKtY and vt = argmaxvRn, v =1 v

Ktv

j =0 j/S

7

Table 1: SPORE comparison.

Dataset

Un-Lap N-Lap KS SPORE

breast-cancer 88.22 93.33 92.77 96.67

diabetes

68.89 69.33 69.44 73.33

fourclass

70.00 70.00 70.44 78.00

heart

71.97 75.56 76.42 81.97

ionosphere 67.77 68.00 68.11 76.11

sonar

58.81 58.97 59.29 63.92

mnist-1vs2 75.55 80.55 79.66 85.77

mnist-3v8

76.88 81.88 83.33 86.11

mnist-4v9

68.44 72.00 72.22 74.88

Table 2: Large Scale - 2000 Nodes. Dataset Un-Lap N-Lap KS SPORE mnist-1vs2 83.80 96.23 94.95 96.72 mnist-3vs8 55.15 87.35 87.35 91.35 mnist-5vs6 96.30 94.90 92.05 97.35 mnist-1vs7 90.65 96.80 96.55 97.25 mnist-4vs9 65.55 65.05 61.30 87.40

Graph Transduction (SPORE): We use two datasets UCI [12] and MNIST [10]. For the UCI

datasets, we use the RBF kernel6 and threshold with the mean, and for the MNIST datasets we con-

struct a similarity matrix using cosine distance for a random sample of 500 nodes, and threshold

with 0.4 to obtain unweighted graphs. With 10% labelled nodes, we compare SPORE with for-

mulation (3) using graph kernels - Unnormalized Laplacian (c1I + L)-1, Normalized Laplacian

c2I

+

D-

1 2

LD-

1 2

-1 and K-Scaling [1], where L

=

D - A, D being a diagonal matrix of

degrees. We choose parameters c1, c2, C and  by cross validation. Table 1 summarizes the re-

sults, averaged over 5 different labelled samples, with each entry being accuracy in % w.r.t. 0-1 loss

function. As expected from Section 3, SPORE significantly outperforms existing methods. We also

tackle large scale graph transduction problems, Table 2 shows superior performance of Algorithm 1

for a random sample of 2000 nodes, with only 5 outer iterations and 20 inner projections.

Multiple Graph Transduction (MKL-SPORE): We illustrate the effectiveness of combining
multiple graphs, using mixture of random graphs - G(p, q), p, q  [0, 1] where we fix |V | = n = 100 and the labels y  Yn such that yi = 1 if i  n/2; -1 otherwise. An edge (i, j) is present with probability p if yi = yj; otherwise present with probability q. We generate three datasets to simulate
homogenous, heterogenous and noisy case, shown in Table 3.

Table 3: Synthetic multiple graphs dataset.

Graph Homo. Heter.

Noisy

G(1) G(0.7, 0.3) G(0.7, 0.5) G(0.7, 0.3)

G(2) G(0.7, 0.3) G(0.6, 0.4) G(0.6, 0.4)

G(3) G(0.7, 0.3) G(0.5, 0.3) G(0.5, 0.5)

Table 4: Superior performance of MKL-SPORE.

Graph

Homo. Heter. Noisy

G(1) 84.4 69.2 84.4

G(2) 84.8 68.6 68.2

G(3) 86.4 72.0 54.4

Union

85.5 69.3 69.3

Intersection

83.8 67.5 69.0

Majority

93.7 76.9 76.6

Multiple Graphs 95.6 80.6 81.9

MKL-SPORE was compared with individual graphs; and with the union, intersection and majority graphs7. We use SPORE to solve for single graph transduction, and the results were averaged over
10 random samples of 5% labelled nodes. For the comparison metric as before, Table 4 shows that
combining multiple graphs improves classification accuracy. Furthermore, the noisy case illustrates
the robustness of the proposed formulation, a key observation from (18).

7 Conclusion

We show that the class of orthonormal graph embeddings are efficiently PAC learnable. Our analysis motivates a Spectral Norm regularized formulation - SPORE for graph transduction. Using inexact proximal method, we design an efficient first order method to solve for the proposed formulation. The algorithm and analysis presented readily generalize to the multiple graphs setting.

Acknowledgments

We acknowledge support from a grant from Indo-French Center for Applied Mathematics (IFCAM).

6The (i, j)th entry of an RBF kernel is given by exp

xi-xj 2 22

, where  is set as the mean distance.

7Majority graph is a graph where an edge (i, j) is present, if a majority of the graphs have the edge (i, j).

8

References
[1] R. K. Ando and T. Zhang. Learning on graph with Laplacian regularization. In NIPS, 2007. [2] N. Balcan and A. Blum. An augmented PAC model for semi-supervised learning. In O. Chapelle,
B. Scholkopf, and A. Zien, editors, Semi-supervised learning. MIT press Cambridge, 2006. [3] J. P. Boyle and R. L. Dykstra. A Method for Finding Projections onto the Intersection of Convex Sets
in Hilbert Spaces. In Advances in Order Restricted Statistical Inference, volume 37 of Lecture Notes in Statistics, pages 28-47. Springer New York, 1986. [4] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein. Introduction to algorithms, volume 2. MIT press Cambridge, 2001. [5] M. Eisenberg-Nagy, M. Laurent, and A. Varvitsiotis. Forbidden minor characterizations for low-rank optimal solutions to semidefinite programs over the elliptope. J. Comb. Theory, Ser. B, 108:40-80, 2014. [6] A. Erdem and M. Pelillo. Graph transduction as a Non-Cooperative Game. Neural Computation, 24(3):700-723, 2012. [7] M. X. Goemans. Semidefinite programming in combinatorial optimization. Mathematical Programming, 79(1-3):143-161, 1997. [8] V. Jethava, A. Martinsson, C. Bhattacharyya, and D. P. Dubhashi. The Lovasz  function, SVMs and finding large dense subgraphs. In NIPS, pages 1169-1177, 2012. [9] R. Johnson and T. Zhang. On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning. JMLR, 8(7):1489-1517, 2007. [10] Y. LeCun and C. Cortes. The MNIST database of handwritten digits, 1998. [11] M. Leordeanu, A. Zanfir, and C. Sminchisescu. Semi-supervised learning and optimization for hypergraph matching. In ICCV, pages 2274-2281. IEEE, 2011. [12] M. Lichman. UCI machine learning repository, 2013. [13] L. Lovasz. On the shannon capacity of a graph. Information Theory, IEEE Transactions on, 25(1):1-7, 1979. [14] N. Parikh and S. Boyd. Proximal algorithms. Foundations and Trends in optimization, 1(3):123-231, 2013. [15] M. Schmidt, N. L. Roux, and F. R. Bach. Convergence rates of inexact proximal-gradient methods for convex optimization. In NIPS, pages 1458-1466, 2011. [16] R. Shivanna and C. Bhattacharyya. Learning on graphs using Orthonormal Representation is Statistically Consistent. In NIPS, pages 3635-3643, 2014. [17] L. Tran. Application of three graph Laplacian based semi-supervised learning methods to protein function prediction problem. IJBB, 2013. [18] S. Villa, S. Salzo, L. Baldassarre, and A. Verri. Accelerated and Inexact Forward-Backward Algorithms. SIAM Journal on Optimization, 23(3):1607-1633, 2013. [19] T. Zhang and R. K. Ando. Analysis of spectral kernel design based semi-supervised learning. NIPS, 18:1601, 2005. [20] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scholkopf. Learning with local and global consistency. NIPS, 16(16):321-328, 2004. [21] D. Zhou and C. J. C. Burges. Spectral clustering and transductive learning with multiple views. In ICML, pages 1159-1166. ACM, 2007.
9

