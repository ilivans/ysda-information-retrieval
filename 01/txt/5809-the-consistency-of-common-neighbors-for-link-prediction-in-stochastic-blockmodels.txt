The Consistency of Common Neighbors for Link Prediction in Stochastic Blockmodels

Purnamrita Sarkar Department of Statistics University of Texas at Austin
purnamritas@austin.utexas.edu

Deepayan Chakrabarti IROM, McCombs School of Business
University of Texas at Austin
deepay@utexas.edu

Peter Bickel Department of Statistics University of California, Berkeley bickel@stat.berkeley.edu
Abstract
Link prediction and clustering are key problems for network-structured data. While spectral clustering has strong theoretical guarantees under the popular stochastic blockmodel formulation of networks, it can be expensive for large graphs. On the other hand, the heuristic of predicting links to nodes that share the most common neighbors with the query node is much fast, and works very well in practice. We show theoretically that the common neighbors heuristic can extract clusters with high probability when the graph is dense enough, and can do so even in sparser graphs with the addition of a "cleaning" step. Empirical results on simulated and real-world data support our conclusions.

1 Introduction
Networks are the simplest representation of relationships between entities, and as such have attracted significant attention recently. Their applicability ranges from social networks such as Facebook, to collaboration networks of researchers, citation networks of papers, trust networks such as Epinions, and so on. Common applications on such data include ranking, recommendation, and user segmentation, which have seen wide use in industry. Most of these applications can be framed in terms of two problems: (a) link prediction, where the goal is to find a few similar nodes to a given query node, and (b) clustering, where we want to find groups of similar individuals, either around a given seed node or a full partitioning of all nodes in the network.
An appealing model of networks is the stochastic blockmodel, which posits the existence of a latent cluster for each node, with link probabilities between nodes being simply functions of their clusters. Inference of the latent clusters allows one to solve both the link prediction problem and the clustering problem (predict all nodes in the query node's cluster). Strong theoretical and empirical results have been achieved by spectral clustering, which uses the singular value decomposition of the network followed by a clustering step on the eigenvectors to determine the latent clusters.
However, singular value decomposition can be expensive, particularly for (a) large graphs, when (b) many eigenvectors are desired. Unfortunately, both of these are common requirements. Instead, many fast heuristic methods are often used, and are empirically observed to yield good results [8]. One particularly common and effective method is to predict links to nodes that share many "common neighbors" with the query node q, i.e., rank nodes by |CN (q, i)|, where CN (q, i) = {u | q  u  i} (i  j represents an edge between i

1

and j). The intuition is that q probably has many links with others in its cluster, and hence probably also shares many common friends with others in its cluster. Counting common neighbors is particularly fast (it is a Join operation supported by all databases and Map-Reduce systems). In this paper, we study the theoretical properties of the common neighbors heuristic.
Our contributions are the following:
(a) We present, to our knowledge the first, theoretical analysis of the common neighbors for the stochastic blockmodel.
(b) We demarcate two regimes, which we call semi-dense and semi-sparse, under which common neighbors can be successfully used for both link prediction and clustering.
(c) In particular, in the semi-dense regime, the number of common neighbors between the query node q and another node within its cluster is significantly higher than that with a node outside its cluster. Hence, a simple threshold on the number of common neighbors suffices for both link prediction and clustering.
(d) However, in the semi-sparse regime, there are too few common neighbors with any node, and hence the heuristic does not work. However, we show that with a simple additional "cleaning" step, we regain the theoretical properties shown for the semi-dense case.
(e) We empirically demonstrate the effectiveness of counting common neighbors followed by the "cleaning" post-process on a variety of simulated and real-world datasets.
2 Related Work
Link prediction has recently attracted a lot of attention, because of its relevance to important practical problems like recommendation systems, predicting future connections in friendship networks, better understanding of evolution of complex networks, study of missing or partial information in networks, etc [9, 8]. Algorithms for link prediction fall into two main groups: similarity-based, and model-based.
Similarity-based methods: These methods use similarity measures based on network topology for link prediction. Some methods look at nodes two hops away from the query node: counting common neighbors, the Jaccard index, the Adamic-Adar score [1] etc. More complex methods include nodes farther away, such as the Katz score [7], and methods based on random walks [16, 2]. These are often intuitive, easily implemented, and fast, but they typically lack theoretical guarantees.
Model-based methods: The second approach estimates parametric models for predicting links. Many popular network models fall in the latent variable model category [12, 3]. These models assign n latent random variables Z := (Z1, Z2, . . . , Zn) to n nodes in a network. These variables take values in a general space Z. The probability of linkage between two nodes is specified via a symmetric map h : Z x Z  [0, 1]. These Zi's can be i.i.d Uniform(0,1) [3], or positions in some d-dimensional latent space [12]. In [5] a mixture of multivariate Gaussian distributions is used, each for a separate cluster. A Stochastic Blockmodel [6] is a special class of these models, where Zi is a binary length k vector encoding membership of a node in a cluster. In a well known special case (the planted partition model), all nodes in the same cluster connect to each other with probability , whereas all pairs in different clusters connect with probability . In fact, under broad parameter regimes, the blockmodel approximation of networks has recently been shown to be analogous to the use of histograms as non-parametric summaries of an unknown probability distribution [11]. Varying the number of bins or the bandwidth corresponds to varying the number or size of communities. Thus blockmodels can be used to approximate more complex models (under broad smoothness conditions) if the number of blocks are allowed to increase with n.
Empirical results: As the models become more complex, they also become computationally demanding. It has been commonly observed that simple and easily computable measures like common neighbors often have competitive performance with more complex methods.
2

This behavior has been empirically established across a variety of networks, starting from co-authorship networks [8] to router level internet connections, protein protein interaction networks and electrical power grid network [9].
Theoretical results: Spectral clustering has been shown to asymptotically recover cluster memberships for variations of Stochastic Blockmodels [10, 4, 13]. However, apart from [15], there is little understanding of why simple methods such as common neighbors perform so well empirically.
Given their empirical success and computational tractability, the common neighbors heuristic is widely applied for large networks. Understanding the reasons for the accuracy of common neighbors under the popular stochastic blockmodel setting is the goal of our work.
3 Proposed Work
Many link prediction methods ultimately make two assumptions: (a) each node belongs to a latent "cluster", where nodes in the same cluster have similar behavior; and (b) each node is very likely to connect to others in its cluster, so link prediction is equivalent to finding other nodes in the cluster. These assumptions can be relaxed: instead of belonging to the same cluster, nodes could have "topic distributions", with links being more likely between pairs of nodes with similar topical interests. However, we will focus on the assumptions stated above, since they are clean and the relaxations appear to be fundamentally similar.
Model. Specifically, consider a stochastic blockmodel where each node i belongs to an unknown cluster ci  {C1, . . . , CK }. We assume that the number of clusters K is fixed as the number of nodes n increases. We also assume that each cluster has  = n/K members, though this can be relaxed easily. The probability P (i  j) of a link between nodes i and j (i = j) depends only on the clusters of i and j: P (i  j) = Bci,cj {ci = cj} + {ci = cj} for some  >  > 0; in other words, the probability of a link is  between nodes in the same cluster, and  otherwise. By definition, P (i  i) = 0. If the nodes were arranged so that all nodes in a cluster are contiguous, then the corresponding matrix, when plotted, attains a block-like structure, with the diagonal blocks (corresponding to links within a cluster) being denser than off-diagonal blocks (since  > ).
Under these assumptions, we ask the following two questions:
Problem 1 (Link Prediction and Recommendation). Given node i, how can we identify at least a constant number of nodes from ci? Problem 2 (Local Cluster Detection). Given node i, how can we identify all nodes in ci?
Problem 1 can be considered as the problem of finding good recommendations for a given node i. Here, the goal is to find a few good nodes that i could connect to (e.g., recommending a few possible friends on Facebook, or a few movies to watch next on Netflix). Since withincluster links have higher probability than across-cluster links ( > ), predicting nodes from ci gives the optimal answer. Crucially, it is unnecessary to find all good nodes. As against that, Problem 2 requires us to find everyone in the given node's cluster. This is the problem of detecting the entire cluster corresponding to a given node. Note that Problem 2 is clearly harder than Problem 1.
We next present a summary of our results and the underlying intuition before delving into the details.
3.1 Intuition and Result Summary
Current approaches. Standard approaches to inference for the stochastic blockmodel attempt to solve an even harder problem:
Problem 3 (Full Cluster Detection). How can we identify the latent clusters ci for all i?
A popular solution is via spectral clustering, involving two steps: (a) computing the top-K eigenvectors of the graph Laplacian, and (b) clustering the projections of each node on the
3

corresponding eigenspace via an algorithm like k-means[13]. A slight variation of this has been shown to work as long as ( - )/  = (log n/ n) and the average degree grows faster than poly-logarithmic powers of n [10].
However, (a) spectral clustering solves a harder problem than Problems 1 and 2, and (b) eigen-decompositions can be expensive, particularly for very large graphs. Our claim is that a simpler operation -- counting common neighbors between nodes -- can yield results that are almost as good in a broad parameter regime.
Common neighbors. Given a node i, link prediction via common neighbors follows a simple prescription: predict a link to node j such that i and j have the maximum number |CN (i, j)| of shared friends CN (i, j) = {u | i  u  j}. The usefulness of common neighbors have been observed in practice [8] and justified theoretically for the latent distance model [15]. However, its properties under the stochastic blockmodel remained unknown.
Intuitively, we would expect a pair of nodes i and j from the same cluster to have many common neighbors u from the same cluster, since both the links i  u and u  j occur with probability , whereas for ci = cj, at least one of the edges i  u and u  j must have the lower probability .
P (u  CN (i, j) | ci = cj) = 2P (cu = ci | ci = cj) + 2P (cu = ci | ci = cj) = 2 + (1 - )2
P (u  CN (i, j) | ci = cj) = P (cu = ci or cu = cj | ci = cj) + 2P (cu = ci, cu = cj | ci = cj) = 2 + (1 - 2)2 = P (u  CN (i, j) | ci = cj) - ( - )2  P (u  CN (i, j) | ci = cj)
Thus the expected number of common neighbors E [|CN (i, j)|] is higher when ci = cj. If we can show that the random variable CN (i, j) concentrates around its expectation, node pairs with the most common neighbors would belong to the same cluster. Thus, common neighbors would offer a good solution to Problem 1.
We show conditions under which this is indeed the case. There are three key points regarding our method: (a) handling dependencies between common neighbor counts, (b) defining the graph density regime under which common neighbors is consistent, and (c) proposing a variant of common neighbors which significantly broadens this region of consistency.
Dependence. CN (i, j) and CN (i, j ) are dependent; hence, distinguishing between within-group and outside-group nodes can be complicated even if each CN (i, j) concentrates around its expectation. We handle this via a careful conditioning step.
Dense versus sparse graphs. In general, the parameters  and  can be functions of n, and we can try to characterize parameter settings when common neighbors consistently returns nodes from the same cluster as the input node. We show that when the graph is sufficiently "dense" (average degree is growing faster than n log n), common neighbors is powerful enough to answer Problem 2. Also, ( - )/ can go to zero at a suitable rate.
On the other hand, the expected number of common neighbors between nodes tends to zero for sparser graphs, irrespective of whether the nodes are in the same cluster or not. Further, the standard deviation is of a higher order than the expectation, so there is no concentration. In this case, counting common neighbors fails, even for Problem 1.
A variant with better consistency properties. However, we show that the addition of an extra post-processing step (henceforth, the "cleaning" step) still enables common neighbors to identify nodes from its own cluster, while reducing the number of off-cluster nodes to zero with probability tending to one as n  . This requires a stronger separation condition between  and . However, such "strong consistency" is only possible when the average degree grows faster than (n log n)1/3. Thus, the cleaning step extends the consistency of common neighbors beyond the O(1/ n) range.
4

4 Main Results
We first split the edge set of the complete graph on n nodes into two sets: K1 and its complement K2 (independent of the given graph G). We compute common neighbors on G1 = G  K1 and perform a "cleaning" process on G2 = G  K2. The adjacency matrices of G1 and G2 are denoted by A1 and A2. We will fix a reference node q, which belongs to class C1 without loss of generality (recall that there are K clusters C1 . . . CK , each of size n).
Let Xi(i = q) denote the number of common neighbors between q and i. Algorithm 1 computes the set S = {i : Xi  tn} of nodes who have at least tn common neighbors with q on A1, whereas Algorithm 2 does a further degree thresholding on A2 to refine S into S1.
Algorithm 1 Common neighbors screening algorithm
1: procedure Scan(A1, q, tn) 2: For 1  i  n, Xi  A21(q, i) 3: Xq  0 4: S  {i : Xi  tn} 5: return S

Algorithm 2 Post Selection Cleaning algorithm
1: procedure Clean(S, A2, q, sn) 2: S1  {i : jS A2(i, j)  sn} 3: return S1

To analyze the algorithms, we must specify conditions on graph densities. Recall that  and  represent within-cluster and across-cluster link probabilities. We assume that / is constant while   0,   0; equivalently, assume that both  and  are both some constant times , where   0.
The analysis of graphs has typically been divided into two regimes. The dense regime consists of graphs with n  , where the expected degree n is a fraction of n as n grows. In the sparse regime, n = O(1), so degree is roughly constant. Our work explores a finer gradation, which we call semi-dense and semi-sparse, defined next.
Definition 4.1 (Semi-dense graph). A sequence of graphs is called semi-dense if n2/ log n   as n  .
Definition 4.2 (Semi-sparse graph). A sequence of graphs is called semi-sparse if n2  0 but n2/3/ log n   as n  .

Our first result is that common neighbors is enough to solve not only the link-prediction problem (Problem 1) but also the local clustering problem (Problem 2) in the semi-dense case. This is because even though both nodes within and outside the query node's cluster have a growing number of common neighbors with q, there is a clear distinction in the expected number of common neighbors between the two classes. Also, since the standard deviation is of a smaller order than the expectation, the random variables concentrate. Thus, we can pick a threshold tn such that SCAN(A1, q, tn) yields just the nodes in the same cluster as q with high probability. Note that the cleaning step (Algorithm 2) is not necessary in this case.

Theorem 4.1 (Algorithm 1 solves Problem 2 in semi-dense graphs). Let tn =

n ( + )2/2 + (1 - 2)2 . Let S be the set of nodes returned by SCAN(A1, q, tn). Let

nw and no denote the number of nodes in S  C1 and S \ C1 respectively. If the graph is

semi-dense,

and

if

- 



2 

1/4

log n n2

, then P (nw = n)  1 and P (no = 0)  1.

Proof Sketch. We only sketch the proof here, deferring details to the supplementary material. Let dqa = iCa A1(q, i) be the number of links from the query node q to nodes in

5

cluster Ca. Let dq = {dq1, . . . qqK } and d = a dqa. We first show that

P (dq  Good) P

dq1  n(1  n) dqa  n(1  n) a = 1



1

-

K n2

,

(1)

n (6 log n)/(n) = log n/n * ( log n/(n2))  0.

(2)

Conditioned on dq, Xi is the sum of K Binomial(dqa, B1a) independent random variables representing the number of common neighbors between q and i via nodes in each of the K clusters: E[Xi | dq, i  Ca] = dqa + (d - dqa). We have:
1 E[Xi | dq  Good, i  C1]  n 2 + (1 - )2 (1 - n) n(1 - n)

a E[Xi | dq  Good, i  Ca, a = 1]  n 2 + (1 - 2)2 (1 + n) un(1 + n)

Note that tn = ( n +un)/2, un  tn  n, and n -un = n(-)2  4 log n n2/ log n  , where we applied condition on ( - )/ noted in the theorem statement. We show:

P (Xi  tn | dq  Good, i  C1)  n-4/3+o(1)

P (Xi  tn | dq  Good, i  Ca, a = 1)  n-4/3+o(1)

Conditioned on dq, both nw and no are sums of conditionally independent and identically distributed Bernoullis.

P (nw = n)  P (dq  Good)P (nw = n | dq  Good) 

K 1 - n2

* (1 - n * n-4/3)  1

P (no = 0)  P (dq  Good) * P (no = 0 | dq  Good)  1 - (n-1/3)  1

There are two major differences between the semi-sparse and semi-dense cases. First, in the semi-sparse case, both expectations 1 and a are of the order O(n2) which tends to zero. Second, standard deviations on the number of common neighbors are of a larger order than expectations. Together, this means that the number of common neighbors to within-cluster and outside-cluster nodes can no longer be separated; hence, Algorithm 1 by itself cannot work. However, after cleaning, the entire cluster of the query node q can still be recovered.
Theorem 4.2 (Algorithm 1 followed by Algorithm 2 solves Problem 2 in semi-sparse graphs). Let tn = 1 and sn = n2 ( + (1 - ))2 ( + )/2. Let S = Scan(A1, q, tn) and S1 = Clean(S, A2, q, sn). Let n(wc) n(oc) denote the number of nodes in S1  C1
(S1 \ C1). If the graph is semi-sparse, and   3(1 - ), then P n(wc) = n  1 and
P n(oc) = 0  1.

Proof Sketch. We only sketch the proof here, with details being deferred to the supplementary material. The degree bounds of Eq. 1 and the equations for E[Xi|dq  Good] hold even in the semi-sparse case. We can also bound the variances of Xi (which are sums of conditionally independent Bernoullis):
var[Xi | dq  Good, i  C1]  E[Xi | dq  Good, i  C1] = 1

Since the expected number of common neighbors vanishes and the standard deviation is an order larger than the expectation, there is no hope for concentration; however, there are slight differences in the probability of having at least one common neighbor.

First, by an application of the Paley-Zygmund inequality, we find:

p1 P (Xi  1 | dq  Good, i  C1)



var(Xi

|

dq



E[Xi | Good, i

dq  Good, i  C1) + E[Xi

 C1]2 | dq 

Good, i



C1]2



12 1 + 12



n(1 - n)(1 - 1)

since 1  0

6

For a > 1, Markov's inequality gives:
pa P (Xi  1 | dq  Good, i  Ca, a = 1)  E[Xi | dq  Good, i  Ca, a = 1] = a
Even though pa  0, npa = (n22)  , so we can use concentration inequalities like the Chernoff bound again to bound nw and no.
P (nw  np1(1 - 6 log n/np1))  1 - n-4/3 P (no  n(1 - )pa(1 + 6 log n/n(1 - )pa))  1 - n-4/3

Unlike the denser regime, nw and no can be of the same order here. Hence, the candidate set S returned by thresholding the common neighbors has a non-vanishing fraction of nodes from outside q's community. However, this fraction is relatively small, which is what we would exploit in the cleaning step.

Let w and o denote the expected number of edges in A2 from a node to S. The separation condition in the theorem statement gives w - o  4 w log n. Setting the degree threshold sn = (w + o)/2, we bound the probability of mistakes in the cleaning step:

P (i  C1 s.t.

A2(i, j)  sn | dq  Good)  n-1/3+o(1)

jS

P (i  C1 s.t.

A2(i, j)  sn | dq  Good)  n-1/3+o(1)

jS

Removing the conditioning on dq  Good (as in Theorem 4.1) yields the desired result.

5 Experiments

We present our experimental results in two parts. First, we use simulations to support our theoretical claims. Next we present link prediction accuracies on real world collaborative networks to show that common neighbors indeed perform close to gold standard algorithms like spectral clustering and the Katz score.
Implementation details: Recall that our algorithms are based on thresholding. When there is a large gap between common neighbors between node q and nodes in its cluster (e.g., in the semi-dense regime), this is equivalent to using the k-means algorithm with k = 2 to find S in Algorithm 1. The same holds for finding S1 in algorithm 2. When the number of nodes with more than two common neighbors is less than ten, we define the set S by finding all neighbors with at least one common neighbor (as in the semi-sparse regime). On the other hand, since the cleaning step works only when S is sufficiently large (so that degrees concentrate), we do not perform any cleaning when |S| < 30. While we used the split sample graph A2 in the cleaning step for ease of analysis, we did the cleaning using the same network in the experiments.
Experimental setup for simulations: We use a stochastic blockmodel of 2000 nodes split into 4 equal-sized clusters. For each value of (, ) we pick 50 query nodes at random, and calculate the precision and recall of the result against nodes from the query node's cluster (for any subset S and true cluster C, precision = |S  C|/|S| and recall = |S  C|/|C|). We report mean precision and recall over 50 random generated graph instances.
Accuracy on simulated data: Figure 1 shows the precision and recall as degree grows, with the parameters (, ) satisfying the condition   3(1 - ) of Thm. 4.2. We see that cleaning helps both precision and recall, particularly in the medium-degree range (the semi-sparse regime). As a reference, we also plot the precision of spectral clustering, when it was given the correct number of clusters (K = 4). Above average degree of 10, spectral clustering gives perfect precision, whereas common neighbors can identify a large fraction of the true cluster once average degree is above 25. On the other hand, for average degree less than seven, spectral clustering performs poorly, whereas the precision of common neighbors is remarkably higher. Precision is relatively higher than recall for a broad degree regime, and this explains why common neighbors are a popular choice for link prediction. On a side

7

(A) (B)

Figure 1: Recall and precision versus average degree: When degree is very small, none of the methods work well. In the medium-degree range (semi-sparse regime), we see that common neighbors gets increasingly better precision and recall, and cleaning helps. With high enough degrees (semi-dense regime), just common neighbors is sufficient and gets excellent accuracy.

Dataset
HepTH Citeseer
NIPS

n
5969 4520 1222

Table 1: AUC scores for co-authorship networks

Mean degree Time-steps

AUC

CN CN-clean SPEC

4

6

.70 .74

.82

5 11 .88 .89 .89

3.95

9

.63 .69

.68

Katz .82 .95 .78

Random .49 .52 .47

note, it is not surprising that in a very sparse graph common neighbors cannot identify the whole cluster, since not everyone can be reached in two hops.
Accuracy on real-world data: We used publicly available co-authorship datasets over time where nodes represent authors and an edge represents a collaboration between two authors. In particular, we used subgraphs of the High Energy Physics (HepTH) coauthorship dataset (6 timesteps), the NIPS dataset (9 timesteps) and the Citeseer dataset (11 timesteps). We obtain the training graph by merging the first T-2 networks, use the T-1th step for cross-validation and use the last timestep as the test graph. The number of nodes and average degrees are reported in Table 1. We merged 1-2 years of papers to create one timestep (so that the median degree of the test graph is at least 1).
We compare our algorithm (CN and CN-clean) with the Katz score which is used widely in link prediction [8] and spectral clustering of the network. Spectral clustering is carried out on the giant component of the network. Furthermore, we cross-validate the number of clusters using the held out graph. Our setup is very similar to link prediction experiments in related literature [14].
Since these datasets are unlabeled, we cannot calculate precision or recall as before. Instead for any score or affinity measure, we propose to perform link prediction experiments as follows. For a randomly picked node we calculate the score from the node to everyone else. We compute the AUC score of this vector against the edges in the test graph. We report the average AUC for 100 randomly picked nodes. Table 1 shows that even in sparse regimes common neighbors performs similar to benchmark algorithms.
6 Conclusions
Counting common neighbors is a particularly useful heuristic: it is fast and also works well empirically. We prove the effectiveness of common neighbors for link prediction as well as local clustering around a query node, under the stochastic blockmodel setting. In particular, we show the existence of a semi-dense regime where common neighbors yields the right cluster w.h.p, and a semi-sparse regime where an additional "cleaning" step is required. Experiments with simulated as well as real-world datasets shows the efficacy of our approach, including the importance of the cleaning step.

8

References
[1] L. Adamic and E. Adar. Friends and neighbors on the web. Social Networks, 25:211-230, 2003.
[2] L. Backstrom and J. Leskovec. Supervised random walks: Predicting and recommending links in social networks. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining, pages 635-644, New York, NY, USA, 2011. ACM.
[3] P. J. Bickel and A. Chen. A nonparametric view of network models and newman girvan and other modularities. Proceedings of the National Academy of Sciences of the Unites States of America, 106(50):2106821073, 2009.
[4] K. Chaudhuri, F. C. Graham, and A. Tsiatas. Spectral clustering of graphs with general degrees in the extended planted partition model. Journal of Machine Learning Research - Proceedings Track, 23:35.1-35.23, 2012.
[5] M. S. Handcock, A. E. Raftery, and J. M. Tantrum. Model-based clustering for social networks. Journal of the Royal Statistical Society: Series A (Statistics in Society), 170(2):301-354, 2007.
[6] P. W. Holland, K. Laskey, and S. Leinhardt. Stochastic blockmodels: First steps. Social Networks, 5(2):109-137, 1983.
[7] L. Katz. A new status index derived from sociometric analysis. In Psychometrika, volume 18, pages 39-43, 1953.
[8] D. Liben-Nowell and J. Kleinberg. The link prediction problem for social networks. In Conference on Information and Knowledge Management. ACM, 2003.
[9] L. Lu and T. Zhou. Link prediction in complex networks: A survey. Physica A, 390(6):11501170, 2011.
[10] F. McSherry. Spectral partitioning of random graphs. In FOCS, pages 529-537, 2001. [11] S. C. Olhede and P. J. Wolfe. Network histograms and universality of blockmodel
approximation. Proceedings of the National Academy of Sciences of the Unites States of America, 111(41):14722-14727, 2014. [12] A. E. Raftery, M. S. Handcock, and P. D. Hoff. Latent space approaches to social network analysis. Journal of the American Statistical Association, 15:460, 2002. [13] K. Rohe, S. Chatterjee, and B. Yu. Spectral clustering and the high-dimensional stochastic blockmodel. Annals of Statistics, 39:1878-1915, 2011. [14] P. Sarkar and P. J. Bickel. Role of normalization in spectral clustering for stochastic blockmodels. To appear in the Annals of Statistics., 2014. [15] P. Sarkar, D. Chakrabarti, and A. Moore. Theoretical justification of popular link prediction heuristics. In Conference on Learning Theory. ACM, 2010. [16] P. Sarkar and A. Moore. A tractable approach to finding closest truncated-commutetime neighbors in large graphs. In Proc. UAI, 2007.
9

