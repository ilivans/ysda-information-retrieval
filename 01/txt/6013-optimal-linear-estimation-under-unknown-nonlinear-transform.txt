Optimal Linear Estimation under Unknown Nonlinear Transform

Xinyang Yi The University of Texas at Austin
yixy@utexas.edu
Constantine Caramanis The University of Texas at Austin constantine@utexas.edu

Zhaoran Wang Princeton University zhaoran@princeton.edu
Han Liu Princeton University hanliu@princeton.edu

Abstract
Linear regression studies the problem of estimating a model parameter   Rp, from n observations {(yi, xi)}ni=1 from linear model yi = xi,  + i. We consider a significant generalization in which the relationship between xi,  and yi is noisy, quantized to a single bit, potentially nonlinear, noninvertible, as well as unknown. This model is known as the single-index model in statistics, and, among other things, it represents a significant generalization of one-bit compressed sensing. We propose a novel spectral-based estimation procedure and show that we can recover  in settings (i.e., classes of link function f ) where previous algorithms fail. In general, our algorithm requires only very mild restrictions on the (unknown) functional relationship between yi and xi,  . We also consider the high dimensional setting where  is sparse, and introduce a two-stage nonconvex framework that addresses estimation challenges in high dimensional regimes where p n. For a broad class of link functions between xi,  and yi, we establish minimax lower bounds that demonstrate the optimality of our estimators in both the classical and high dimensional regimes.

1 Introduction

We consider a generalization of the one-bit quantized regression problem, where we seek to recover the regression coefficient   Rp from one-bit measurements. Specifically, suppose that X is a random vector in Rp and Y is a binary random variable taking values in {-1, 1}. We assume the
conditional distribution of Y given X takes the form

P(Y

= 1|X

= x) =

1 f(
2

x, 

)+

1 ,
2

(1.1)

where f : R  [-1, 1] is called the link function. We aim to estimate  from n i.i.d. observations

{(yi, xi)}ni=1 of the pair (Y, X). In particular, we assume the link function f is unknown. Without any loss of generality, we take  to be on the unit sphere Sp-1 since its magnitude can always be

incorporated into the link function f .

The model in (1.1) is simple but general. Under specific choices of the link function f , (1.1) immediately leads to many practical models in machine learning and signal processing, including logistic regression and one-bit compressed sensing. In the settings where the link function is assumed to be known, a popular estimation procedure is to calculate an estimator that minimizes a certain loss

1

function. However, for particular link functions, this approach involves minimizing a nonconvex objective function for which the global minimizer is in general intractable to obtain. Furthermore, it is difficult or even impossible to know the link function in practice, and a poor choice of link function may result in inaccurate parameter estimation and high prediction error. We take a more general approach, and in particular, target the setting where f is unknown. We propose an algorithm that can estimate the parameter  in the absence of prior knowledge on the link function f . As our results make precise, our algorithm succeeds as long as the function f satisfies a single moment condition. As we demonstrate, this moment condition is only a mild restriction on f . In particular, our methods and theory are widely applicable even to the settings where f is non-smooth, e.g., f (z) = sign(z), or noninvertible, e.g., f (z) = sin(z).

In particular, as we show in 2, our restrictions on f are sufficiently flexible so that our results provide a unified framework that encompasses a broad range of problems, including logistic regression, one-bit compressed sensing, one-bit phase retrieval as well as their robust extensions. We use these important examples to illustrate our results, and discuss them at several points throughout the paper.

Main contributions. The key conceptual contribution of this work is a novel use of the method of moments. Rather than considering moments of the covariate, X, and the response variable, Y , we look at moments of differences of covariates, and differences of response variables. Such a simple yet critical observation enables everything that follows and leads to our spectral-based procedure.

We also make two theoretical contributions. First, we simultaneously establish the statistical and computational rates of convergence of the proposed spectral algorithm. We consider both the low dimensional setting where the number of samples exceeds the dimension and the high dimensional setting where the dimensionality may (greatly) exceed the number of samples. In both these settings, our proposed algorithm achieves the same statistical rate of convergence as that of linear regression applied on data generated by the linear model without quantization. Second, we provide minimax lower bounds for the statistical rate of convergence, and thereby establish the optimality of our procedure within a broad model class. In the low dimensional setting, our results obtain the optimal rate with the optimal sample complexity. In the high dimensional setting, our algorithm requires estimating a sparse eigenvector, and thus our sample complexity coincides with what is believed to be the best achievable via polynomial time methods [2]; the error rate itself, however, is informationtheoretically optimal. We discuss this further in 3.4.

Related works. Our model in (1.1) is close to the single-index model (SIM) in statistics. In the SIM,

we assume that the response-covariate pair (Y, X) is determined by

Y = f( X,  ) + W

(1.2)

with unknown link function f and noise W . Our setting is a special case of this, as we restrict Y to be a binary random variable. The single index model is a classical topic, and therefore there is extensive literature - too much to exhaustively review it. We therefore outline the pieces of work most relevant to our setting and our results. For estimating  in (1.2), a feasible approach is M -estimation [8, 9, 12], in which the unknown link function f is jointly estimated using nonparametric estimators. Although these M -estimators have been shown to be consistent, they are not computationally efficient since they involve solving a nonconvex optimization problem. Another approach to estimate  is named the average derivative estimator (ADE; [24]). Further improvements of ADE are considered in [13, 22]. ADE and its related methods require that the link function f is at least differentiable, and thus excludes important models such as one-bit compressed sensing with f (z) = sign(z). Beyond estimating , the works in [15, 16] focus on iteratively estimating a function f and vector  that are good for prediction, and they attempt to control the generalization error. Their algorithms are based on isotonic regression, and are therefore only applicable when the link function is monotonic and satisfies Lipschitz constraints. The work discussed above focuses on the low dimensional setting where p n. Another related line of works is sufficient dimension reduction, where the goal is to find a subspace U of the input space such that the response Y only depends on the projection U X. Single-index model and our problem can be regarded as special cases of this problem as we are primarily interested in recovering a one-dimensional subspace. Due to space limit, we refer readers to the long version of this paper for a detailed survey [29].

2

In the high dimensional regime with p n and  has some structure (for us this means sparsity), we note there exists some recent progress [1] on estimating f via PAC Bayesian methods. In the special case when f is linear function, sparse linear regression has attracted extensive study over the years. The recent work by Plan et al. [21] is closest to our setting. They consider the setting of normal covariates, X  N (0, Ip), and they propose a marginal regression estimator for estimating , that, like our approach, requires no prior knowledge about f . Their proposed algorithm relies on the assumption that EzN (0,1) zf (z) = 0, and hence cannot work for link functions that are even. As we will describe below, our algorithm is based on a novel moment-based estimator, and avoids requiring such a condition, thus allowing us to handle even link functions under a very mild moment restriction, which we describe in detail below. Generally, the work in [21] requires different conditions, and thus beyond the discussion above, is not directly comparable to the work here. In cases where both approaches apply, the results are minimax optimal.

2 Example models

In this section, we discuss several popular (and important) models in machine learning and signal processing that fall into our general model (1.1) under specific link functions. Variants of these models have been studied extensively in the recent literature. These examples trace through the paper, and we use them to illustrate the details of our algorithms and results.

Logistic regression. In logistic regression (LR), we assume that P(Y = 1|X = x) =

1 1+exp (- x,

-) , where 

is the intercept.

The

link function

corresponds to f (z)

=

exp exp

(z+)-1 (z+)+1

.

One robust variant of LR is called flipped logistic regression, where we assume that the labels

Y generated from standard LR model are flipped with probability pe, i.e., P(Y = 1|X = x) =

1-pe 1+exp (- x,

-)

+

pe 1+exp ( x,

+) .

This

reduces

to

the

standard

LR

model

when

pe

=

0.

For

flipped LR, the link function f can be written as

f (z)

=

exp (z exp (z

+ +

) - 1 ) + 1

+ 2pe

*

1- 1+

exp (z exp (z

+ +

) .
)

(2.1)

Flipped LR has been studied by [19, 25]. In both papers, estimating  is based on minimizing some

surrogate loss function involving a certain tuning parameter connected to pe. However, pe is unknown in practice. In contrast to their approaches, our method does not hinge on the unknown parameter pe.

Our approach has the same formulation for both standard and flipped LR, thus unifies the two models.

One-bit compressed sensing. One-bit compressed sensing (CS) aims at recovering sparse signals

from quantized linear measurements (see e.g., [11, 20]). In detail, we define B0(s, p) := {  Rp :

| supp()|  s} as the set of sparse vectors in Rp with at most s nonzero elements. We assume

(Y, X)  {-1, 1} x Rp satisfies

Y = sign( X,  ),

(2.2)

where   B0(s, p). In this paper, we also consider its robust version with noise , i.e., Y = sign( X,  + ). Assuming  N (0, 2), the link function f of robust 1-bit CS thus corresponds

to

f (z) = 2   1 e-(u-z)2/22 du - 1.

(2.3)

0 2

Note that (2.2) also corresponds to the probit regression model without the sparse constraint on .

Throughout the paper, we do not distinguish between the two model names. Model (2.2) is referred to as one-bit compressed sensing even in the case where  is not sparse.

One-bit phase retrieval. The goal of phase retrieval (e.g., [5]) is to recover signals based on linear measurements with phase information erased, i.e., pair (Y, X)  R x Rp is determined by equation Y = | X,  |. Analogous to one-bit compressed sensing, we consider a new model named one-bit
phase retrieval where the linear measurement with phase information erased is quantized to one bit. In detail, pair (Y, X)  {-1, 1} x Rp is linked through Y = sign(| X,  | - ), where  is the
quantization threshold. Compared with one-bit compressed sensing, this problem is more difficult because Y only depends on  through the magnitude of X,  instead of the value of X,  .
Also, it is more difficult than the original phase retrieval problem due to the additional quantization.

3

Using our general model, The link function thus corresponds to f (z) = sign(|z| - ).
It is worth noting that, unlike previous models, here f is neither odd nor monotonic.

(2.4)

3 Main results
We now turn to our algorithms for estimating  in both low and high dimensional settings. We first introduce a second moment estimator based on pairwise differences. We prove that the eigenstructure of the constructed second moment estimator encodes the information of . We then propose algorithms to estimate  based upon this second moment estimator. In the high dimensional setting where  is sparse, computing the top eigenvector of our pairwise-difference matrix reduces to computing a sparse eigenvector. Beyond algorithms, we discuss minimax lower bound in 3.5. We present simulation results in 3.6

3.1 Conditions for success

We now introduce several key quantities, which allow us to state precisely the conditions required for the success of our algorithm.

Definition 3.1. For any (unknown) link function, f , define the quantity (f ) as follows:

(f ) := 21 - 02 + 20.

(3.1)

where 0, 1 and 2 are given by k := E f (Z)Zk , k = 0, 1, 2 . . . ,

(3.2)

where Z  N (0, 1).

As we discuss in detail below, the key condition for success of our algorithm is (f ) = 0. As we
show below, this is a relatively mild condition, and in particular, it is satisfied by the three examples
introduced in 2. For odd and monotonic f , (f ) > 0 unless f (z) = 0 for all z in which case no algorithm is able to recover . For even f , we have 1 = 0. Thus (f ) = 0 if and only if 0 = 2.

3.2 Second moment estimator

We describe a novel moment estimator that enables our algorithm. Let {(yi, xi)}ni=1 be the n i.i.d. observations of (Y, X). Assuming without loss of generality that n is even, we consider the following
key transformation

yi := y2i - y2i-1, xi := x2i - x2i-1,

(3.3)

for i = 1, 2, ..., n/2. Our procedure is based on the following second moment

n/2

2 M :=
n

yi2xixi  Rpxp.

i=1

(3.4)

The intuition behind this second moment is as follows. By (1.1), the variation of X along the direction  has the largest impact on the variation of X,  . Thus, the variation of Y directly depends

on the variation of X along . Consequently, {(yi, xi)}ni=/12 encodes the information of such a dependency relationship. In the following, we make this intuition more rigorous by analyzing the eigenstructure of E(M) and its relationship with .

Lemma 3.2. For   Sp-1, we assume that (Y, X)  {-1, 1} x Rp satisfies (1.1). For X 

N (0, Ip), we have

E(M) = 4(f ) *  + 4(1 - 20) * Ip,

(3.5)

where 0 and (f ) are defined in (3.2) and (3.1).

Lemma 3.2 proves that  is the leading eigenvector of E(M) as long as the eigengap (f ) is positive. If instead we have (f ) < 0, we can use a related moment estimator which has analogous properties.

4

To this end, define M

:=

2 n

n/2 i=1

(y2i

+

y2i-1)2xixi

.

In

parallel

to

Lemma

3.2,

we

have

a

similar result for M as stated below.

Corollary 3.3. Under the setting of Lemma 3.2, E(M ) = -4(f ) *  + 4(1 + 20) * Ip.

Corollary 3.3 therefore shows that when (f ) < 0, we can construct another second moment estimator M such that  is the leading eigenvector of E(M ). As discussed above, this is precisely the setting for one-bit phase retrieval when the quantization threshold in (3.1) satisfies  < m. For simplicity of the discussion, hereafter we assume that (f ) > 0 and focus on the second moment estimator M
defined in (3.4).

A natural question to ask is whether (f ) = 0 holds for specific models. The following lemma demonstrates exactly this, for the example models introduced in 2.

Lemma 3.4. (a) Consider the flipped logistic regression where f is given in (2.1). By setting the

intercept to be  = 0, we have (f ) (1 - 2pe)2. (b) For robust one-bit compressed sensing where f

is given in (2.3). We have (f ) min

1-2 1+2

2

,

C 4 (1+ 3 )2

. (c) For one-bit phase retrieval where

f is given in (2.4). For Z  N (0, 1), we let m be the median of |Z|, i.e., P(|Z|  m) = 1/2. We have |(f )| | - m| exp(-2) and sign[(f )] = sign( - m). We thus obtain (f ) > 0 for
 > m.

3.3 Low dimensional recovery

We consider estimating  in the classical (low dimensional) setting where p n. Based on the second moment estimator M defined in (3.4), estimating  amounts to solving a noisy eigenvalue problem. We solve this by a simple iterative algorithm: provided an initial vector 0  Sp-1 (which
may be chosen at random) we perform power iterations as shown in Algorithm 1.

Theorem 3.5. We assume X  N (0, Ip) and (Y, X) follows (1.1). Let {(yi, xi)}ni=1 be n i.i.d. samples of response input pair (Y, X). For any link function f in (1.1) with 0, (f ) defined in (3.2) and (3.1), and (f ) > 01. We let

 :=

1 - 20 (f ) + 1 -

20

+

1

2,

and

 :=

(f ) + (1 + )

( - (f )

1)(1 - 20 + 1 - 20

)

.

(3.6)

There exist constant Ci such that when n  C1p/2, for Algorithm 1, we have that with probability

at least 1 - 2 exp(-C2p),

t - 

2



C3

*

(f ) + 1 - (f )

20

*

p +
n

1

- 2 2

*

t

, for t = 1, . . . , Tmax. (3.7)

Statistical Error

Optimization Error

Here  = 0,  , where  is the first leading eigenvector of M.

Note that by (3.6) we have   (0, 1). Thus, the optimization error term in (3.7) decreases at

a geometric rate to zero as t increases. For Tmax sufficiently large such that the statistical error

and optimization error terms in (3.7) are of the same order, we have Tmax -  2

p/n.

This statistical rate of convergence matches the rate of estimating a p-dimensional vector in linear

regression without any quantization, and will later be shown to be optimal. This result shows that the

lack of prior knowledge on the link function and the information loss from quantization do not keep

our procedure from obtaining the optimal statistical rate.

3.4 High dimensional recovery
Next we consider the high dimensional setting where p n and  is sparse, i.e.,   Sp-1  B0(s, p) with s being support size. Although this high dimensional estimation problem is closely
1Recall that we have an analogous treatment and thus results for (f ) < 0.

5

related to the well-studied sparse PCA problem, the existing works [4, 6, 17, 23, 27, 28, 31, 32] on sparse PCA do not provide a direct solution to our problem. In particular, they either lack statistical guarantees on the convergence rate of the obtained estimator [6, 23, 28] or rely on the properties of the sample covariance matrix of Gaussian data [4, 17], which are violated by the second moment estimator defined in (3.4). For the sample covariance matrix of sub-Gaussian data, [27] prove that the convex relaxation proposed by [7] achieves a suboptimal s log p/n rate of convergence. Yuan and
Zhang [31] propose the truncated power method, and show that it attains the optimal s log p/n rate

Algorithm 1 Low dimensional recovery
Input {(yi, xi)}ni=1, number of iterations Tmax 1: Second moment estimation: Construct M
from samples according to (3.4). 2: Initialization: Choose a random vector 0 
Sn-1 3: For t = 1, 2, . . . , Tmax do 4: t  M * t-1 5: t  t/ t 2 6: end For Output Tmax
locally; that is, it exhibits this rate of convergence
only in a neighborhood of the true solution where 0,  > C where C > 0 is some constant. It
is well understood that for a random initialization on Sp-1, such a condition fails with probability going to one as p  .

Algorithm 2 Sparse recovery
Input {(yi, xi)}ni=1, number of iterations Tmax, regularization parameter , sparsity level s.
1: Second moment estimation: Construct M
from samples according to (3.4).
2: Initialization: 3: 0  argmin{- M,  +   1,1
Rpxp
| Tr() = 1, 0  I} (3.8)
4: 0  first leading eigenvector of 0 5: 0  trunc(0, s) 6: 0  0/ 0 2 7: For t = 1, 2, . . . , Tmax do 8: t  trunc(M * t-1, s) 9: t  t/ t 2 10: end For Output Tmax

Instead, we propose a two-stage procedure for estimating  in our setting. In the first stage, we adapt
the convex relaxation proposed by [27] and use it as an initialization step, in order to obtain a good enough initial point satisfying the condition 0,  > C. The convex optimization problem can be
easily solved by the alternating direction method of multipliers (ADMM) algorithm (see [3, 27] for
details). Then we adapt the truncated power method. This procedure is illustrated in Algorithm 2. In particular, we define truncation operator trunc(*, *) as [trunc(, s)]j = 1(j  S)j, where S is the index set corresponding to the top s largest |j|. The initialization phase of our algorithm requires O(s2 log p) samples (see below for more precise details) to succeed. As work in [2] suggests, it is
unlikely that a polynomial time algorithm can avoid such dependence. However, once we are near the
solution, as we show, this two-step procedure achieves the optimal error rate of s log p/n.

Theorem 3.6. Let

 := 4(1 - 20) + (f ) 4(1 - 20) + 3(f ) < 1,

(3.9)

and the minimum sample size be

nmin := C * s2 log p * (f )2 * min (1 - 1/2)/2, /8 (1 - 20) + (f ) 2 .

(3.10)

Suppose  = C (f )+(1-20) log p/n with a sufficiently large constant C, where (f ) and 0 are specified in (3.2) and (3.5). Meanwhile, assume the sparsity parameter s in Algorithm 2 is set to be s = C max 1/(-1/2 -1)2 ,1 *s. For n  nmin with nmin defined in (3.10), we have

t -  2  C *

(f ) + (1 - 20)

5 2

(1

-

20)

1 2

*

(f )3

s log p + t * n

min (1 - 1/2)/2, 1/8

Statistical Error

Optimization Error
(3.11)

with high probability. Here  is defined in (3.9).

The first term on the right-hand side of (3.11) is the statistical error while the second term gives the optimization error. Note that the optimization error decays at a geometric rate since  < 1. For Tmax

6

sufficiently large, we have

Tmax - 
2

s log p/n.

In the sequel, we show that the right-hand side gives the optimal statistical rate of convergence for a

broad model class under the high dimensional setting with p n.

3.5 Minimax lower bound

We establish the minimax lower bound for estimating  in the model defined in (1.1). In the sequel we define the family of link functions that are Lipschitz continuous and are bounded away from 1. Formally, for any m  (0, 1) and L > 0, we define

F(m, L) := f : |f (z)|  1 - m, |f (z) - f (z )|  L|z - z |, for all z, z  R . (3.12)

Let Xfn := {(yi, xi)}ni=1 be the n i.i.d. realizations of (Y, X), where X follows N (0, Ip) and Y satisfies (1.1) with link function f . Correspondingly, we denote the estimator of   B to be (Xfn), where B is the domain of . We define the minimax risk for estimating  as

R(n, m, L, B) := inf inf sup E
f F (m,L) (Xfn) B

(Xfn) - 

2.

(3.13)

In the above definition, we not only take the infimum over all possible estimators , but also all possible link functions in F(m, L). For a fixed f , our formulation recovers the standard definition of minimax risk [30]. By taking the infimum over all link functions, our formulation characterizes the minimax lower bound under the least challenging f in F(m, L). In the sequel we prove that our procedure attains such a minimax lower bound for the least challenging f given any unknown link function in F(m, L). That is to say, even when f is unknown, our estimation procedure is as accurate as in the setting where we are provided the least challenging f , and the achieved accuracy is not improvable due to the information-theoretic limit. The following theorem establishes the minimax lower bound in the high dimensional setting.

Theorem 3.7. Let B = Sp-1B0(s, p). We assume that n > m(1-m)/(2L2)2* Cs log(p/s)/2-log 2 . For any s  (0, p/4], the minimax risk defined in (3.13) satisfies

R(n, m, L, B)  C *

m(1 - m) *

s log(p/s) .

Ln

Here C and C are absolute constants, while m and L are defined in (3.12).

Theorem 3.7 establishes the minimax optimality of the statistical rate attained by our procedure for p n and s-sparse . In particular, for arbitrary f  F(m, L)  {f : (f ) > 0}, the estimator  attained by Algorithm 2 is minimax-optimal in the sense that its s log p/n rate of convergence is not improvable, even when the information on the link function f is available. For general   Rp, one can show the best possible convergence rate is ( m(1 - m)p/n/L) by setting s = p/4 in Theorem 3.7.
It is worth to note that our lower bound becomes trivial for m = 0, i.e., there exists some z such that |f (z)| = 1. One example is the noiseless one-bit compressed sensing for which we have f (z) = sign(z). In fact, for noiseless one-bit compressed sensing, the s log p/n rate is not optimal. For example, the Jacques et al. [14] provide an algorithm (with exponential running time) that achieves rate s log p/n. Understanding such a rate transition phenomenon for link functions with zero margin, i.e., m = 0 in (3.12), is an interesting future direction.

3.6 Numerical results

We now turn to the numerical results that support our theory. For the three models introduced in 2,
we apply Algorithm 1 and Algorithm 2 to do parameter estimation in the classic and high dimensional regimes. Our simulations are based on synthetic data. For classic recovery,  is randomly chosen from Sp-1; for sparse recovery, we set j = s-1/21(j  S) for all j  [p], where S is a random index subset of [p] with size s. In Figure 1, as predicted by Theorem 3.5, we observe that the same

7

p/n leads to nearly identical estimation error. Figure 2 demonstrates similar results for the predicted rate s log p/n of sparse recovery and thus validates Theorem 3.6.

Estimation Error

0.7 0.5 0.7

Estimation Error

Estimation Error

0.6 0.6 0.4
0.5 0.5

0.4
0.3
0.2 0.05

p = 10 p = 20 p = 40

0.1 0.15 p/n

0.2

0.3 0.2
0.05

p = 10 p = 20 p = 40

0.1 0.15 p/n

0.2

0.4
0.3
0.2 0.05

p = 10 p = 20 p = 40

0.1 0.15 p/n

0.2

(a) Flipped Logistic Regression (b) One-bit Compressed Sensing

(c) One-bit Phase Retrieval

Figure 1: Estimation error of low dimensional recovery. (a) pe = 0.1. (b) 2 = 0.1. (c)  = 1.

1.2 0.8

Estimation Error

Estimation Error

1
0.8
0.6
0.4
0.2 0.1

p = 100, s = 5 p = 100, s = 10 p = 200, s = 5 p = 200, s = 10

0.2 0.3 s log p/n

0.4

0.6
0.4
0.2
0 0.1

p = 100, s = 5 p = 100, s = 10 p = 200, s = 5 p = 200, s = 10

0.2 0.3 s log p/n

0.4

1 0.8 0.6 0.4 0.2
0 0.1

p = 100, s = 5 p = 100, s = 10 p = 200, s = 5 p = 200, s = 10
0.15 0.2 0.25 0.3 s log p/n

(a) Flipped Logistic Regression (b) One-bit Compressed Sensing

(c) One-bit Phase Retrieval

Figure 2: Estimation error of sparse recovery. (a) pe = 0.1. (b) 2 = 0.1. (c)  = 1.

Estimation Error

4 Discussion

Sample complexity. In high dimensional regime, while our algorithm achieves optimal convergence rate, the sample complexity we need is (s2 log p). The natural question is whether it can be reduced to O(s log p). We note that breaking the barrier s2 log p is challenging. Consider a simpler problem sparse phase retrieval where yi = | xi,  |, with a fairly extensive body of literature, the state-ofthe-art efficient algorithms (i.e., with polynomial running time) for recovering sparse  requires sample complexity (s2 log p) [10]. It remains open to show whether it's possible to do consistent
sparse recovery with O(s log p) samples by any polynomial time algorithms.

Acknowledgment

XY and CC would like to acknowledge NSF grants 1056028, 1302435 and 1116955. This research was also partially supported by the U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center. HL is grateful for the support of NSF CAREER Award DMS1454377, NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841. ZW was partially supported by MSR PhD fellowship while this work was done.

References
[1] A L Q U I E R , P. and B I A U , G . (2013). Sparse single-index model. Journal of Machine Learning Research, 14 243-280.
[2] B E R T H E T, Q . and R I G O L L E T, P. (2013). Complexity theoretic lower bounds for sparse principal component detection. In Conference on Learning Theory.
[3] B O Y D , S ., PA R I K H , N ., C H U , E ., P E L E AT O , B . and E C K S T E I N , J . (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends R in Machine Learning, 3 1-122.

8

[4] C A I , T. T., M A , Z . and W U , Y. (2013). Sparse PCA: Optimal rates and adaptive estimation. Annals of Statistics, 41 3074-3110.
[5] C A N D E S , E . J ., E L D A R , Y. C ., S T R O H M E R , T. and V O R O N I N S K I , V. (2013). Phase retrieval via matrix completion. SIAM Journal on Imaging Sciences, 6 199-225.
[6] D ' A S P R E M O N T, A ., B A C H , F. and E L G H A O U I , L . (2008). Optimal solutions for sparse principal component analysis. Journal of Machine Learning Research, 9 1269-1294.
[7] D ' A S P R E M O N T, A ., E L G H A O U I , L ., J O R D A N , M . I . and L A N C K R I E T, G . R . (2007). A direct formulation for sparse PCA using semidefinite programming. SIAM Review 434-448.
[8] D E L E C R O I X , M ., H R I S TA C H E , M . and PAT I L E A , V. (2000). Optimal smoothing in semiparametric index approximation of regression functions. Tech. rep., Interdisciplinary Research Project: Quantification and Simulation of Economic Processes.
[9] D E L E C R O I X , M ., H R I S TA C H E , M . and PAT I L E A , V. (2006). On semiparametric M -estimation in single-index regression. Journal of Statistical Planning and Inference, 136 730-769.
[10] E L D A R , Y. C . and M E N D E L S O N , S . (2014). Phase retrieval: Stability and recovery guarantees. Applied and Computational Harmonic Analysis, 36 473-494.
[11] G O P I , S ., N E T R A PA L L I , P., JA I N , P. and N O R I , A . (2013). One-bit compressed sensing: Provable support and vector recovery. In International Conference on Machine Learning.
[12] H A R D L E , W., H A L L , P. and I C H I M U R A , H . (1993). Optimal smoothing in single-index models. Annals of Statistics, 21 157-178.
[13] H R I S TA C H E , M ., J U D I T S K Y, A . and S P O K O I N Y, V. (2001). Direct estimation of the index coefficient in a single-index model. Annals of Statistics, 29 pp. 595-623.
[14] JA C Q U E S , L ., L A S K A , J . N ., B O U F O U N O S , P. T. and B A R A N I U K , R . G . (2011). Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors. arXiv preprint arXiv:1104.3160.
[15] K A K A D E , S . M ., K A N A D E , V., S H A M I R , O . and K A L A I , A . (2011). Efficient learning of generalized linear and single index models with isotonic regression. In Advances in Neural Information Processing Systems.
[16] K A L A I , A . T. and S A S T RY, R . (2009). The isotron algorithm: High-dimensional isotonic regression. In Conference on Learning Theory.
[17] M A , Z . (2013). Sparse principal component analysis and iterative thresholding. The Annals of Statistics, 41 772-801.
[18] M A S S A R T, P. and P I C A R D , J . (2007). Concentration inequalities and model selection, vol. 1896. Springer.
[19] N ATA R A J A N , N ., D H I L L O N , I ., R AV I K U M A R , P. and T E WA R I , A . (2013). Learning with noisy labels. In Advances in Neural Information Processing Systems.
[20] P L A N , Y. and V E R S H Y N I N , R . (2013). One-bit compressed sensing by linear programming. Communications on Pure and Applied Mathematics, 66 1275-1297.
[21] P L A N , Y., V E R S H Y N I N , R . and Y U D O V I N A , E . (2014). High-dimensional estimation with geometric constraints. arXiv preprint arXiv:1404.3749.
[22] P O W E L L , J . L ., S T O C K , J . H . and S T O K E R , T. M . (1989). Semiparametric estimation of index coefficients. Econometrica, 57 pp. 1403-1430.
[23] S H E N , H . and H U A N G , J . (2008). Sparse principal component analysis via regularized low rank matrix approximation. Journal of Multivariate Analysis, 99 1015-1034.
[24] S T O K E R , T. M . (1986). Consistent estimation of scaled coefficients. Econometrica, 54 pp. 1461-1481. [25] T I B S H I R A N I , J . and M A N N I N G , C . D . (2013). Robust logistic regression using shift parameters.
arXiv preprint arXiv:1305.4987. [26] V E R S H Y N I N , R . (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027. [27] V U , V. Q ., C H O , J ., L E I , J . and R O H E , K . (2013). Fantope projection and selection: A near-optimal
convex relaxation of sparse PCA. In Advances in Neural Information Processing Systems. [28] W I T T E N , D ., T I B S H I R A N I , R . and H A S T I E , T. (2009). A penalized matrix decomposition, with
applications to sparse principal components and canonical correlation analysis. Biostatistics, 10 515-534. [29] Y I , X ., WA N G , Z ., C A R A M A N I S , C . and L I U , H . (2015). Optimal linear estimation under unknown
nonlinear transform. arXiv preprint arXiv:1505.03257. [30] Y U , B . (1997). Assouad, Fano, and Le Cam. In Festschrift for Lucien Le Cam. Springer, 423-435. [31] Y U A N , X . - T. and Z H A N G , T. (2013). Truncated power method for sparse eigenvalue problems.
Journal of Machine Learning Research, 14 899-925. [32] Z O U , H ., H A S T I E , T. and T I B S H I R A N I , R . (2006). Sparse principal component analysis. Journal
of Computational and Graphical Statistics, 15 265-286.
9

