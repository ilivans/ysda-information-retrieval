Exactness of Approximate MAP Inference in Continuous MRFs
Nicholas Ruozzi Department of Computer Science
University of Texas at Dallas Richardson, TX 75080
Abstract
Computing the MAP assignment in graphical models is generally intractable. As a result, for discrete graphical models, the MAP problem is often approximated using linear programming relaxations. Much research has focused on characterizing when these LP relaxations are tight, and while they are relatively well-understood in the discrete case, only a few results are known for their continuous analog. In this work, we use graph covers to provide necessary and sufficient conditions for continuous MAP relaxations to be tight. We use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and logsupermodular decomposable models. We conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the MAP relaxation can and cannot be tight.
1 Introduction
Graphical models are a popular modeling tool for both discrete and continuous distributions. We are commonly interested in one of two inference tasks in graphical models: finding the most probable assignment (a.k.a., MAP inference) and computing marginal distributions. These problems are NPhard in general, and a variety of approximate inference schemes are used in practice.
In this work, we will focus on approximate MAP inference. For discrete state spaces, linear programming relaxations of the MAP problem (specifically, the MAP LP) are quite common [1; 2; 3]. These relaxations replace global marginalization constraints with a collection of local marginalization constraints. Wald and Globerson [4] refer to these as local consistency relaxations (LCRs). The advantage of LCRs is that they are often much easier to specify and to optimize over (e.g., by using a message-passing algorithm such as loopy belief propagation (LBP)). However, the analogous relaxations for continuous state spaces may not be compactly specified and can lead to an unbounded number of constraints (except in certain special cases). To overcome this problem, further relaxations have been proposed [5; 4]. By construction, each of these further relaxations can only be tight if the initial LCR was tight. As a result, there are compelling theoretical and algorithmic reasons to investigate when LCRs are tight.
Among the most well-studied continuous models are the Gaussian graphical models. For this class of models, it is known that the continuous MAP relaxation is tight when the corresponding inverse covariance matrix is positive definite and scaled diagonally dominant (a special case of the so-called log-concave decomposable models)[4; 6; 7]. In addition, LBP is known to converge to the correct solution for Gaussian graphical models and log-concave decomposable models that satisfy a scaled diagonal dominance condition [8; 9]. While much of the prior work in this domain has focused on log-concave graphical models, in this work, we provide a general necessary and sufficient condition for the continuous MAP relaxation to be tight. This condition mirrors the known results for the discrete case and is based on the notion of graph covers: the MAP LP is tight if and only if the
1

optimal solution to the MAP problem is an upper bound on the MAP solution over any graph cover, appropriately scaled. This characterization will allow us to understand when the MAP relaxation is tight for more general models.
Apart from this characterization theorem, the primary goal of this work is to move towards a uniform treatment of the discrete and continuous cases; they are not as different as they may initially appear. To this end, we explore the relationship between log-concave decomposable models and logsupermodular decomposable models (introduced here in the continuous case). Log-supermodular models provide an example of continuous graphical models for which the MAP relaxation is tight, but the objective function is not necessarily log-concave. These two concepts have analogs in discrete state spaces. In particular, log-concave decomposability is related to log-concave closures of discrete functions and log-supermodular decomposability is a known condition which guarantees that the MAP LP is exact in the discrete setting. We prove a number of results that highlight the similarities and differences between these two concepts as well as a general condition under which the MAP relaxation corresponding to a pairwise twice continuously differentiable model cannot be tight.

2 Prerequisites
Let f : X n  R0 be a non-negative function where X is the set of possible assignments of each variable. A function f factors with respect to a hypergraph G = (V, A), if there exist potential functions fi : X  R0 for each i  V and f : X ||  R0 for each   A such that
f (x1, . . . , xn) = fi(xi) f(x).
iV A
The hypergraph G together with the potential functions fiV and fA define a graphical model.
We are interested computing supxX n f G(x). In general, this MAP inference task is NP-hard, but in practice, local message-passing algorithms based on approximations from statistical physics, such as LBP, produce reasonable estimates in many settings. Much effort has been invested into understanding when LBP solves the MAP problem. In this section, we briefly review approximate MAP inference in the discrete setting (i.e., when X is a finite set). For simplicity and consistency, we will focus on log-linear models as in [4]. Given a vector of sufficient statistics i(xi)  Rk for each i  V and xi  X and a parameter vector i  Rk, we will assume that fi(xi) = exp ( i, i(xi) ) . Similarly, given a vector of sufficient statistics (x) for each   A and x  X || and a parameter vector , we will assume that f(x) = exp ( , (x) ) . We will write (x) to represent the concatenation of the individual sufficient statistics and  to represent the concatenation of the parameters. The objective function can then be expressed as f G(x) = exp ( , (x) ) .

2.1 The MAP LP relaxation

The MAP problem can be formulated in terms of mean parameters [10].

sup log f (x) = sup , 

xX n

M

M = {  Rm :    s.t. E [(x)] = } where  is the space of all densities over X n and M is the set of all realizable mean parameters.

In general, M is a difficult object to compactly describe and to optimize over. As a result, one typically constructs convex outerbounds on M that are more manageable. In the case that X is finite,
one such outerbound is given by the MAP LP. For each i  V and k  X , define i(xi)k 1{xi=k}. Similarly, for each   A and k  X ||, define (x)k 1{x=k}. With this choice of sufficient statistics, M is equivalent to the set of all marginal distributions over the individual variables and elements of A that arise from some joint probability distribution. The MAP LP is obtained by replacing M with a relaxation that only enforces local consistency constraints.

ML =   0 :

x\{i} (x) = i(xi), for all   A, i  , xi  X

xi i(xi) = 1,

for all i  V

The set of constraints, ML, is known as the local marginal polytope. The approximate MAP prob-

lem is then to compute maxML ,  .

2

1, 2, 3 1, 4 2, 3, 4 1, 2, 3 1, 4 2, 3, 4 2, 3, 4 1, 4 1, 2, 3

1234 (a) A hypergraph graph, G.

2314

4132

(b) One possible 2-cover of G.

Figure 1: An example of a graph cover of a factor graph. The nodes in the cover are labeled for the node that they copy in the base graph.

2.2 Graph covers

In this work, we are interested in understanding when this relaxation is tight (i.e., when does supML ,  = supxX n log f (x)). For discrete MRFs, the MAP LP is known to be tight in a variety of different settings [11; 12; 13; 14]. Two different theoretical tools are often used to investigate the tightness of the MAP LP: duality and graph covers. Duality has been particularly useful in the design of convergent and correct message-passing schemes that solve the MAP LP [1; 15; 2; 16]. Graph covers provide a theoretical framework for understanding when and why message-passing algorithms such as belief propagation fail to solve the MAP problem [17; 18; 3].
Definition 2.1. A graph H covers a graph G = (V, E) if there exists a graph homomorphism h : H  G such that for all vertices i  G and all j  h-1(i), h maps the neighborhood j of j in H bijectively to the neighborhood i of i in G.

If a graph H covers a graph G, then H looks locally the same as G. In particular, local messagepassing algorithms such as LBP have difficulty distinguishing a graph and its covers. If h(j) = i, then we say that j  H is a copy of i  G. Further, H is said to be an M -cover of G if every vertex of G has exactly M copies in H.

This definition can be easily extended to hypergraphs. Each hypergraph G can be represented in factor graph form: create a node in the factor graph for each vertex (called variable nodes) and each hyperedge (called factor nodes) of G. Each factor node is connected via an edge in the factor graph to the variable nodes on which the corresponding hyperedge depends. For an example of a 2-cover, see Figure 1.

To any M -cover H = (V H , AH ) of G given by the homomorphism h, we can associate a collection of potentials: the potential at node i  V H is equal to fh(i), the potential at node h(i)  G,
and for each   AH , we associate the potential fh(). In this way, we can construct a function
f H : X M|V |  R0 such that f H factorizes over H. We will say that the graphical model H is an M -cover of the graphical model G whenever H is an M -cover of G and f H is chosen as described above. It will be convenient in the sequel to write f H (xH ) = f H (x1, . . . , xM ) where xmi is the mth copy of variable i  V .

There is a direct correspondence between   ML and assignments on graph covers. This correspondence is the basis of the following theorem.
Theorem 2.2 (Ruozzi and Tatikonda 3).

sup ,  = sup sup sup 1 log f H (xH )

ML

M HCM (G) xH M

where CM (G) is the set of all M -covers of G.

Theorem 2.2 claims that the optimal value of the MAP LP is equal to the supremum over all MAP

assignments over all graph covers, appropriately scaled. In particular, the proof of this result shows

that, under mild conditions, there exists an M -cover H of G and an assignment xH such that

1 M

log f H (xH ) = supML

, 

.

3 Continuous MRFs

In this section, we will describe how to extend the previous results from discrete to continuous MRFs (i.e., X = R) using graph covers. The relaxation that we consider here is the appropriate extension

3

of the MAP LP where each of the sums are replaced by integrals [4].


   ML =  :
  

 densities i,  s.t. (x)dx\i = i(xi),
i = Ei [i],  = E [],





for all   A, i  , xi  X

 

for all i  V for all   A

  

Our goal is to understand under what conditions this continuous relaxation is tight. Wald and Globerson [4] have approached this problem by introducing a further relaxation of ML which they call the weak local consistency relaxation (weak LCR). They provide conditions under which the weak LCR (and hence the above relaxation) is tight. In particular, they show that weak LCR is tight for the class of log-concave decomposable models. In this work, we take a different approach. We first prove the analog of Theorem 2.2 in the continuous case and then we show that the known conditions that guarantee tightness of the continuous relaxation are simple consequences of this general theorem.

Theorem 3.1.

sup ,  = sup sup sup 1 log f H (xH )

ML

M HCM (G) xH M

where CM (G) is the set of all M -covers of G.

The proof of Theorem 3.1 is conceptually straightforward, albeit technical, and can be found in Appendix A. The proof approximates the expectations in ML as expectations with respect to simple functions, applies the known results for finite spaces, and takes the appropriate limit. Like its discrete counterpart, Theorem 3.1 provides necessary and sufficient conditions for the continuous relaxation to be tight. In particular, for the relaxation to be tight, the optimal solution on any M cover, appropriately scaled, cannot exceed the value of the optimal solution of the MAP problem over G.

3.1 Tightness of the MAP relaxation
Theorem 3.1 provides necessary and sufficient conditions for the tightness of the continuous relaxation. However, checking that the maximum value attained on any M -cover is bounded by the maximum value over the base graph to the M , in and of itself, appears to be a daunting task. In this section, we describe two families of graphical models for which this condition is easy to verify: the log-concave decomposable functions and the log-supermodular decomposable functions. Log-concave decomposability has been studied before, particularly in the case of Gaussian graphical models. Log-supermodularity with respect to graphical models, however, appears to have been primarily studied in the discrete case.

3.1.1 Log-concave decomposability

A function f : Rn  R0 is log-concave if f (x)f (y)1-  f (x + (1 - )y) for all x, y  Rn and all   [0, 1]. If f can be written as a product of log-concave potentials over a hypergraph G, we say that f is log-concave decomposable over G.
Theorem 3.2. If f is log-concave decomposable, then supx log f (x) = supML ,  .
Proof. By log-concave decomposability, for any M -cover H of G,

f H (x1, . . . , xM )  f G

x1 + * * * + xM

M
,

M

which we obtain by applying the definition of log-concavity separately to each of the M copies of the potential functions for each node and factor of G. As a result, supx1,...,xM f H (x1, . . . , xM ) 
supx f G(x)M . The proof of the theorem then follows by applying Theorem 3.1.

Wald and Globerson [4] provide a different proof of Theorem 3.2 by exploiting duality and the weak LCR.

4

3.1.2 Log-supermodular decomposability

Log-supermodular functions have played an important role in the study of discrete graphical models,
and log-supermodularity arises in a number of classical correlations inequalities (e.g., the FKG
inequality). For log-supermodular decomposable models, the MAP LP is tight and the MAP problem
can be solved exactly in polynomial time [19; 20]. In the continuous case, log-supermodularity is defined analogously to the discrete case. That is, f : Rn  R0 is log-supermodular if f (x)f (y)  f (x  y)f (x  y) for all x, y  Rn, where x  y is the componentwise maximum of the vectors x and y and x  y is the componentwise minimum. Continuous log-supermodular functions are
sometimes said to be multivariate totally positive of order two [21]. We will say that a graphical model is log-supermodular decomposable if f can be factorized as a product of log-supermodular
potentials.

For any collection of vectors x1, . . . , xk  Rn, let zi(x1, . . . , xk) be the vector whose jth component is the ith largest element of x1j , . . . , xkj for each j  {1, . . . , n}.
Theorem 3.3. If f is log-supermodular decomposable, then supx log f (x) = supML ,  .
Proof. By log-supermodular decomposability, for any M -cover H of G,

M
f H (x1, . . . , xM )  f G(zm(x1, . . . , xM )).

m=1

Again, this follows by repeatedly applying the definition of log-supermodularity separately to

each of the M copies of the potential functions for each node and factor of G. As a result,

supx1,...,xM f H (x1, . . . , xM )  supx1,...,xM

M m=1

f G(xm).

The

proof

of

the

theorem

then

fol-

lows by applying Theorem 3.1.

4 Log-supermodular decomposability vs. log-concave decomposability

As discussed above, log-concave decomposable and log-supermodular decomposable models are both examples of continuous graphical models for which the MAP relaxation is tight. These two classes are not equivalent: twice continuously differentiable functions are supermodular if and only if all off diagonal elements of the Hessian matrix are non-negative. Contrast this with twice continuously differentiable concave functions where the Hessian matrix must be negative semidefinite. In particular, this means that log-supermodular functions can be multimodel. In this section, we explore the relationship between log-supermodularity and log-concavity.

4.1 Gaussian MRFs

We begin with the case of Gaussian graphical models, i.e., pairwise graphical models given by

f (x)  = -1/2xT Ax + bT x =

exp

-

1 2

Aiix2i

+

bixi

exp (-Aijxixj)

iV (i,j)E

for some symmetric positive definite matrix A  Rnxn and vector b  Rn. Here, f factors over the graph G corresponding to the non-zero entries of the matrix A.

Gaussian graphical models are a relatively well-studied class of continuous graphical models. In fact, sufficient conditions for the convergence and correctness of Gaussian belief propagation (GaBP) are known for these models. Specifically, GaBP converges to the optimal solution if the positive definite matrix A is walk-summable, scaled diagonally dominant, or log-concave decomposable [22; 7; 8; 9]. These conditions are known to be equivalent [23; 6].
Definition 4.1.   Rnxn is scaled diagonally dominant if w  Rn, w > 0 such that |ii|wi > j=i |ij |wj .

In addition, the following theorem provides a characterization of scaled diagonal dominance (and hence log-concave decomposability) in terms of graph covers for these models.
Theorem 4.2 (Ruozzi and Tatikonda 6). Let A be a symmetric positive definite matrix. The following are equivalent.

5

1. A is scaled diagonally dominant.
2. All covers of A are positive definite.
3. All 2-covers of A are positive definite.
The proof of this theorem constructs a specific 2-cover whose covariance matrix has negative eigenvalues whenever the matrix A is positive definite but not scaled diagonally dominant. The joint distribution corresponding to this 2-cover is not bounded from above, so the optimal value of the MAP relaxation is + as per Theorem 3.1.
For Gaussian graphical models, log-concave decomposability and log-supermodular decomposability are related: every positive definite, log-supermodular decomposable model is log-concave decomposable, and every positive definite, log-concave decomposable model is a signed version of some positive definite, log-supermodular decomposable Gaussian graphical model. This follows from the following simple lemma. Lemma 4.3. A symmetric positive definite matrix A is scaled diagonally dominant if and only if the matrix B such that Bii = Aii for all i and Bij = -|Aij| for all i = j is positive definite.
If A is positive definite and scaled diagonally dominant, then the model is log-concave decomposable. In contrast, the model would be log-supermodular decomposable if all of the off-diagonal elements of A were negative, independent of the diagonal. In particular, the diagonal could have both positive and negative elements, meaning that f (x) could be either log-concave, log-convex, or neither. As log-convex quadratic forms do not correspond to normalizable Gaussian graphical models, the log-convex case appears to be less interesting as the MAP problem is unbounded from above. However, the situation is entirely different for constrained (over some convex set) log-quadratic maximization. As an example, consider a box constrained log-quadratic maximization problem where the matrix A has all negative off-diagonal entries. Such a model is always log-supermodular decomposable. Hence, the MAP relaxation is tight, but the model is not necessarily log-concave.
4.2 Pairwise twice differentiable MRFs
All of the results from the previous section can be extended to general twice continuously differentiable functions over pairwise graphical models (i.e., || = 2 for all   A). In this section, unless otherwise specified, assume that all models are pairwise. Theorem 4.4. If log f (x) is strictly concave and twice continuously differentiable, the following are equivalent.
1. 2(log f )(x) is scaled diagonally dominant for all x.
2. 2(log f H )(xH ) is negative definite for every graph cover H of G and every xH .
3. 2(log f H )(xH ) is negative definite for every 2-cover H of G and every xH .
The equivalence of 1-3 in Theorem 4.4 follows from Theorem 4.2. Corollary 4.5. If 2(log f )(x) is scaled diagonally dominant for all x, then the continuous MAP relaxation is tight. Corollary 4.6. If f is log-concave decomposable over a pairwise graphical model and strictly logconcave, then 2(log f )(x) is scaled diagonally dominant for all x.
Whether or not log-concave decomposability is equivalent to the other conditions listed in the statement of Theorem 4.4 remains an open question (though we conjecture that this is the case). Similar ideas can be extended to general twice continuously differentiable functions. Theorem 4.7. Suppose log f (x) is twice continuously differentiable with a maximum at x. Let Bij = |2(log f )(x)ij| for all i = j and Bii = 2(log f )(x)ii. If f admits a pairwise factorization over G and B has both positive and negative eigenvalues, then the continuous MAP relaxation is not tight.
Proof. If B has both positive and negative eigenvalues, then there exists a 2-cover H of G such that 2(log f H )(x, x) has both positive and negative eigenvalues. As a result, the lift of x to the
6

2-cover f H is a saddle point. Consequently, f H (x, x) < supxH f H (xH ). By Theorem 3.1, the continuous MAP relaxation cannot be tight.
This negative result is quite general. If 2(log f ) is positive definite but not scaled diagonally dominant at any global optimum, then the MAP relaxation is not tight. In particular, this means that all log-supermodular decomposable functions that meet the conditions of the theorem must be s.d.d. at their optima.
Algorithmically, Moallemi and Van Roy [9] argued that belief propagation converges for models that are log-concave decomposable and scaled diagonally dominant. It is unknown whether or not a similar convergence argument applies to log-supermodular decomposable functions.

4.3 Concave closures

Many of the tightness results in the discrete case can be seen as a specific case of the continuous results described above. Again, suppose that X  R is a finite set.
Definition 4.8. The concave closure of a function g : X n  R  {-} at x  Rn is given by



 g(x) = sup

 (y)g(y) : y (y) = 1, y (y)y = x, (y)  0

yX n



Equivalently, the concave closure of a function is the smallest concave function such that g(x)  g(x) for all x. A function and its concave closure must necessarily have the same maximum. Computing the concave (or convex) closure of a function is NP-hard in general, but it can be efficiently computed for certain special classes of discrete functions. In particular, when X = {0, 1} and log f is supermodular, then its concave closure can be computed in polynomial time as it is equal to the Lovasz extension of log f . The Lovasz extension has a number of interesting properties. Most notably, it is linear (the Lovasz extension of a sum of functions is equal to sum of the Lovasz extensions). Define the log-concave closure of f to be f(x) = exp(log f (x)). As a result, if f is log-supermodular decomposable, then f is log-concave decomposable.
Theorem 4.9. If f = iV fi A f, then supxX n f (x) = ML ,  . This theorem is a direct consequence of Theorem 3.2. For example, the tightness results of Bayati et al. [11] and Sanghavi et al. [14] (and indeed many others) can be seen as a special case of this theorem. Even when |X | is not finite, the concave closure can be similarly defined, and the theorem holds in this case as well. Given the characterization in the discrete case, this suggests that there could be a, possibly deep, connection between log-concave closures and log-supermodular decomposability.

5 Discussion
We have demonstrated that the same necessary and sufficient condition based on graph covers for the tightness of the MAP LP in the discrete case translates seamlessly to the continuous case. This characterization allowed us to provide simple proofs of the tightness of the MAP relaxation for logconcave decomposable and log-supermodular decomposable models. While the proof of Theorem 3.1 is nontrivial, it provides a powerful tool to reason about the tightness of MAP relaxations. We also explored the intricate relationship between log-concave and log-supermodular decomposablity in both the discrete and continuous cases which provided intuition about when the MAP relaxation can or cannot be tight for pairwise graphical models.

A Proof of Theorem 3.1

The proof of this theorem proceeds in two parts. First, we will argue that

sup ,   sup sup sup 1 log f H (xH ).

ML

M HCM (G) xH M

To see this, fix an M -cover, H, of G via the homomorphism h and consider any assignment xH . Construct the mean parameters   ML as follows.

7

1 i(xi) = M

(xHj - xi)

jV (H):h(j)=i

1 (x) = M

(xH - x)

 A(H ):h( )=

i = i(xi)i(xi)dxi  = (x)(x)dx

Here, (*) is the Dirac delta function1. This implies that
1 log f H (xH ) = ,   sup ,  . M ML
For the other direction, fix some   ML such that  is generated by the vector of densities  . We will prove the result for locally consistent probability distributions with bounded support. The result for arbitrary  will then follow by constructing sequences of these distributions that converge (in measure) to  . For simplicity, we will assume that each potential function is strictly positive2.

Consider the space [-t, t]|V | for some positive integer t. We will consider local probability dis-
tributions that are supported on subsets of this space. That is, supp(i)  [-t, t] for each i and supp()  [-t, t]|| for each . For a fixed positive integer s, divide the interval [-t, t] into 2s+1t intervals of size 1/2s and let Sk denote the kth interval. This partitioning divides [-t, t]|V | into disjoint cubes of volume 1/2s|V |. The distribution  can be approximated as a sequence of distributions  1,  2, . . . as follows. Define a vector of approximate densities  s by setting

is(xi)

2s Sk i(xi)dxi, if xi  Sk 0, otherwise

s (x )

2||s

kj :j Skj (x)dx, if x  kj :j Skj 0, otherwise

We have  s   , [-t,t] is(xi)i(xi)dxi  [-t,t]|| s(x)(x)dx   for each   A(G).

i for each i



V (G), and

The continuous MAP relaxation for local probability distributions of this form can be expressed in

terms of discrete variables over X = {1, . . . , 2s+1t}. To see this, define si (zi) = Szi is(xi)dxi

for each zi  {1, . . . corresponding MAP

, 2s+1t} and s(z) = LP objective, evaluated

aSt zs, iss(xthe)ndx

for

each

z



{1,

.

.

.

,

2s+1t}||.

The

si (zi)

2s log fi(xi)dxi +

s(z)

2||s log f(x)dx.

iV zi

Szi

A z

Sz

(1)

This MAP LP objective corresponds to a discrete graphical model that factors over the hypergraph

G, with potential functions corresponding to the above integrals over the partitions indexed by the

vector z.

gs(z) 

exp

iV (G)

2s log fi(xi)dxi

exp

Szi A(G)

2||s log f(x)dx
Sz

= exp
iV (G)

2|V (G)|s log fi(xi)dx

exp

Sz A(G)

2|V (G)|s log f(x)dx
Sz

Every assignment selects a single cube indexed by z. The value of the objective is calculated by

averaging log f over the cube indexed by z. As a result, maxz gs(z)  supx f (x) and for any

M -cover H of G, maxz1:M gH,s(z1, . . . , zM )  supx1:m f H (x1, . . . , xM ). As this upper bound

holds for any fixed s, it must also hold for any vector of distributions that can be written as a limit of

such distributions. Now, by applying Theorem 2.2 for the discrete case, ,  = lims , s 

supM

supHCM (G) supxH

1 M

log f H (xH ) as desired.

To finish the proof, observe that any Riemann

integrable density can be arbitrarily well approximated by densities of this form as t  .

1In order to make this precise, we would need to use Lebesgue integration or take a sequence of probability distributions over the space RM|V | that arbitrarily well-approximate the desired assignment xH .
2The same argument will apply in the general case, but each of the local distributions must be contained in
the support of the corresponding potential function (i.e., supp(i)  supp(fi)) for the integrals to exist.

8

References
[1] A. Globerson and T. S. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In Proc. 21st Neural Information Processing Systems (NIPS), Vancouver, B.C., Canada, 2007.
[2] T. Werner. A linear programming approach to max-sum problem: A review. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(7):1165-1179, 2007.
[3] N. Ruozzi and S. Tatikonda. Message-passing algorithms: Reparameterizations and splittings. IEEE Transactions on Information Theory, 59(9):5860-5881, Sept. 2013.
[4] Y. Wald and A. Globerson. Tightness results for local consistency relaxations in continuous MRFs. In Proc. 30th Uncertainty in Artifical Intelligence (UAI), Quebec City, Quebec, Canada, 2014.
[5] T. P. Minka. Expectation propagation for approximate Bayesian inference. In Proceedings of the Seventeenth conference on Uncertainty in Artificial Intelligence (UAI), pages 362-369, 2001.
[6] N. Ruozzi and S. Tatikonda. Message-passing algorithms for quadratic minimization. Journal of Machine Learning Research, 14:2287-2314, 2013.
[7] D. M. Malioutov, J. K. Johnson, and A. S. Willsky. Walk-sums and belief propagation in Gaussian graphical models. Journal of Machine Learning Research, 7:2031-2064, 2006.
[8] C. C. Moallemi and B. Van Roy. Convergence of min-sum message passing for quadratic optimization. Information Theory, IEEE Transactions on, 55(5):2413 -2423, May 2009.
[9] C. C. Moallemi and B. Van Roy. Convergence of min-sum message-passing for convex optimization. Information Theory, IEEE Transactions on, 56(4):2041 -2050, April 2010.
[10] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends R in Machine Learning, 1(1-2):1-305, 2008.
[11] M. Bayati, C. Borgs, J. Chayes, and R. Zecchina. Belief propagation for weighted b-matchings on arbitrary graphs and its relation to linear programs with integer solutions. SIAM Journal on Discrete Mathematics, 25(2):989-1011, 2011.
[12] V. Kolmogorov and R. Zabih. What energy functions can be minimized via graph cuts? In Computer VisionECCV 2002, pages 65-81. Springer, 2002.
[13] S. Sanghavi, D. M. Malioutov, and A. S. Willsky. Belief propagation and LP relaxation for weighted matching in general graphs. Information Theory, IEEE Transactions on, 57(4):2203 -2212, April 2011.
[14] S. Sanghavi, D. Shah, and A. S. Willsky. Message passing for maximum weight independent set. Information Theory, IEEE Transactions on, 55(11):4822-4834, Nov. 2009.
[15] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. MAP estimation via agreement on (hyper)trees: Message-passing and linear programming. Information Theory, IEEE Transactions on, 51(11):3697- 3717, Nov. 2005.
[16] David Sontag, Talya Meltzer, Amir Globerson, Yair Weiss, and Tommi Jaakkola. Tightening LP relaxations for MAP using message-passing. In 24th Conference in Uncertainty in Artificial Intelligence, pages 503-510. AUAI Press, 2008.
[17] P. O. Vontobel. Counting in graph covers: A combinatorial characterization of the Bethe entropy function. Information Theory, IEEE Transactions on, Jan. 2013.
[18] P. O. Vontobel and R. Koetter. Graph-cover decoding and finite-length analysis of message-passing iterative decoding of LDPC codes. CoRR, abs/cs/0512078, 2005.
[19] S. Iwata, L. Fleischer, and S. Fujishige. A strongly polynomial-time algorithm for minimizing submodular functions. Journal of The ACM, 1999.
[20] A. Schrijver. A combinatorial algorithm minimizing submodular functions in strongly polynomial time. Journal of Combinatorial Theory, Series B, 80(2):346 - 355, 2000.
[21] S. Karlin and Y. Rinott. Classes of orderings of measures and related correlation inequalities. i. multivariate totally positive distributions. Journal of Multivariate Analysis, 10(4):467 - 498, 1980.
[22] Y. Weiss and W. T. Freeman. Correctness of belief propagation in Gaussian graphical models of arbitrary topology. Neural Comput., 13(10):2173-2200, Oct. 2001.
[23] D. M. Malioutov. Approximate inference in Gaussian graphical models. Ph.D. thesis, EECS, MIT, 2008.
9

