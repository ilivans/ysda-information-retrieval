Fast and Memory Optimal Low-Rank Matrix Approximation

Se-Young Yun MSR, Cambridge seyoung.yun@inria.fr

Marc Lelarge  Inria & ENS
marc.lelarge@ens.fr

Alexandre Proutiere  KTH, EE School / ACL
alepro@kth.se

Abstract

In this paper, we revisit the problem of constructing a near-optimal rank k approximation of a matrix M  [0, 1]mxn under the streaming data model where the

columns of M are revealed sequentially. We present SLA (Streaming Low-rank

Approximation), an algorithm that is asymptotically accurate, when ksk+1(M ) = o( mn) where sk+1(M ) is the (k + 1)-th largest singular value of M . This means that its average mean-square error converges to 0 as m and n grow large

(i.e.,

M (k) - M (k)

2 F

=

o(mn) with high probability, where M (k) and M (k)

de-

note the output of SLA and the optimal rank k approximation of M , respectively).

Our algorithm makes one pass on the data if the columns of M are revealed in

a random order, and two passes if the columns of M arrive in an arbitrary order.

To reduce its memory footprint and complexity, SLA uses random sparsification,

and samples each entry of M with a small probability . In turn, SLA is memory

optimal as its required memory space scales as k(m+n), the dimension of its out-

put. Furthermore, SLA is computationally efficient as it runs in O(kmn) time (a

constant number of operations is made for each observed entry of M ), which can

be as small as O(k log(m)4n) for an appropriate choice of  and if n  m.

1 Introduction

We investigate the problem of constructing, in a memory and computationally efficient man-

ner, an accurate estimate of the optimal rank k approximation M (k) of a large (m x n) matrix M  [0, 1]mxn. This problem is fundamental in machine learning, and has naturally found nu-

merous applications in computer science. The optimal rank k approximation M (k) minimizes, over

all rank k matrices Z, the Frobenius norm M - Z F (and any norm that is invariant under rotation) and can be computed by Singular Value Decomposition (SVD) of M in O(nm2) time (if we

assume that m  n). For massive matrices M (i.e., when m and n are very large), this becomes

unacceptably slow. In addition, storing and manipulating M in memory may become difficult. In

this paper, we design a memory and computationally efficient algorithm, referred to as Streaming

Low-rank Approximation (SLA), that computes a near-optimal rank k approximation M (k). Under

mild assumptions on M , the SLA algorithm is asymptotically accurate in the sense that as m and n

grow large, its average mean-square error converges to 0, i.e.,

M (k) - M (k)

2 F

=

o(mn)

with high

probability (we interpret M (k) as the signal that we aim to recover form a noisy observation M ).

To reduce its memory footprint and running time, the proposed algorithm combines random sparsification and the idea of the streaming data model. More precisely, each entry of M is revealed to the algorithm with probability , called the sampling rate. Moreover, SLA observes and treats the

Work performed as part of MSR-INRIA joint research centre. M.L. acknowledges the support of the
French Agence Nationale de la Recherche (ANR) under reference ANR-11-JS02-005-01 (GAP project). A. Proutiere's research is supported by the ERC FSA grant, and the SSF ICT-Psi project.

1

columns of M one after the other in a sequential manner. The sequence of observed columns may

be chosen uniformly at random in which case the algorithm requires one pass on M only, or can be

arbitrary in which case the algorithm needs two passes. SLA first stores = 1/( log(m)) randomly

selected columns, and extracts via spectral decomposition an estimator of parts of the k top right

singular vectors of M . It then completes the estimator of these vectors by receiving and treating the

remain columns sequentially. SLA finally builds, from the estimated top k right singular vectors, the

linear projection onto the subspace generated by these vectors, and deduces an estimator of M (k).

The analysis of the performance of SLA is presented in Theorems 7, and 8. In summary:

when

m



n,

log4 (m) m







m-8/9,

with

probability

1

-

k,

the

output

M (k)

of

SLA

satisfies:

M (k) - M (k)

2 F

=O

k2

s2k+1(M ) + log(m)

mn mn m

,

(1)

where sk+1(M ) is the (k + 1)-th singular value of M . SLA requires O(kn) memory space, and if





log4 (m) m

and

k



log6(m),

its

time

is

O(kmn).

To

ensure

the

asymptotic

accuracy

of

SLA,

the

upper-bound in (1) needs to converge to 0 which is true as soon as ksk+1(M ) = o( mn). In the

case where M is seen as a noisy version of M (k), this condition quantifies the maximum amount of

noise allowed for our algorithm to be asymptotically accurate.

SLA is memory optimal, since any rank k approximation algorithm needs to at least store its output, i.e., k right and left singular vectors, and hence needs at least O(kn) memory space. Further observe that among the class of algorithms sampling each entry of M at a given rate , SLA is computational optimal, since it runs in O(kmn) time (it does a constant number of operations per observed entry if k = O(1)). In turn, to the best of our knowledge, SLA is both faster and more memory efficient than existing algorithms. SLA is the first memory optimal and asymptotically accurate low rank approximation algorithm.

The approach used to design SLA can be readily extended to devise memory and computationally efficient matrix completion algorithms. We present this extension in the supplementary material.

Notations. Throughout the paper, we use the following notations. For any m x n matrix A, we denote by A its transpose, and by A-1 its pseudo-inverse. We denote by s1(A)  * * *  snm(A) 
0, the singular values of A. When matrices A and B have the same number of rows, [A, B] to denote
the matrix whose first columns are those of A followed by those of B. A denotes an orthonormal basis of the subspace perpendicular to the linear span of the columns of A. Aj, Ai, and Aij de-
note the j-th column of A, the i-th row of A, and the entry of A on the i-th line and j-th column, respectively. For h  l, Ah:l (resp. Ah:l) is the matrix obtained by extracting the columns (resp. lines) h, . . . , l of A. For any ordered set B = {b1, . . . , bp}  {1, . . . , n}, A(B) refers to the matrix
composed by the ordered set B of columns of A. A(B) is defined similarly (but for lines). For real numbers a  b, we define |A|ba the matrix with (i, j) entry equal to (|A|ba)ij = min(b, max(a, Aij)). Finally, for any vector v, v denotes its Euclidean norm, whereas for any matrix A, A F denotes its Frobenius norm, A 2 its operator norm, and A  its -norm, i.e., A  = maxi,j |Aij|.

2 Related Work

Low-rank approximation algorithms have received a lot of attention over the last decade. There are two types of error estimate for these algorithms: either the error is additive or relative.

To translate our bound (1) in an additive error is easy:

M - M (k) F  M - M (k) F + O

k

sk+1(M ) mn

+

log1/2 m (m)1/4

 mn

.

(2)

Sparsifying M to speed-up the computation of a low-rank approximation has been proposed in the

literature and the best additive error bounds have been obtained in [AM07]. When the sampling rate



satisfies





log4 m

m

,

the

authors

show

that

with

probability

1

-

exp(- log4

m),

M - M (k) F  M - M (k) F + O

k1/2n1/2 k1/4n1/4

+

1/2

1/4

M (k)

1/2 F

.

(3)

2

This performance guarantee is derived from Lemma 1.1 and Theorem 1.4 in [AM07]. To

compare (2) and (3), note that our assumptions on the bounded entries of M ensures that:

s2k+1(M ) mn



1 k

and

M (k) F 

bound for (3) is

+k1/2
m

k1/4 (m)1/4

 M F  mn. In particular, we see that the worst case  nm which is always lower than the worst case bound for (2):

k

1 k

+

log m m

1/2  nm. When k = O(1), our bound is only larger by a logarithmic term in m

compared to [AM07]. However, the algorithm proposed in [AM07] requires to store O(mn) en-

tries of M whereas SLA needs O(n) memory space. Recall that log4 m  m  m1/9 so that our

algorithm makes a significant improvement on the memory requirement at a low price in the error

guarantee bounds. Although biased sampling algorithms can reduce the error, the algorithm have to

run leverage scores with multiple passes over data [BJS15]. In a recent work, [CW13] proposes a

time efficient algorithm to compute a low-rank approximation of a sparse matrix. Combined with

[AM07], we obtain an algorithm running in time O(mn) + O(nk2 + k3) but with an increased

additive error term.

We can also compare our result to papers providing an estimate M (k) of the optimal low-rank approximation of M with a relative error , i.e. such that M - M (k) F  (1 + ) M - M (k) F . To
the best of our knowledge, [CW09] provides the best result in this setting. Theorem 4.4 in [CW09]

shows that provided the rank of M is at least 2(k + 1), their algorithm outputs with probability 1 -  a rank-k matrix M (k) with relative error  using memory space O (k/ log(1/)(n + m)) (note that

in [CW09], the authors use as unit of memory a bit whereas we use as unit of memory an entry of the

matrix so we removed a log mn factor in their expression to make fair comparisons). To compare

with our result, we can translate our bound (1) in a relative error, and we need to take:



=

O

 k

sk+1(M ) M

+ -

log1/2 m (m)1/4

 mn



M (k) F



.

First note that since M is assumed to be of rank at least 2(k + 1), we have M - M (k) F  sk+1(M ) > 0 and  is well-defined. Clearly, for our  to tend to zero, we need M - M (k) F to be not too small. For the scenario we have in mind, M is a noisy version of the signal M (k) so that

M- with

Ma c(okn)sitsanthtevnaroiiasnecme,atrMix.-WMhe(nk)evFer=y ent(ry omf

M +

-M (k) is generated independently at random n) while sk+1(M ) = ( n). In such a case,

we have  = o(1) and we improve the memory requirement of [CW09] by a factor -1 log(k)-1.

[CW09] also considers a model where the full columns of M are revealed one after the other in an

arbitrary order, and proposes a one-pass algorithm to derive the rank-k approximation of M with the

same memory requirement. In this general setting, our algorithm is required to make two passes on

the data (and only one pass if the order of arrival of the column is random instead of arbitrary). The running time of the algorithm scales as O(kmn-1 log(k)-1) to project M onto k-1 log(k)-1 dimensional random space. Thus, SLA improves the time again by a factor of -1 log(k)-1.

We could also think of using sketching and streaming PCA algorithms to estimate M (k). When the columns arrive sequentially, these algorithms identify the left singular vectors using one-pass on the matrix and then need a second pass on the data to estimate the right singular vectors. For example, [Lib13] proposes a sketching algorithm that updates the p most frequent directions as columns are observed. [GP14] shows that with O(km/) memory space (for p = k/), this sketching algorithm finds m x k matrix U such that M - PU M F  (1 + ) M - M (k) F , where PU denotes the projection matrix to the linear span of the columns of U . The running time of the algorithm is roughly O(kmn-1), which is much greater than that of SLA. Note also that to identify such matrix U in one pass on M , it is shown in [Woo14] that we have to use (km/) memory space. This result does not contradict the performance analysis of SLA, since the latter needs two passes on M if the columns of M are observed in an arbitrary manner. Finally, note that the streaming PCA algorithm proposed in [MCJ13] does not apply to our problem as this paper investigates a very specific problem: the spiked covariance model where a column is randomly generated in an i.i.d. manner.

3 Streaming Low-rank Approximation Algorithm

3

Algorithm 1 Streaming Low-rank Approximation (SLA)

Input: M , k, , and

=

1  log(m)

1. A(B1), A(B2)  independently sample entries of [M1, . . . , M ] at rate 

2. PCA for the first columns: Q  SPCA(A(B1), k)

3. Trimming the rows and columns of A(B2):

A(B2)  set the entries of rows of A(B2) having more than two non-zero entries to 0

A(B2)  set the entries of the columns of A(B2) having more than 10m non-zero entries to 0

4. W  A(B2)Q 5. V (B1)  (A(B1)) W

6. I  A(B1)V (B1)

Remove A(B1), A(B2), and Q from the memory space

for t = + 1 to n do

7. At  sample entries of Mt at rate  8. V t  (At) W 9. I  I + AtV t

Remove At from the memory space

end for

10. R  find R using the Gram-Schmidt process such that V R is an orthonormal matrix

11.

U



1 

IRR

Output: M (k) = |U V |10

Algorithm 2 Spectral PCA (SPCA)
Input: C  [0, 1]mx , k   x k Gaussian random matrix Trimming: C  set the entries of the rows of C with more than 10 non-zero entries to 0   C C - diag(C C) Power Iteration: QR  QR decomposition of  5 log( )  Output: Q

In this section, we present the Streaming Low-rank Approximation (SLA) algorithm and analyze its performance. SLA makes one pass on the matrix M , and is provided with the columns of M one after the other in a streaming manner. The SVD of M is M = U V where U and V are (m x m) and (n x n) unitary matrices and  is the (m x n) matrix diag(s1(M ), . . . snm(M )). We assume (or impose by design of SLA) that the (specified below) first observed columns of M are
chosen uniformly at random among all columns. An extension of SLA to scenarios where columns are observed in an arbitrary order is presented in 3.5, but this extension requires two passes on M . To be memory efficient, SLA uses sampling. Each observed entry of M is erased (i.e., set equal to 0) with probability 1 - , where  > 0 is referred to as the sampling rate. The algorithm, whose
pseudo-code is presented in Algorithm 1, proceeds in three steps:

1. In the first step, we observe

=

1  log(m)

columns of M

chosen uniformly at random.

These

columns form the matrix M(B) = U (V (B)) , where B denotes the ordered set of the indexes of

the first observed columns. M(B) is sampled at rate . More precisely, we apply two independent

sampling procedures, where in each of them, every entry of M(B) is sampled at rate . The two

resulting independent random matrices A(B1), and A(B2) are stored in memory. A(B1), referred to as A(B) to simplify the notations, is used in this first step, whereas A(B2) will be used in subsequent

steps. Next through a spectral decomposition of A(B), we derive a ( x k) orthonormal matrix Q

such that the span of its column vectors approximates that of the column vectors of V1(:Bk ). The first step corresponds to Lines 1 and 2 in the pseudo-code of SLA.

2. In the second step, we complete the construction of our estimator of the top k right singular
vectors V1:k of M . Denote by V the k x n matrix formed by these estimated vectors. We first compute the components of these vectors corresponding to the set of indexes B as V (B) = A(B1)W with W = A(B2)Q. Then for t = + 1, . . . , n, after receiving the t-th column Mt of M , we set
V t = At W , where At is obtained by sampling entries of Mt at rate . Hence after one pass on M , we get V = A W , where A = [A(B1), A +1, . . . , An]. As it turns out, multiplying W by A amplifies the useful signal contained in W , and yields an accurate approximation of the span of the

4

top k right singular vectors V1:k of M . The second step is presented in Lines 3, 4, 5, 7 and 8 in SLA pseudo-code.

3. In the last step, we deduce from V a set of column vectors gathered in matrix U such that U V

provides an accurate approximation of M (k). First, using the Gram-Schmidt process, we find R

such that V R is an

orthonormal matrix and

compute U

=

1 

AV

RR

in a streaming manner as in

Step 2. Then, U V

=

1 

AV

R(V

R)

where V R(V R)

approximates the projection matrix onto

the linear span of the top k right singular vectors of M . Thus, U V is close to M (k). This last step

is described in Lines 6, 9, 10 and 11 in SLA pseudo-code.

In the next subsections, we present in more details the rationale behind the three steps of SLA, and provide a performance analysis of the algorithm.

3.1 Step 1. Estimating right-singular vectors of the first batch of columns

The objective of the first step is to estimate V1(:Bk ), those components of the top k right singular vectors of M whose indexes are in the set B (remember that B is the set of indexes of the first observed columns). This estimator, denoted by Q, is obtained by applying the power method to extract the top k right singular vector of M(B), as described in Algorithm 2. In the design of this algorithm and its performance analysis, we face two challenges: (i) we only have access to a sampled version A(B) of M(B); and (ii) U (V (B)) is not the SVD of M(B) since the column vectors of V1(:Bk ) are not orthonormal in general (we keep the components of these vectors corresponding to the set of indexes B). Hence, the top k right singular vectors of M(B) that we extract in Algorithm 2 do not necessarily correspond to V1(:Bk ).
To address (i), in Algorithm 2, we do not directly extract the top k right singular vectors of A(B). We first remove the rows of A(B) with too many non-zero entries (i.e., too many observed entries from M(B)), since these rows would perturb the SVD of A(B). Let us denote by A the obtained trimmed matrix. We then form the covariance matrix A A, and remove its diagonal entries to obtain the matrix  = A A - diag(A A). Removing the diagonal entries is needed because of the sampling procedure. Indeed, the diagonal entries of A A scale as , whereas its off-diagonal entries scale as 2. Hence, when  is small, the diagonal entries would clearly become dominant in the spectral decomposition. We finally apply the power method to  to obtain Q. In the analysis of
the performance of Algorithm 2, the following lemma will be instrumental, and provides an upper bound of the gap between  and (M(B)) M(B) using the matrix Bernstein inequality (Theorem 6.1 [Tro12]). All proofs are detailed in Appendix.

Lemma 1

If 



m-

8 9

,

with

probability

1

-

1
2

,

 - 2(M(B))

M(B) 2  c1

m log( ), for

some constant c1 > 1.

To address (ii), we first establish in Lemma 2 that for an appropriate choice of , the column vectors of V1(:Bk ) are approximately orthonormal. This lemma is of independent interest, and relates the SVD of a truncated matrix, here M(B), to that of the initial matrix M . More precisely:

Lemma 2 If   m-8/9, there exists a x k matrix V (B) such that its column vectors are orthonor-

mal,

and

with probability

1 - exp(-m1/7),

for

all

i



k

satisfying

that

s2i (M )



n 

m log( ),

n V1(:Bi ) - V1(:Bi )

2



m-

1 3

.

Note that as suggested by the above lemma, it might be impossible to recover Vi(B) when the corre-

sponding

singular

value

si(M )

is

small

(more

precisely,

when

s2i (M )



n 

m log( )). However,

the singular vectors corresponding to such small singular values generate very little error for low-

rank approximation. Thus, we are only interested in singular vectors whose singular values are

above

the

threshold

(

n 

m

log(

))1/2. Let k

=

max{i

:

s2i (M )



n 

m log( ), i  k}.

Now to analyze the performance of Algorithm 2 when applied to A(B), we decompose  as  =

2 n

V1(:Bk )(11::kk

)2(V1(:Bk ))

+ Y , where Y

=



-

2 n

V1(:Bk )(11::kk

)2(V1(:Bk ))

is a noise matrix. The

5

following lemma quantifies how noise may affect the performance of the power method, i.e., it provides an upper bound of the gap between Q and V1(:Bk ) as a function of the operator norm of the noise matrix Y :

Lemma 3

With probability 1 -

1
2

,

the

output

Q

of

SPCA

when

applied

to

A(B)

satisfies for all

ik:

(V1(:Bi ))

* Q

2



.3 Y 2
2 n si(M )2

In the proof, we analyze the power iteration algorithm from results in [HMT11].

To complete the performance analysis of Algorithm 2, it remains to upper bound Y 2. To this aim, we decompose Y into three terms:
Y =  - 2(M(B)) M(B) + 2(M(B)) I - U1:k U1:k M(B)+

2 (M(B)) U1:k U1:k M(B) - n V1(:Bk )(11::kk )2(V1(:Bk ))

.

The first term can be controlled using Lemma 1, and the last term is upper bounded using Lemma 2. Finally, the second term corresponds to the error made by ignoring the singular vectors which are not within the top k . To estimate this term, we use the matrix Chernoff bound (Theorem 2.2 in [Tro11]), and prove that:

Lemma 4 With probability 1 - exp(-m1/4),

(I

-

U1:k U1:k )M(B)

2 2



2 

m log( ) +

n s2k+1(M ).

In summary, combining the four above lemmas, we can establish that Q accurately estimates V1(:Bk ):

Theorem 5 If   m-8/9, A(B) satisfies for all i  k:

with probability (V1(:Bi )) * Q

1- 2

3
2

,

the

output

Q

32(s2k+1(M )+2m

23on2fs)A2i+(l3Mg(o2)+rict1h)m n2wmhelnoga( p)p, lwiehdertoe

c1 is the constant from Lemma 1.

3.2 Step 2: Estimating the principal right singular vectors of M

In this step, we aim at estimating the top k right singular vectors V1:k, or at least at producing
k vectors whose linear span approximates that of V1:k. Towards this objective, we start from Q derived in the previous step, and define the (m x k) matrix W = A(B2)Q. W is stored and kept in memory for the remaining of the algorithm.

It is tempting to directly read from W the top k left singular vectors U1:k . Indeed, we know that

Q  n V1(:Bk ), and E[A(B2)] = U (V (B)) , and hence E[W ]   n U1:k11::kk. However, the

level of the noise in W is too important so as to accurately extract U1:k . In turn, W can be written

as U (V (B)) Q + Z, where Z = (A(B2) - U (V (B)) )Q partly captures the noise inW . It

is then easy to see that the level of the noise Z satisfies E[ Z 2]  E[ Z F / k] = ( m).

Indeed, first observe that Z

is of rank k.

Then E[

Z

2 F

]

=

m i=1

k j=1

E[Zi2j

]



mk:

this

is

due to the facts that (i) Q and A(B2) - U (V (B)) are independent (since A(B1) and A(B2) are

independent), (ii)

Qj

2 2

=

1 for all j



k, and (iii) the entries of A(B2) are independent with

variance ((1 - )). However, for all j  k , the j-th singular value of U (V (B)) Q scales as

 O( m ) = O(

m log(m)

),

since

sj(M )



 mn

and

sj (M(B))



n sj(M ) when j  k from

Lemma 2.

Instead, from W , A(B1) and the subsequent sampled arriving columns At, t > , we produce a (n x k) matrix V whose linear span approximates that of V1:k . More precisely, we first let V (B) = A(B1)W . Then for all t = + 1, . . . , n, we define V t = At W , where At is obtained from the t-th observed column of M after sampling each of its entries at rate . Multiplying W by A = [A(B1), A +1, . . . , An] amplifies the useful signal in W , so that V = A W constitutes a good approximation of V1:k. To understand why, we can rewrite V as follows:
V = 2M M(B)Q + M (A(B2) - M(B))Q + (A - M ) W.

6

In the above equation, the first term corresponds to the useful signal and the two remaining terms
constitute noise matrices. From Theorem 5, the linear span of columns of Q approximates that of
the columns of V (B) and thus, for j  k , sj(2M M(B)Q)  2s2j (M ) n   mn log( ).
The spectral norms of the noise matrices are bounded using random matrix arguments, and the fact that (A(B2) - M(B)) and (A - M ) are zero-mean random matrices with independent entries. We can show (see Lemma 14 given in the supplementary material) using the independence of A(B1) and A(B2) that with high probability, M (A(B2) - M(B))Q 2 = O( mn). We may also establish that with high probability, (A - M ) W 2 = O( m(m + n)). This is a consequence of a result derived in [AM07] (quoted in Lemma 13 in the supplementary material) stating that with high probability, A - M = O( (m + n)) and of the fact that due to the trimming process presented in Line 3 in Algorithm 1, W 2 = O( m). In summary, as soon as n scales at least as m, the noise level becomes negligible, and the span of V1:k provides an accurate approximation of that of V1:k . The above arguments are made precise and rigorous in the supplementary material. The following theorem summarizes the accuracy of our estimator of V1:k.

Theorem 6

With

log4 (m) m







m-

8 9

for all i



k, there exists a constant c2 such that with

probability 1 - k,

Vi

(V1:k )

2



c2

s2k+1

(M

)+n

log(m) s2i

m/+m (M )

n log(m)/ .

3.3 Step 3: Estimating the principal left singular vectors of M

In the last step, we estimate the principal left singular vectors of M to finally derive an estimator of

M (k), the optimal rank-k approximation of M . The construction of this estimator is based on the

observation that M (k) = U1:k11::kkV1:k = M PV1:k , where PV1:k = V1:kV1:k is an (n x n) matrix representing the projection onto the linear span of the top k right singular vectors V1:k of M . Hence

to estimate M (k), we try to approximate the matrix PV1:k . To this aim, we construct a (k x k) matrix R so that the column vectors of V R form an orthonormal basis whose span corresponds to that

of the column vectors of V . This construction is achieved using Gram-Schmidt process. We then

approximate PV1:k by PV = V RR

V

,

and

finally

our

estimator

M (k)

of

M (k)

is

1 

APV

.

The construction of M (k) can be made in a memory efficient way accommodating for our streaming

model where the columns of M arrive one after the other, as described in the pseudo-code of SLA.

First, after constructing V (B) in Step 2, we build the matrix I = A(B1)V (B). Then, for t =

+ 1, . . . , n, after constructing the t-th line V t of V , we update I by adding to it the matrix AtV t,

so that after all columns of M are observed, I = AV . Hence we can build an estimator U of the

principal

left

singular

vectors

of

M

as

U

=

1 

IRR

, and finally obtain M (k) = |U V

|10.

To quantify the estimation error of M (k), we decompose M (k) - M (k) as: M (k) - M (k) =

M (k)(I

-

PV )

+ (M (k)

-

M )PV

+

(M

-

1 

A)PV

.

The

first

term

of

the

r.h.s.

of the above

equation can be bounded using Theorem 6: for i  k, we have si(M )2 Vi V  z =

c2(s2k+1(M ) + n log(m) m/ + m n log(m)/), and hence we can conclude that for all i  k,

si(M )UiVi

(I - PV )

2 F

 z.

The second term can be easily bounded observing that the matrix

(M (k) - M )PV is of rank k:

(M (k) - M )PV

2 F

k

(M (k) - M )PV

2 2



k

M (k) - M

2 2

=

ksk+1(M )2. The last term in the r.h.s. can be controlled as in the performance analysis of Step 2, and

observing

that

(

1 

A

-

M

)PV

is of rank k:

1 

A

-

M

PV

2 F



k

1 

A

-

M

2
= O(k(m + n)).
2

It is then easy to remark that for the range of the parameter  we are interested in, the upper bound z

of the first term dominates the upper bound of the two other terms. Finally, we obtain the following

result (see the supplementary material for a complete proof):

Theorem 7

When

log4 (m) m







m-

8 9

,

with

probability

1

-

k,

the

output

of

the

SLA

algorithm

satisfies with constant c3:

M (k)-[U V mn

]10

2 F

= c3k2

s2k+1(M ) mn

+

log(m) m

+

log(m) n

.

7

Note

that

if

log4 (m) m







m-

8 9

,

then

log(m) m

=

o(1).

Hence

if

n



m,

the

SLA

algorithm

provides

an asymptotically accurate estimate of M (k)

as soon as

sk+1(M )2 mn

=

o(1).

3.4 Required Memory and Running Time

Required memory.

Lines 1-6 in SLA pseudo-code. A(B1) and A(B2) have O(m ) non-zero entries and we need O(m log m) bits to store the id of these entries. Similarly, the memory required to store  is

O(2m 2 log( )). Storing Q further requires O( k) memory. Finally, V (B1) and I computed in

Line 6 require O( k) and O(km) memory space, respectively. Thus, when

=



1 log

m

,

this

first

part

of the algorithm requires O(k(m + n)) memory.

Lines 7-9. Before we treat the remaining columns, A(B1), A(B2), and Q are removed from the memory. Using this released memory, when the t-th column arrives, we can store it, compute V t and I,

and remove the column to save memory. Therefore, we do not need additional memory to treat the

remaining columns.

Lines 10 and 11. From I and V , we compute U . To this aim, the memory required is O(k(m + n)).

Running time.

From line 1 to 6. The SPCA algorithm requires O( k(2m + k) log( )) floating-point operations to

compute Q. W , V , and I are inner products, and their computations require O(km ) operations.

With

=



1 log(m)

,

the

number

of

operations

to

treat

the

first

columns is O( k(2m + k) log( ) +

km

)

=

O(km)

+

O(

k2 

).

From line 7 to 9. To compute V t and I when the t-th column arrives, we need O(km) operations.

Since there are n - remaining columns, the total number of operations is O(kmn).

Lines 10 and 11 R is computed from V using the Gram-Schmidt process which requires O(k2m)

operations. We then compute IRR using O(k2m) operations. Hence we conclude that:

In summary, we have shown that:

Theorem 8 The memory required to run the SLA algorithm is O(k(m + n)). Its running time is

O(kmn

+

k2 

+

k2m).

Observe

that

when





max(

(log(m))4 m

,

(log(m))2 n

)

and

k



(log(m))6,

we

have

kmn



k2/



k2m, and therefore, the running time of SLA is O(kmn).

3.5 General Streaming Model

SLA is a one-pass low-rank approximation algorithm, but the set of the first observed columns

of M needs to be chosen uniformly at random. We can readily extend SLA to deal with scenarios

where the columns of M can be observed in an arbitrary order. This extension requires two passes

on M , but otherwise performs exactly the same operations as SLA. In the first pass, we extract a set

of columns chosen uniformly at random, and in the second pass, we deal with all other columns.

To extract randomly selected columns in the first pass, we proceed as follows. Assume that when

the t-th column of M arrives, we have already extracted l columns. Then the t-th column is extracted

with

probability

-l n-t+1

.

This

two-pass

version

of

SLA

enjoys

the

same

performance

guarantees

as

those of SLA.

4 Conclusion
This paper revisited the low rank approximation problem. We proposed a streaming algorithm that samples the data and produces a near optimal solution with a vanishing mean square error. The algorithm uses a memory space scaling linearly with the ambient dimension of the matrix, i.e. the memory required to store the output alone. Its running time scales as the number of sampled entries of the input matrix. The algorithm is relatively simple, and in particular, does exploit elaborated techniques (such as sparse embedding techniques) recently developed to reduce the memory requirement and complexity of algorithms addressing various problems in linear algebra.

8

References
[AM07] Dimitris Achlioptas and Frank Mcsherry. Fast computation of low-rank matrix approximations. Journal of the ACM (JACM), 54(2):9, 2007.
[BJS15] Srinadh Bhojanapalli, Prateek Jain, and Sujay Sanghavi. Tighter low-rank approximation via sampling the leveraged element. In Proceedings of the Twenty-Sixth Annual ACMSIAM Symposium on Discrete Algorithms, pages 902-920. SIAM, 2015.
[CW09] Kenneth L Clarkson and David P Woodruff. Numerical linear algebra in the streaming model. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 205-214. ACM, 2009.
[CW13] Kenneth L Clarkson and David P Woodruff. Low rank approximation and regression in input sparsity time. In Proceedings of the forty-fifth annual ACM symposium on Theory of computing, pages 81-90. ACM, 2013.
[GP14] Mina Ghashami and Jeff M Phillips. Relative errors for deterministic low-rank matrix approximations. In SODA, pages 707-717. SIAM, 2014.
[HMT11] Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217-288, 2011.
[Lib13] Edo Liberty. Simple and deterministic matrix sketching. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 581- 588. ACM, 2013.
[MCJ13] Ioannis Mitliagkas, Constantine Caramanis, and Prateek Jain. Memory limited, streaming PCA. In Advances in Neural Information Processing Systems, 2013.
[Tro11] Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform. Advances in Adaptive Data Analysis, 3(01n02):115-126, 2011.
[Tro12] Joel A Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389-434, 2012.
[Woo14] David Woodruff. Low rank approximation lower bounds in row-update streams. In Advances in Neural Information Processing Systems, pages 1781-1789, 2014.
9

