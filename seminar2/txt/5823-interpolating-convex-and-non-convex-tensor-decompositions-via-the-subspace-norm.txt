Interpolating Convex and Non-Convex Tensor Decompositions via the Subspace Norm

Qinqing Zheng University of Chicago qinqing@cs.uchicago.edu

Ryota Tomioka Toyota Technological Institute at Chicago
tomioka@ttic.edu

Abstract

We consider the problem of recovering a low-rank tensor from its noisy obser-

vation. Previous work has shown a recovery guarantee with signal to noise ratio

O(n K/2 /2) for recovering a Kth order rank one tensor of size n x * * * x n by

recursive unfolding. In this paper, we first improve this bound to O(nK/4) by a

much simpler approach, but with a more careful analysis. Then we propose a new

norm called the subspace norm, which is based on the Kronecker products of fac-

taollroswosbtuasintoedshboywthaenperaorplyosieddeaslimOp(lenes+timaHtoKr.-T1h)ebiomunpdo,seind

Kronecker structure which the parameter

H controls the blend from the non-convex estimator to mode-wise nuclear norm

minimization. Furthermore, we empirically demonstrate that the subspace norm

achieves the nearly ideal denoising performance even with H = O(1).

1 Introduction

Tensor is a natural way to express higher order interactions for a variety of data and tensor decomposition has been successfully applied to wide areas ranging from chemometrics, signal processing, to neuroimaging; see [15, 18] for a survey. Moreover, recently it has become an active area in the context of learning latent variable models [3].

Many problems related to tensors, such as, finding the rank, or a best rank-one approaximation of

a tensor is known to be NP hard [11, 8]. Nevertheless we can address statistical problems, such as,

how well we can recover a low-rank tensor from its randomly corrupted version (tensor denoising)

or from partial observations (tensor completion). Since we can convert a tensor into a matrix by

an operation known as unfolding, recent work [25, 19, 20, 13] has shown that we do get nontrivial

guarantees by using some norms or singular value decompositions. More specifically, Richard &

Montanari [20] has shown that when a rank-one Kth order tensor of size n x * * * x n is corrupted

by standard Gaussian noise ratio / n

noise, a nontrivial bound can be shown with high probability if K/2 /2 by a method called the recursive unfolding1. Note that

the signal to / n

is sufficient for matrices (K = 2) and also for tensors if we use the best rank-one approximation

(which is known to be NP hard) as an estimator. On the other hand, Jain & Oh [13] analyzed the

tensor completion problem and proposed an algorithm that requires O(n3/2*polylog(n)) samples for

K = 3; while information theoretically we need at least (n) samples and the intractable maximum

likelihood estimator would require O(n * polylog(n)) samples. Therefore, in both settings, there is

a wide gap between the ideal estimator and current polynomial time algorithms. A subtle question

that we will address in this paper is whether we need to unfold the tensor so that the resulting matrix

become as square as possible, which was the reasoning underlying both [19, 20].

As a parallel development, non-convex estimators based on alternating minimization or nonlinear optimization [1, 21] have been widely applied and have performed very well when appropriately

1We say an bn if there is a constant C > 0 such that an  C * bn.

1

Table 1: Comparison of required signal-to-noise ratio / of different algorithms for recovering a Kth order rank one tensor of size n x * * * x n contaminated by Gaussian noise with Standard deviation . See model (2). The bound for the ordinary unfolding is shown in Corollary 1. The bound for the subspace norm is shown in Theorem 2. The ideal estimator is proven in Appendix A.

Overlapped/ Recursive

Ordinary un- Subspace norm (pro-

Ideal

Latent nuclear unfolding[20]/ folding

posed)

norm[23]

square

norm[19] O(n(K-1)/2) O(n K/2 /2)

O(nK/4)

 O( n + HK-1) O( nK log(K))

set up. Therefore it would be of fundamental importance to connect the wisdom of non-convex estimators with the more theoretically motivated estimators that recently emerged.

In this paper, we explore such a connection by defining a new norm based on Kronecker products of factors that can be obtained by simple mode-wise singular value decomposition (SVD) of unfoldings (see notation section below), also known as the higher-order singular value decomposition (HOSVD) [6, 7]. We first study the non-asymptotic behavior of the leading singular vector from the ordinary (rectangular) unfolding X(k) and show a nontrivial bound for signal to noise ratio / nK/4. Thus the result also applies to odd order tensors confirming a conjecture in [20]. Furthermore, this motivates us to use the solution of mode-wise truncated SVDs to construct a new norm. We propose the subspace norm, which predicts an unknown low-rank tensor as a mixture of K low-rank tensors, in which each term takes the form

foldk(M (k)(P

(1)



*

*

*



P

(k-1)



P

(k+1)



*

*

*



P

(K)
)

),

where foldk is the inverse of unfolding (*)(k),  denotes the Kronecker product, and P (k)  RnxH is a orthonormal matrix estimated from the mode-k unfolding of the observed tensor, for k = 1, . . . , K; H is a user-defined parameter, and M (k)  RnxHK-1 . Our theory tells us that with
(k)
sufficiently high signal-to-noise ratio the estimated P spans the true factors.

We highlight our contributions below:

1. We prove that the required signal-to-noise ratio for recovering a Kth order rank one tensor from the ordinary unfolding is O(nK/4). Our analysis shows a curious two phase behavior: with high probability, when nK/4 / nK/2, the error shows a fast decay as 1/4; for / nK/2, the error decays slowly as 1/2. We confirm this in a numerical simulation. 2. The proposed subspace norm is an interpolation between the intractable estimators that directly control the rank (e.g., HOSVD) and the tractable norm-based estimators. It becomes equivalent to the latent trace norm [23] when H = n at the cost of increased signal-to-noise ratio threshold (see Table 1). 3. The proposed estimator is more efficient than previously proposed norm based estimators, because the size of the SVD required in the algorithm is reduced from n x nK-1 to n x HK-1. 4. We also empirically demonstrate that the proposed subspace norm performs nearly optimally for constant order H.

Notation
Let X  Rn1xn2x***xnK be a Kth order tensor. We will often use n1 = * * * = nK = n to simplify the notation but all the results in this paper generalizes to general dimensions. The inner product between a pair of tensors is defined as the inner products of them as vectors; i.e., X , W = vec(X ), vec(W) . For u  Rn1 , v  Rn2 , w  Rn3 , u  v  w denotes the n1 x n2 x n3 rank-one tensor whose i, j, k entry is uivjwk. The rank of X is the minimum number of rank-one tensors required to write X as a linear combination of them. A mode-k fiber of tensor X is an nk dimensional vector that is obtained by fixing all but the kth index of X . The mode-k unfolding X(k) of tensor X is an nk x k =k nk matrix constructed by concatenating all the mode-k fibers along columns. We denote the spectral and Frobenius norms for matrices by * and * F , respectively.

2

2 The power of ordinary unfolding

2.1 A perturbation bound for the left singular vector

We first establish a bound on recovering the left singular vector of a rank-one n x m matrix (with m > n) perturbed by random Gaussian noise.

Consider the following model known as the information plus noise model [4]:

X = uv + E,

(1)

where u and v are unit vectors,  is the signal strength,  is the noise standard deviation, and the noise matrix E is assumed to be random with entries sampled i.i.d. from the standard normal distribution. Our goal is to lower-bound the correlation between u and the top left singular vector u of X for signal-to-noise ratio / (mn)1/4 with high probability.
A direct application of the classic Wedin perturbation theorem [28] to the rectangular matrix X does not provide us the desired result. This is becauseit requires the signal to noise ratio /  2 E . Since the spectral norm of E scales as Op( n + m) [27], this would mean that we require / m1/2; i.e., the threshold is dominated by the number of columns m, if m  n.
Alternatively, we can view u as the leading eigenvector of X X , a square matrix. Our key insight is that we can decompose X X as follows:

X X = (2uu + m2I) + (2EE - m2I) + (uv E + Evu ).

Note that u is the leading eigenvector of the first term because adding an identity matrix does not change the eigenvectors. Moreover, we notice that there are two noise terms: the first term is a centered Wishart matrix and it is independent of the signal ; the second term is Gaussian distributed and depends on the signal .

This implies a two-phase behavior corresponding to either the Wishart or the Gaussian noise term being dominant, depending on the value of . Interestingly, we get a different speed of convergence for each of these phases as we show in the next theorem (the proof is given in Appendix D.1).
Theorem 1. There exists a constant C such that with probability at least 1 - 4e-n, if m/n  C,

 Cnm

1 

-

(/)4

,

| u, u | 

1

-

Cn (/)2

,

if

 m>





(C nm)

1 4

,



if



  m,





otherwise,

|

u, u

|



1-

Cn (/)2

if

/



C n.

In other words, if X has sufficiently many more columns than rows, as the signal to noise ratio / increases, u first converges to u as 1/4, and then as 1/2. Figure 1(a) illustrates these results. We randomly generate a rank-one 100 x 10000 matrix perturbed by Gaussian noise, and measure the distance between u and u. The phase transition happens at / = (nm)1/4, and there are two
regimes of different convergence rates as Theorem 1 predicts.

2.2 Tensor Unfolding

Now let's apply the above result to the tensor version of information plus noise model studied by [20]. We consider a rank one n x * * * x n tensor (signal) contaminated by Gaussian noise as follows:

Y = X  + E = u(1)  * * *  u(K) + E,

(2)

where factors u(k)  Rn, k = 1, . . . , K, are unit vectors, which are not necessarily identical, and the entries of E  Rnx***xn are i.i.d samples from the normal distribution N (0, 1). Note that this is slightly more general (and easier to analyze) than the symmetric setting studied by [20].

3

n = 100, m = 10000 0

-2

-4

-6 log(1 - | u, u |) -3.81 log(/) + 12.76
-2.38 log(/) + 6.18 -8 log (nm)1/4
 log ( m) -10
23

4 log(/)

5

6

(a) Synthetic experiment showing phase transition at / = (nm)1/4 and regimes with different rates of convergence. See Theorem 1.

1 | u1, u1 | - [20 40 60] | u2, u2 | - [20 40 60]
0.8 | u1, u1 | - [40 80 120] | u2, u2 | - [40 80 120] 1
0.6 2

correlation

0.4

0.2

0

0 5 10 15 20 25 30

(

i

ni)

1 4

(b) Synthetic experiment showing phase transition

at  = ( k nk)1/4 for odd order tensors. See Corollary 1.

Figure 1: Numerical demonstration of Theorem 1 and Corollary 1.

Several estimators for recovering X  from its noisy version Y have been proposed (see Table 1).

Both the overlapped nuclear norm and latent nuclear norm discussed in [23] achives the relative

performance guarantee

 |||X - X |||F /  Op  nK-1/ ,

(3)

where X is the estimator. This bound implies that if we want to obtain relative error smaller than , we need the signal to noise ratio / to scale as / nK-1/.

Mu et al. [19] proposed the square norm, defined as the nuclear norm of the matrix obtained by grouping the first K/2 indices along the rows and the last K/2 indices along the columns. This norm improves the right hand side of inequality (3) to Op( n K/2 /), which translates to requiring / n K/2 / for obtaining relative error . The intuition here is the more square the unfolding is the better the bound becomes. However, there is no improvement for K = 3.

Richard and Montanari [20] studied the (symmetric version of) model (2) and proved thata recursive unfolding algorithm achieves the factor recovery error dist(u(k), u(k)) =  with / n K/2 /

with high probability, where dist(u, u ) := min( u - u , u + u ). They also showed that the

randomly threshold

in/itializmedatxe(nsonr/po2w, nerKm/2e)thoKd

[7, 16, log K

3] can achieve also with high

the same error probability.



with

slightly

worse

The reasoning underlying both [19] and [20] is that square unfolding is better. However, if we take the (ordinary) mode-k unfolding

Y (k) = u(k) u(k-1)  * * *  u(1)  u(K)  * * *  u(k+1) + E(k),

(4)

we can see (4) as an instance of information plus noise model (1) where m/n = nK-2. Thus the

ordinary unfolding satisfies the condition of Theorem 1 for n or K large enough.

Corollary 1. Consider a K( 3)th order rank one tensor contaminated by Gaussian noise as in (2). There exists a constant C such that if nK-2  C, with probability at least 1 - 4Ke-n, we have

 2CnK

dist2(u (k),

u(k))



  

(/)4

,

  

2C n (/)2

,

if

K-1
n2

>

/



C

1K
n4 4

,

K-1
if /  n 2 ,

for k = 1, . . . , K,

where u(k) is the leading left singular vector of the rectangular unfolding Y (k).

This proves that as conjectured by [20], the threshold / nK/4 applies not only to the even

order case but also to the odd order case. Note that Hopkins et al. [10] have shown a similar

result without the sharp rate of convergence. The above corollary easily extends to more general

n1 x * * * x nK tensor by replacing the conditions by

=k n > /  (C

K k=1

nk )1/4

and

/ 

=k n . The result also holds when X  has rank higher than 1; see Appendix E.

4

We demonstrate this result in Figure 1(b). The models behind the experiment are slightly more

general ones in which [n1, n2, n3] = [20, 40, 60] or [40, 80, 120] and the signal X  is rank two with

1 = 20 and 2 = 10. The plot shows the inner products u(11), u(11) and u(21), u(21) as a measure

of the quality of estimating the two mode-1 factors. The horizontal axis is the normalized noise

standard deviation (

K k=1

nk )1/4 .

We

can

clearly

see

that

the

inner

product

decays

symmetrically

around 1 and 2 as predicted by Corollary 1 for both tensors.

3 Subspace norm for tensors

Suppose the true tensor X   Rnx***xn admits a minimum Tucker decomposition [26] of rank (R, . . . , R):

X =

R i1 =1

*

*

*

R iK =1

i1 i2 ...iK

u(i11)



*

*

*



u(iKK).

(5)

If the core tensor C = the canonical polyadic

((CiP1.)..diKec)ompRoRsixti*o**nxR[9i,s1s5u]p. eTrhdeiamgoondael-,kthuenfaoblodvinegdoefcothmeptrouseititoennsroerdXucescatno

be written as follows:

X

 (k)

=

U (k)C(k)

U (1)  * * *  U (k-1)  U (k+1)  * * *  U (K)

,

(6)

where C(k) is the mode-k unfolding of the core tensor C; U (k) is a n x R matrix U (k) = [u(1k), . . . , u(Rk)] for k = 1, . . . , K. Note that U (k) is not necessarily orthogonal.

Let

X

 (k)

=

P (k)(k)Q(k)

be

the

SVD

of

X

 (k)

.

We

will

observe

that

Q(k)  Span P (1)  * * *  P (k-1)  P (k+1)  * * *  P (K)

(7)

because of (6) and U (k)  Span(P (k)).

Corollary 1 shows that the left singular vectors P (k) can be recovered under mild conditions; thus

the span of the right singular vectors can also be recovered. Inspired by this, we define a norm that

models a tensor X as a mixture of tensors Z(1), . . . , Z(K). We require that the mode-k unfolding

of

Z (k) ,

i.e.

Z ((kk)) ,

has

a

low

rank

factorization

Z

(k) (k)

=

M (k)S(k)

, where M (k)  RnxHK-1 is a

variable, and S(k)  RnK-1xHK-1 is a fixed arbitrary orthonormal basis of some subspace, which

we choose later to have the Kronecker structure in (7).

In the following, we define the subspace norm, suggest an approach to construct the right factor S(k), and prove the denoising bound in the end.

3.1 The subspace norm

Consider a Kth order tensor of size n x * * * n.

Definition 1. Let S(1), . . . , S(K) be matrices such that S(k)  RnK-1xHK-1 with H  n. The subspace norm for a Kth order tensor X associated with {S(k)}Kk=1 is defined as

|||X |||s :=

inf {M (k)}Kk=1 +,

K k=1

M (k)

,

if X  Span({S(k)}Kk=1),

otherwise,

where *  is the nuclear norm, and Span({S(k)}Kk=1) :=

M (1), . . . , M (K), X =

K k=1

foldk

(M

(k) S (k)

).

X  Rnx***xn :

In the next lemma (proven in Appendix D.2), we show the dualnorm of the subspace norm has a simple appealing form. As we see in Theorem 2, it avoids the O( nK-1) scaling (see the first column of Table 1) by restricting the influence of the noise term in the subspace defined by S(1), . . . , S(K).

Lemma 1. The dual norm of |||*|||s is a semi-norm

|||X |||s

=

max
k=1,...,K

X(k)S(k) ,

where * is the spectral norm.

5

3.2 Choosing the subspace

A natural question that arises is how to choose the matrices S(1), . . . , S(k).

Lemma 2.

Let

the

X

 (k)

=

P (k)(k)Q(k)

be the SVD of X(k), where P (k) is n x R and Q(k)

is

nK-1 x R. Assume that R  n and U (k) has full column rank. It holds that for all k,

i) U (k)  Span(P (k)),

ii) Q(k)  Span P (1)  * * *  P (k-1)  P (k+1)  * * *  P (K) .

Proof. We prove the lemma in Appendix D.4.

Corollary 1 shows that when the signal to noise ratio is high enough, we can recover P (k) with high probability. Hence we suggest the following three-step approach for tensor denoising:

(i) For each k, unfold the observation tensor in mode k and compute the top H left singular
(k)
vectors. Concatenate these vectors to obtain a n x H matrix P .

(ii)

Construct

S(k)

as

S(k)

=

P

(1)



*

*

*



P

(k-1)



(k+1)
P



*

**



(K )
P.

(iii) Solve the subspace norm regularized minimization problem

min
X

1 |||Y 2

-

X |||2F

+

|||X |||s,

(8)

where the subspace norm is associated with the above defined {S(k)}Kk=1.

See Appendix B for details.

3.3 Analysis

Let Y  Rnx***xn be a tensor corrupted by Gaussian noise with standard deviation  as follows:

Y = X  + E.

(9)

We define a slightly modified estimator X as follows:

X

=

arg min
X ,{M (k)}Kk=1

1 |||Y 2

-

X |||2F

+ |||X |||s :

X

K
= foldk
k=1

M (k)S(k)

, {M (k)}Kk=1  M() (10)

where M() is a restriction of the set of matrices M (k)  RnxHK-1 , k = 1, . . . , K defined as

follows:

M() :=

{M (k)}Kk=1 :

foldk(M (k))( )





 (n

+

 H K -1 ),

k

=

K

.

This restriction makes sure that M (k), k = 1, . . . , K, are incoherent, i.e., each M (k) has a spectral norm that is as low as a random matrix when unfolded at a different mode . Similar assumptions were used in low-rank plus sparse matrix decomposition [2, 12] and for the denoising bound for the latent nuclear norm [23].

Then we have the following statement (we prove this in Appendix D.3). Theorem 2. Let Xp be any tensor that can be expressed as

K

Xp =

foldk M (pk)S(k)

k=1

,

which satisfies the above incoherence condition {M (pk)}Kk=1  M() and let rk be the rank

of

M

(k) p

for

k

=

1, . . . , K.

In addition, we assume that each S(k) is constructed as S(k)

=

6

(k-1)

(k+1)

(k) (k)

P  * * *  P with (P ) P = IH . Then there are universal constants c0

and c1 suchthat any solution X of the minimization problem (10) with  = |||Xp - X |||s + c0 n + HK-1 + 2 log(K/) satisfies the following bound

|||X - X |||F  |||Xp - X |||F + c1 with probability at least 1 - .

K
k=1 rk,

Note that the right-hand side of the bound consists of two terms. The first term is the approximation error. This term will be zero if X  lies in Span({S(k)}Kk=1). This is the case, if we choose S(k) = InK-1 as in the latent nuclear norm, or if the condition of Corollary 1 is satisfied for the smallest R when we use the Kronecker product construction we proposed. Note that the regularization constant  should also scale with the dual subspace norm of the residual Xp - X .
The second term is the estimation error with respect to Xp. If we take Xp to be the orthogonal projection of X  to the Span({S(k)}Kk=1), we can ignore the contribution of the residual to , because (Xp - X )(k)S(k) = 0. Then the estimation error scales mildly with the dimensions n, HK-1 and with the sum of the ranks. Note that if we take S(k) = InK-1 , we have HK-1 = nK-1, and we recover the guarantee (3) .

4 Experiments

In this section, we conduct tensor denoising experiments on synthetic and real datasets, to numerically confirm our analysis in previous sections.

4.1 Synthetic data

We randomly generated the true rank two tensor X  of size 20x30x40 with singular values 1 = 20 and 2 = 10. The true factors are generated as random matrices with orthonormal columns. The observation tensor Y is then generated by adding Gaussian noise with standard deviation  to X .

Our approach is compared to the CP decomposition, the overlapped approach, and the latent approach. The CP decomposition is computed by the tensorlab [22] with 20 random initializations. We assume CP knows the true rank is 2. For the subspace norm, we use Algorithm 2 described
(k)
in Section 3. We also select the top 2 singular vectors when constructing U 's. We computed the solutions for 20 values of regularization parameter  logarithmically spaced between 1 and 100. For the overlapped and the latent norm, we use ADMM described in [25]; we also computed 20 solutions with the same 's used for the subspace norm.

We measure the performance in the relative error defined as |||X -X |||F /|||X |||F . We report the minimum error obtained by choosing the optimal regularization parameter or the optimal initialization. Although the regularization parameter could be selected by leaving out some entries and measuring the error on these entries, we will not go into tensor completion here for the sake of simplicity.

Figure 2 (a) and (b) show the result of this experiment. The left panel shows the relative error for 3

representative values of  for the subspace norm. The black dash-dotted line shows the minimum

error across all the 's. motivated choice  =

Th(emmaxagke(ntankda+shedHliKne-s1h)o+ws

the error corresponding to the 2 log(K)) for each . The

theoretically two vertical

lines are thresholds of  from Corollary 1 corresponding to 1 and 2, namely, 1/( k nk)1/4 and

2/( k nk)1/4. It confirms that there is a rather sharp increase in the error around the theoretically predicted places (see also Figure 1(b)). We can also see that the optimal  should grow linearly with

. For large  (small SNR), the best relative error is 1 since the optimal choice of the regularization

parameter  leads to predicting with X = 0.

Figure 2 (b) compares the performance of the subspace norm to other approaches. For each method the smallest error corresponding to the optimal choice of the regularization parameter  is shown.

7

Relative Error Relative Error Relative Error


1

=

1.62

2


2

=

5.46


3

=

14.38

min error

suggested

1.5

1 0.9 0.8 0.7

1.8

cp

1.6 subspace latent

1.4

overlap suggested

1.2 optimistic

0.6 1

0.5 1 0.8
0.4 0.6
0.3

0.5

0.2

cp subspace

0.4

0.1

latent overlap

0.2

0 0

0.5

1

1.5

0 20

0.5

1

optimistic

0

1.5 2 0 500 1000 1500 2000

 

(a) (b) (c)

Figure 2: Tensor denoising. (a) The subspace approach with three representative 's on synthetic data. (b) Comparison of different methods on synthetic data. (c) Comparison on amino acids data.

In addition, to place the numbers in context, we plot the line corresponding to

Relative error =

R

k nk log(K) |||X |||F

*

,

(11)

which we call "optimistic". This can be motivated from considering the (non-tractable) maximum likelihood estimator for CP decomposition (see Appendix A).

Clearly, the error of CP, the subspace norm, and "optimistic" grows at the same rate, much slower than overlap and latent. The error of CP increases beyond 1, as no regularization is imposed (see Appendix C for more experiments). We can see that both CP and the subspace norm are behaving near optimally in this setting, although such behavior is guaranteed for the subspace norm whereas it is hard to give any such guarantee for the CP decomposition based on nonlinear optimization.

4.2 Amino acids data
The amino acid dataset [5] is a semi-realistic dataset commonly used as a benchmark for low rank tensor modeling. It consists of five laboratory-made samples, each one contains different amounts of tyrosine, tryptophan and phenylalanine. The spectrum of their excitation wavelength (250-300 nm) and emission (250-450 nm) are measured by fluorescence, which gives a 5 x 201 x 61 tensor. As the true factors are known to be these three acids, this data perfectly suits the CP model. The true rank is fed into CP and the proposed approach as H = 3. We computed the solutions of CP for 20 different random initializations, and the solutions of other approaches with 20 different values of . For the subspace and the overlapped approach, 's are logarithmically spaced between 103 and 105. For the latent approach, 's are logarithmically spaced between 104 and 106. Again, we include the optimistic scaling (11) to put the numbers in context.
Figure 2(c) shows the smallest relative error achieved by all methods we compare. Similar to the synthetic data, both CP and the subspace norm behaves near ideally, though the relative error of CP can be larger than 1 due to the lack of regularization. Interestingly the theoretically suggested scaling of the regularization parameter  is almost optimal.

5 Conclusion
We have settled a conjecture posed by [20] and showed that indeed O(nK/4) signal-to-noise ratio is sufficient also for odd order tensors. Moreover, our analysis shows an interesting two-phase behavior of the error. This finding lead us to the development of the proposed subspace norm. The
(1) (K)
proposed norm is defined with respect to a set of orthonormal matrices P , . . . , P , which are estimated by mode-wise singular value decompositions. We have analyzed the denoising performance of the proposed norm, and shown that the error can be bounded by the sum of two terms, which can be interpreted as an approximation error term coming from the first (non-convex) step, and an estimation error term coming from the second (convex) step.

8

