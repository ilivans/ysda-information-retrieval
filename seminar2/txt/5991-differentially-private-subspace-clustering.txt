Differentially Private Subspace Clustering
Yining Wang, Yu-Xiang Wang and Aarti Singh Machine Learning Department, Carnegie Mellon Universty, Pittsburgh, USA
{yiningwa,yuxiangw,aarti}@cs.cmu.edu
Abstract
Subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple "clusters" so that data points in a single cluster lie approximately on a low-dimensional linear subspace. It is originally motivated by 3D motion segmentation in computer vision, but has recently been generically applied to a wide range of statistical machine learning problems, which often involves sensitive datasets about human subjects. This raises a dire concern for data privacy. In this work, we build on the framework of differential privacy and present two provably private subspace clustering algorithms. We demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees; the other one asymptotically preserves differential privacy while having good performance in practice. Along the course of the proof, we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests.
1 Introduction
Subspace clustering was originally proposed to solve very specific computer vision problems having a union-of-subspace structure in the data, e.g., motion segmentation under an affine camera model [11] or face clustering under Lambertian illumination models [15]. As it gains increasing attention in the statistics and machine learning community, people start to use it as an agnostic learning tool in social network [5], movie recommendation [33] and biological datasets [19]. The growing applicability of subspace clustering in these new domains inevitably raises the concern of data privacy, as many such applications involve dealing with sensitive information. For example, [19] applies subspace clustering to identify diseases from personalized medical data and [33] in fact uses subspace clustering as a effective tool to conduct linkage attacks on individuals in movie rating datasets. Nevertheless, privacy issues in subspace clustering have been less explored in the past literature, with the only exception of a brief analysis and discussion in [29]. However, the algorithms and analysis presented in [29] have several notable deficiencies. For example, data points are assumed to be incoherent and it only protects the differential privacy of any feature of a user rather than the entire user profile in the database. The latter means it is possible for an attacker to infer with high confidence whether a particular user is in the database, given sufficient side information.
It is perhaps reasonable why there is little work focusing on private subspace clustering, which is by all means a challenging task. For example, a negative result in [29] shows that if utility is measured in terms of exact clustering, then no private subspace clustering algorithm exists when neighboring databases are allowed to differ on an entire user profile. In addition, state-of-the-art subspace clustering methods like Sparse Subspace Clustering (SSC, [11]) lack a complete analysis of its clustering output, thanks to the notorious "graph connectivity" problem [21]. Finally, clustering could have high global sensitivity even if only cluster centers are released, as depicted in Figure 1. As a result, general private data releasing schemes like output perturbation [7, 8, 2] do not apply.
In this work, we present a systematic and principled treatment of differentially private subspace clustering. To circumvent the negative result in [29], we use the perturbation of recovered low-
1

dimensional subspace from the ground truth as the utility measure. Our contributions are two-fold. First, we analyze two efficient algorithms based on the sample-aggregate framework [22] and established formal privacy and utility guarantees when data are generated from some stochastic model or satisfy certain deterministic separation conditions. New results on (non-private) subspace clustering are obtained along our analysis, including a fully agnostic subspace clustering on well-separated datasets using stability arguments and exact clustering guarantee for thresholding-based subspace clustering (TSC, [14]) in the noisy setting. In addition, we employ the exponential mechanism [18] and propose a novel Gibbs sampler for sampling from this distribution, which involves a novel tweak in sampling from a matrix Bingham distribution. The method works well in practice and we show it is closely related to the well-known mixtures of probabilistic PCA model [27].

Related work Subspace clustering can be thought as a generalization of PCA and k-means clustering. The former aims at finding a single low-dimensional subspace and the latter uses zerodimensional subspaces as cluster centers. There has been extensive research on private PCA [2, 4, 10] and k-means [2, 22, 26]. Perhaps the most similar work to ours is [22, 4]. [22] applies the sample-aggregate framework to k-means clustering and [4] employs the exponential mechanism to recover private principal vectors. In this paper we give non-trivial generalization of both work to the private subspace clustering setting.

2 Preliminaries

2.1 Notations

For a vector x  Rd, its p-norm is defined as then the 2-norm is used. For a matrix A 

x p= Rnxm,

( we

i xpi )1/p. If use 1(A)

p is not  ***

explicitly specified  n(A)  0 to

denote its singular values (assuming without loss of generality that n  m). We use *  to

denote matrix norms, with  = 2 the matrix spectral norm and  = F the Frobenious norm. That

is, A 2 = 1(A) and A F =

n i=1

i(A)2.

For a q-dimensional subspace S



Rd, we

associate with a basis U  Rdxq, where the q columns in U are orthonormal and S = range(U).

We use Sdq to denote the set of all q-dimensional subspaces in Rd.

Given x  Rd and S  Rd, the distance d(x, S) is defined as d(x, S) = infyS x - y 2. If S is
a subspace associated with a basis U , then we have d(x, S) = x - PS (x) 2 = x - UU x 2, where PS (*) denotes the projection operator onto subspace S. For two subspaces S, S of dimension q, the distance d(S, S ) is defined as the Frobenious norm of the sin matrix of principal angles; i.e.,

d(S, S ) = sin (S, S ) F = UU - U U F , where U, U are orthonormal basis associated with S and S , respectively.

(1)

2.2 Subspace clustering

Given n data points x1, * * * , xn  Rd, the task of subspace clustering is to cluster the data points into k clusters so that data points within a subspace lie approximately on a low-dimensional subspace. Without loss of generality, we assume xi 2  1 for all i = 1, * * * , n. We also use X = {x1, * * * , xn} to denote the dataset and X  Rdxn to denote the data matrix by stacking all data points in columnwise order. Subspace clustering seeks to find k q-dimensional subspaces C = {S1, * * * , Sk} so as to minimize the Wasserstein's distance or distance squared defined as

k

d2W

(C,

C)

=

min
:[k][k]

d2(Si, S(i)),

i=1

(2)

where  are taken over all permutations on [k] and S are the optimal/ground-truth subspaces. In a

model based approach, stochastically from one

C of

is fixed and data the ground-truth

spuobinsptsac{exsi}inni=C1awreitghennoeirsaetecdoerriuthpetriodne;teformr ainciostmicpallelyteolyr

agnostic setting, C is defined as the minimizer of the k-means subspace clustering objective:

C

:=

argminC={S1,*** ,Sk}Sdq cost(C; X )

=

argminC={S1,*** ,Sk}Sdq

1 n

n

min d2(xi, Sj).
j

i=1

To simplify notations, we use k(X ) = cost(C; X ) to denote cost of the optimal solution.

(3)

2

Algorithm 1 The sample-aggregate framework [22]

1: Input: X = {xi}ni=1  Rd, number of subsets m, privacy parameters , ; f , dM. 2: Initialize: s = m,  = /(5 2 ln(2/)),  = /(4(D + ln(2/))).

3: Subsampling: Select m random subsets of size n/m of X independently and uniformly at

random without replacement. Repeat this step until no single data point appears in more than

m of the sets. Mark the subsampled subsets XS1 , * * * , XSm .

4: 5:

SAegpgarreagtaetiqoune:rCieosm: Cpuotme pgu(tBe)B==s{isiw}hmi=e1reiR=D

, where si = argminmi=1ri

f (XSi ). (t0) with

t0

=

(

m+s 2

+ 1).

Here

ri(t0) denotes the distance dM(*, *) between si and the t0-th nearest neighbor to si in B.

6: Noise calibration: Compute S(B) = 2 maxk((t0 + (k + 1)s) * e-k), where (t) is the mean

of the top s/ values in {r1(t), * * * , rm(t)}.

7:

Output:

A(X ) = g(B) +

S(B) 

u,

where

u

is

a

standard

Gaussian

random

vector.

2.3 Differential privacy

Definition 2.1 (Differential privacy, [7, 8]). A randomized algorithm A is (, )-differentially private if for all X , Y satisfying d(X , Y) = 1 and all sets S of possible outputs the following holds:

Pr[A(X )  S]  e Pr[A(Y)  S] + .

(4)

In addition, if  = 0 then the algorithm A is -differentially private.

In our setting, the distance d(*, *) between two datasets X and Y is defined as the number of different columns in X and Y. Differential privacy ensures the output distribution is obfuscated to the point that every user has a plausible deniability about being in the dataset, and in addition any inferences about individual user will have nearly the same confidence before and after the private release.

3 Sample-aggregation based private subspace clustering
In this section we first summarize the sample-aggregate framework introduced in [22] and argue why it should be preferred to conventional output perturbation mechanisms [7, 8] for subspace clustering. We then analyze two efficient algorithms based on the sample-aggregate framework and prove formal privacy and utility guarantees. We also prove new results in our analysis regarding the stability of k-means subspace clustering (Lem. 3.3) and graph connectivity (i.e., consistency) of noisy threshold-based subspace clustering (TSC, [14]) under a stochastic model (Lem. 3.5).

3.1 Smooth local sensitivity and the sample-aggregate framework

Most existing privacy frameworks [7, 8] are

based on the idea of global sensitivity, which

is defined as the maximum output perturbation

f (X1) - f (X2) , where maximum is over all neighboring databases X1, X2 and  = 1 or
2. Unfortunately, global sensitivity of cluster-

ing problems is usually high even if only clus-

ter centers are released. For example, Figure

1 shows that the global sensitivity of k-means

subspace clustering could be as high as O(1), Figure 1: Illustration of instability of k-means

which ruins the algorithm utility.

subspace clustering solutions (d = 2, k = 2, q =

1). Blue dots represent evenly spaced data points

To circumvent the above-mentioned chal- on the unit circle; blue crosses indicate an addi-

lenges, Nissim et al. [22] introduces the tional data point. Red lines are optimal solutions.

sample-aggregate framework based on the con-

cept of a smooth version of local sensitivity.
Unlike global sensitivity, local sensitivity measures the maximum perturbation f (X ) - f (X )  over all databases X neighboring to the input database X . The proposed sample-aggregate frame-

work (pseudocode in Alg. 1) enjoys local sensitivity and comes with the following guarantee:

Theorem 3.1 ([22], Theorem 4.2). Let f : D  RD be an efficiently computable function where D is the collection of all databases and D is the output dimension. Let dM(*, *) be a semimetric on

3

the outer space of f .

1

Set 

>

 2D/ m and m

=

(log2 n).

The sample-aggregate algorithm

A in Algorithm 1 is an efficient (, )-differentially private algorithm. Furthermore, if f and m are

chosen such that the 1 norm of the output of f is bounded by  and

Pr
XS X

[dM(f (XS), c)



r]



3 4

(5)

for

some

c



RD

and

r>


0,

then

the

standard

deviation

of

Gaussian

noise

added

is

upper

bounded

by

O(r/)

+

 

e-(



m D

)

.

In addition, when m satisfies m

= (D2 log2(r/)/2), with high

probability each coordinate of A(X ) - c is upper bounded by O(r/), where c depending on A(X )

satisfies dM(c, c) = O(r).

Let f be any subspace clustering solver that outputs k estimated low-dimensional subspaces and dM be the Wasserstein's distance as defined in Eq. (2). Theorem 3.1 provides privacy guarantee for an efficient meta-algorithm with any f . In addition, utility guarantee holds with some more assumptions on input dataset X . In following sections we establish utility guarantees. The main
idea is to prove stability results as outlined in Eq. (5) for particular subspace clustering solvers and
then apply Theorem 3.1.

3.2 The agnostic setting

sWetetifinrgstthceonospitdimeratlhseosluettitoinngCwhiesndedfiatnaedpoaisnttshe{xoni}eni=th1atarmeianribmitirzaersiltyheplka-cmede.anUsncdoesrt

such as in

agnostic Eq. (3).

The solver f is taken to be any (1 + )-approximation2 of optimal k-means subspace clustering; that

is, f always outputs subspaces C satisfying cost(C; X )  (1 + )cost(C; X ). Efficient core-set

based approximation algorithms exist, for example, in [12]. The key task of this section it to identify

assumptions under which the stability condition in Eq. (5) holds with respect to an approximate

solver f . The example given in Figure 1 also suggests that identifiability issue arises when the input

data X itself cannot be well clustered. For example, no two straight lines could well approximate

data uniformly distributed on a circle. To circumvent the above-mentioned difficulty, we impose the

following well-separation condition on the input data X :

Definition 3.2 (Well-separation condition for k-means subspace clustering). A dataset X is (, , )-well separated if there exist constants ,  and , all between 0 and 1, such that

2k(X )  min 22k-1(X ), 2k,-(X ) - , 2k,+(X ) +  ,

(6)

where k-1, k,- and k,+ are defined as 2k-1(X ) = minS1:k-1Sdq cost({Si}; X ); 2k,-(X ) = minS1Sdq-1,S2:kSdq cost({Si}; X ); and 2k,+(X ) = minS1Sdq+1,S2:kSdq cost({Si}; X ).

The first condition in Eq. (6), 2k(X )  22k-1(X ), constrains that the input dataset X cannot be well clustered using k - 1 instead of k clusters. It was introduced in [23] to analyze stability of
k-means solutions. For subspace clustering, we need another two conditions regarding the intrinsic dimension of each subspace. The 2k(X )  2k,-(X ) -  asserts that replacing a q-dimensional subspace with a (q - 1)-dimensional one is not sufficient, while 2k(X )  2k,+(X ) +  means an additional subspace dimension does not help much with clustering X .

The following lemma is our main stability result for subspace clustering on well-separated datasets. It states that when a candidate clustering C is close to the optimal clustering C in terms of clustering
cost, they are also close in terms of the Wasserstein distance defined in Eq. (2).

Lemma 3.3 (Stability of agnostic k-means subspace clustering). Assume X is (, , )-well sepa-

rated with 2 < 1/1602,  > . Suppose a candidate clustering C = {S1, * * * , Sk}  Sdq satisfies

cost(C; X )



a

*

cost(C; X )

for

some

a

<

1-8022 8002

.

Then

the

following

holds:

dW (C, C)



(1

600 22 k - 1502)( -

. )

(7)

The following theorem is then a simple corollary, with a complete proof in Appendix B.

1dM(*, *) satisfies dM(x, y)  0, dM(x, x) = 0 and dM(x, y)  dM(x, z) + dM(y, z) for all x, y, z. 2Here is an approximation constant and is not related to the privacy parameter .

4

Algorithm 2 Threshold-based subspace clustering (TSC), a simplified version

1: 2:

Input: X = {xi}ni=1  Rd, Thresholding: construct G

number of clusters k and number  {0, 1}nxn by connecting xi to

of neighbors s. the other s data

points

in

X

with

the largest absolute inner products | xi, x |. Complete G so that it is undirected. 3: Clustering: Let X (1), * * * , X ( ) be the connected components in G. Construct X( ) by sam-

pling q points from X ( ) uniformly at random without replacement.

4: Output: subspaces C = {S( )}k=1; S( ) is the subspace spanned by q arbitrary points in X( ).

Theorem 3.4. Fix a (, , )-well separated dataset X with n data points and 2 < 1/1602,

 > . Suppose XS  X is a subset of X with size m, sampled uniformly at random without

replacement. Let C = {S1, * * * , S2} be an (1 + )-approximation of optimal k-means subspace

clustering

computed

on

XS .

If

m

=

(

kqd

log(qd/ 2k  24k(X )

(X

))

)

with



<

1-8022 8002

- 2(1 +

), then we

have:



Pr
XS

dW (C, C)



(1

600 22 k - 1502)( -

)



3 ,

4

(8)

where C = {S1, * * * , Sk} is the optimal clustering on X ; that is, cost(C; X ) = 2k(X ).

Consequently, applying Theorem 3.4 together with the sample-aggregate framework we obtain a

weak polynomial-time -differentially private algorithm for agnostic k-means subspace clustering,

with

additional

amount

of

per-coordinate

Gaussian

noise

upper

bounded

by

O

(

2 k (-)

).

Our

bound

is comparable to the one obtained in [22] for private k-means clustering, except for the ( - ) term

which characterizes the well-separatedness under the subspace clustering scenario.

3.3 The stochastic setting

We further consider the case when data points are stochastically generated from some underlying "true" subspace set C = {S1, * * * , Sk}. Such settings were extensively investigated in previous development of subspace clustering algorithms [24, 25, 14]. Below we give precise definition of the
considered stochastic subspace clustering model:

The stochastic model For every cluster associated with subspace S, a data point x(i )  Rd

belonging to random from

cluster {y  S

can be : y2

written as x(i ) = 1} and i 

N=(0y, (i)2 /+d

(i ), * Id)

where y(i ) is sampled uniformly for some noise parameter .

at

Under the stochastic setting we consider the solver f to be the Threshold-based Subspace Clustering
(TSC, [14]) algorithm. A simplified version of TSC is presented in Alg. 2. An alternative idea is to
apply results in the previous section since the stochastic model implies well-separated dataset when noise level  is small. However, the running time of TSC is O(n2d), which is much more efficient than core-set based methods. TSC is provably correct in that the similarity graph G has no false
connections and is connected per cluster, as shown in the following lemma:

Lemma 3.5 (Connectivity of TSC). Fix  > 1 and assume max 0.04n  s  min n /6. If for every  {1, * * * , k}, the number of data points n and the noise level  satisfy

n > 2q(12)q-1 ; (1 + ) q 

1

d2(S, S )

- 1 - min

;

log n 0.01(q/2 - 1)(q - 1) log n d 15 log n

=q

1

d  2q log n q-1

 < cos 12

- cos

24 log n

n

1
0.01(q/2- 1)(q - 1) q-1 

,

 where  = 2 5 + 2. Then with probability at least 1 - n2e- d - n e-n /400 -

n1-/( log n ) - 12/n - n e-c(n -1), the connected components in G correspond ex-

actly to the k subspaces.

Conditions in Lemma 3.5 characterize the interaction between sample complexity n , noise level  and "signal" level min = d(S, S ). Theorem 3.6 is then a simple corollary of Lemma 3.5. Complete proofs are deferred to Appendix C.

5

Theorem 3.6 (Stability of TSC on stochastic data). Assume conditions in Lemma 3.5 hold with respect to n = n/m for (log2 n)  m  o(n). Assume in addition that limn n =  for all
= 1, * * * , L and the failure probability does not exceed 1/8. Then for every > 0 we have

lim Pr
n XS

dW (C, C) >

= 0.

(9)

Compared to Theorem 3.4 for the agnostic model, Theorem 3.6 shows that one can achieve consis-

tent estimation of underlying subspaces under a stochastic model. It is an interesting question to

derive finite sample bounds for the differentially private TSC algorithm.

3.4 Discussion
It is worth noting that the sample-aggregate framework is an (, )-differentially private mechanism for any computational subroutine f . However, the utility claim (i.e., the O(r/) bound on each coordinate of A(X ) - c) requires the stability of the particular subroutine f , as outlined in Eq. (5). It is unfortunately hard to theoretically argue for stability of state-of-the-art subspace clustering methods such as sparse subspace cluster (SSC, [11]) due to the "graph connectivity" issue [21]3. Nevertheless, we observe satisfactory performance of SSC based algorithms in simulations (see Sec. 5). It remains an open question to derive utility guarantee for (user) differentially private SSC.

4 Private subspace clustering via the exponential mechanism

In Section 3 we analyzed two algorithms with provable privacy and utility guarantees for sub-

space clustering based on the sample-aggregate framework. However, empirical evidence shows

that sample-aggregate based private clustering suffers from poor utility in practice [26]. In this sec-

tion, we propose a practical private subspace clustering algorithm based on the exponential mecha-

nism [18]. In particular, given the dataset X with n data points, we propose to samples parameters  = ({S }k=1, {zi}ni=1) where S  Sqd, zj  {1, * * * , k} from the following distribution:

p(; X )  exp

- * 2

n

d2(xi, Szi )

,

i=1

(10)

where  > 0 is the privacy parameter. The following proposition shows that exact sampling from

the distribution in Eq. (10) results in a provable differentially private algorithm. Its proof is trivial

and is deferred to Appendix D.1. Note that unlike sample-aggregate based methods, the exponential

mechanism can privately release clustering assignment z. This does not violate the lower bound in

[29] because the released clustering assignment z is not guaranteed to be exactly correct.

Proposition 4.1. The random algorithm A : X   that outputs one sample from the distribution

defined in Eq. (10) is -differential private.

4.1 A Gibbs sampling implementation

It is hard in general to sample parameters from distributions as complicated as in Eq. (10). We present a Gibbs sampler that iteratively samples subspaces {Si} and cluster assignments {zj} from their conditional distributions.

Update of zi: When {S } and z-i are fixed, the conditional distribution of zi is

p(zi|{S }k=1, z-i; X )  exp(-/2 * d2(xi, Szi )).

(11)

Since d(xi, Szi ) can be efficiently computed (given an orthonormal basis of Szi ), update of zi can be easily done by sampling zj from a categorical distribution.

Update of S : Let X ( ) = {xi  X : zi = } denote data points that are assigned to cluster and n = |X ( )|. Denote X( )  Rdxn as the matrix with columns corresponding to all data points in X ( ). The distribution over S conditioned on z can then be written as
p(S = range(U )|z; X )  exp(/2 * tr(U A U )); U  Rdxq, U U = Iqxq, (12)
where A = X( )X( ) is the unnormalized sample covariance matrix. Distribution of the form in Eq. (12) is a special case of the matrix Bingham distribution, which admits a Gibbs sampler [16]. We give implementation details in Appendix D.2 with modifications so that the resulting Gibbs sampler is empirically more efficient for a wide range of parameter settings.

3Recently [28] established full clustering guarantee for SSC, however, under strong assumptions.

6

4.2 Discussion The proposed Gibbs sampler resembles the k-plane algorithm for subspace clustering [3]. It is in fact a "probabilistic" version of k-plane since sampling is performed at each iteration rather than deterministic updates. Furthermore, the proposed Gibbs sampler could be viewed as posterior sampling for the following generative model: first sample U uniformly at random from Sdq for each subspace S ; afterwards, cluster assignments {zi}ni=1 are sampled such that Pr[zi = j] = 1/k and xi is set as xi = U yi + PU wi, where yi is sampled uniformly at random from the qdimensional unit ball and wi  N (0, Id/). Connection between the above-mentioned generative model and Gibbs sampler is formally justified in Appendix D.3. The generative model is strikingly similar to the well-known mixtures of probabilistic PCA (MPPCA, [27]) model by setting variance parameters  in MPPCA to 1/. The only difference is that yi are sampled uniformly at random from a unit ball 4 and noise wi is constrained to U, the complement space of U . Note that this is closely related to earlier observation that "posterior sampling is private" [20, 6, 31], but different in that we constructed a model from a private procedure rather than the other way round.
As the privacy parameter    (i.e., no privacy guarantee), we arrive immediately at the exact k-plane algorithm and the posterior distribution concentrates around the optimal k-means solution (C, z). This behavior is similar to what a small-variance asymptotic analysis on MPPCA models reveals [30]. On the other hand, the proposed Gibbs sampler is significantly different from previous Bayesian probabilisitic PCA formulation [34, 30] in that the subspaces are sampled from a matrix Bingham distribution. Finally, we remark that the proposed Gibbs sampler is only asymptotically private because Proposition 4.1 requires exact (or nearly exact [31]) sampling from Eq. (10).
5 Numerical results
We provide numerical results of both the sample-aggregate and Gibbs sampling algorithms on synthetic and real-world datasets. We also compare with a baseline method implemented based on the k-plane algorithm [3] with perturbed sample covariance matrix via the SuLQ framework [2] (details presented in Appendix E). Three solvers are considered for the sample-aggregate framework: threshold-based subspace clustering (TSC, [14]), which has provable utility guarantee with sampleaggregation on stochastic models, along with sparse subspace clustering (SSC, [11]) and low-rank representation (LRR, [17]), the two state-of-the-art methods for subspace clustering. For Gibbs sampling, we use non-private SSC and LRR solutions as initialization for the Gibbs sampler. All methods are implemented using Matlab.
For synthetic datasets, we first generate k random q-dimensional linear subspaces. Each subspace is generated by first sampling a d x q random Gaussian matrix and then recording its column space. n data points are then assigned to one of the k subspaces (clusters) uniformly at random. To generate a data point xi assigned with subspace S , we first sample yi  Rq with yi 2 = 1 uniformly at random from the q-dimensional unit sphere. Afterwards, xi is set as xi = U yi + wi, where U  Rdxq is an orthonormal basis associated with S and wi  N (0, 2Id) is a noise vector.
Figure 2 compares the utility (measured in terms of k-means objective cost(C; X ) and the Wasserstein's distance dW (C, C)) of sample aggregation, Gibbs sampling and SuLQ subspace clustering. As shown in the plots, sample-aggregation algorithms have poor utility unless the privacy parameter  is truly large (which means very little privacy protection). On the other hand, both Gibbs sampling and SuLQ subspace clustering give reasonably good performance. Figure 2 also shows that SuLQ scales poorly with the ambient dimension d. This is because SuLQ subspace clustering requires calibrating noise to a d x d sample covariance matrix, which induces much error when d is large. Gibbs sampling seems to be robust to various d settings.
We also experiment on real-world datasets. The right two plots in Figure 2 report utility on a subset of the extended Yale Face Dataset B [13] for face clustering. 5 random individuals are picked, forming a subset of the original dataset with n = 320 data points (images). The dataset is preprocessed by projecting each individual onto a 9D affine subspace via PCA. Such preprocessing step was adopted in [32, 29] and was theoretically justified in [1]. Afterwards, ambient dimension of the entire dataset is reduced to d = 50 by random Gaussian projection. The plots show that Gibbs sampling significantly outperforms the other algorithms.
4In MPPCA latent variables yi are sampled from a normal distribution N (0, 2Iq).
7

K-means cost

s.a., SSC 0.3 s.a., TSC
s.a., LRR 0.25 exp., SSC
exp. LRR 0.2 SuLQ-10
SuLQ-50 0.15
0.1
0.05
0 -1 -0.5 0 0.5 1 1.5 2 2.5 3
Log10
3
2.5
2
1.5 s.a., SSC s.a., TSC
1 s.a., LRR exp., SSC
0.5 exp. LRR
0 SuLQ-10 SuLQ-50
-0.5 -1 -0.5 0 0.5 1 1.5 2 2.5 3 Log10

Wasserstein distance

K-means cost

0.7
0.6
0.5
0.4 s.a., SSC
0.3 s.a., TSC s.a., LRR
0.2 exp., SSC exp. LRR
0.1 SuLQ-10 SuLQ-50
0 -1 -0.5 0 0.5 1 1.5 2 2.5 3
Log10
4
3.5
3
2.5
2 s.a., SSC s.a., TSC
1.5 s.a., LRR exp., SSC
1 exp. LRR 0.5 SuLQ-10
SuLQ-50 0 -1 -0.5 0 0.5 1 1.5 2 2.5 3
Log10

K-means cost

Wasserstein distance

0.9
0.8
0.7
0.6
0.5 s.a., SSC 0.4 s.a., TSC
s.a., LRR 0.3 exp., SSC 0.2 exp. LRR
SuLQ-10 0.1 SuLQ-50
0 -1 -0.5 0 0.5 1 1.5 2 2.5 3
Log10
9
8
7 s.a., SSC
6 s.a., TSC s.a., LRR
5 exp., SSC exp. LRR
4 SuLQ-10 SuLQ-50
3
2 -1 -0.5 0 0.5 1 1.5 2 2.5 3
Log10

Wasserstein distance

Figure 2: Utility under fixed privacy budget . Top row shows k-means cost and bottom row shows the Wasserstein's distance dW (C, C). From left to right: synthetic dataset, n = 5000, d = 5, k = 3, q = 3,  = 0.01; n = 1000, d = 10, k = 3, q = 3,  = 0.1; extended Yale Face Dataset B (a subset). n = 320, d = 50, k = 5, q = 9,  = 0.01.  is set to 1/(n ln n) for (, )-privacy
algorithms. "s.a." stands for smooth sensitivity and "exp." stands for exponential mechanism.
"SuLQ-10" and "SuLQ-50" stand for the SuLQ framework performing 10 and 50 iterations. Gibbs
sampling is run for 10000 iterations and the mean of the last 100 samples is reported.

Test statistic K-means cost Wasserstein distance

1 =0.1
0.8 =1 =10
0.6 =100
0.4
0.2
0 0 20 40 60 80 100
x 100 iterations

0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1
0 0 20 40 60 80 100
x 100 iterations

4 3.5
3 2.5
2 1.5
1 0.5
0 0 20 40 60 80 100
x 100 iterations

Figure 3: Test statistics, k-means cost and dW (C, C) of 8 trials of the Gibbs sampler under different privacy settings. Synthetic dataset setting: n = 1000, d = 10, k = 3, q = 3,  = 0.1.

In Figure 3 we investigate the mixing behavior of proposed Gibbs sampler. We plot for multiple trials of Gibbs sampling the k-means objective, Wasserstein's distance and a test statistic 1/ kq *

(

k =1

1/T *

T t=1

U(t)

2 F

)1/2,

where

U(t)

is

a

basis

sample

of

S

at the tth iteration.

The test

statistic has mean zero under distribution in Eq. (10) and a similar statistic was used in [4] as a

diagnostic of the mixing behavior of another Gibbs sampler. Figure 3 shows that under various

privacy parameter settings, the proposed Gibbs sampler mixes quite well after 10000 iterations.

6 Conclusion
In this paper we consider subspace clustering subject to formal differential privacy constraints. We analyzed two sample-aggregate based algorithms with provable utility guarantees under agnostic and stochastic data models. We also propose a Gibbs sampling subspace clustering algorithm based on the exponential mechanism that works well in practice. Some interesting future directions include utility bounds for state-of-the-art subspace clustering algorithms like SSC or LRR.

Acknowledgement This research is supported in part by grant NSF CAREER IIS-1252412, NSF Award BCS-0941518, and a grant by Singapore National Research Foundation under its International Research Centre @ Singapore Funding Initiative administered by the IDM Programme Office.

8

References
[1] R. Basri and D. Jacobs. Lambertian reflectance and linear subspaces. IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(2):218-233, 2003.
[2] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the SULQ framework. In PODS, 2015.
[3] P. S. Bradley and O. L. Mangasarian. k-plane clustering. Journal of Global Optimization, 16(1), 2000. [4] K. Chaudhuri, A. Sarwate, and K. Sinha. Near-optimal algorithms for differentially private principal
components. In NIPS, 2012. [5] Y. Chen, A. Jalali, S. Sanghavi, and H. Xu. Clustering partially observed graphs via convex optimization.
The Journal of Machine Learning Research, 15(1):2213-2238, 2014. [6] C. Dimitrakakis, B. Nelson, A. Mitrokotsa, and B. I. Rubinstein. Robust and private bayesian inference.
In Algorithmic Learning Theory, pages 291-305. Springer, 2014. [7] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov, and M. Naor. Our data, ourselves: Privacy via
distributed noise generation. In EUROCRYPT, 2006. [8] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis.
In TCC, 2006. [9] C. Dwork and A. Roth. The algorithmic foundations of differential privacy. Foundations and Trends in
Theoretical Computer Science, 9(3-4):211-407, 2014. [10] C. Dwork, K. Talwar, A. Thakurta, and L. Zhang. Analyze Gauss: Optimal bounds for privacy-preserving
principal component analysis. In STOC, 2014. [11] E. Elhamifar and R. Vidal. Sparse subspace clustering: Algorithm, theory and applications. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 35(11):2765-2781, 2013. [12] D. Feldman, M. Schmidt, and C. Sohler. Turning big data into tiny data: Constant-size coresets for
k-means, pca and projective clustering. In SODA, 2013. [13] A. Georghiades, P. Belhumeur, and D. Kriegman. From few to many: Illumination cone models for
face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(6):643-660, 2001. [14] R. Heckel and H. Bolcskei. Robust subspace clustering via thresholding. arXiv:1307.4891, 2013. [15] J. Ho, M.-H. Yang, J. Lim, K.-C. Lee, and D. Kriegman. Clustering appearances of objects under varying illumination conditions. In CVPR, 2003. [16] P. Hoff. Simulation of the matrix bingham-conmises-fisher distribution, with applications to multivariate and relational data. Journal of Computational and Graphical Statistics, 18(2):438-456, 2009. [17] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Ma, and Y. Yu. Robust recovery of subspace structures by low-rank representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(1):171-184, 2012. [18] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, 2007. [19] B. McWilliams and G. Montana. Subspace clustering of high-dimensional data: a predictive approach. Data Mining and Knowledge Discovery, 28(3):736-772, 2014. [20] D. J. Mir. Differential privacy: an exploration of the privacy-utility landscape. PhD thesis, Rutgers University, 2013. [21] B. Nasihatkon and R. Hartley. Graph connectivity in sparse subspace clustering. In CVPR, 2011. [22] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. In STOC, 2007. [23] R. Ostrovksy, Y. Rabani, L. Schulman, and C. Swamy. The effectiveness of Lloyd-type methods for the k-means problem. In FOCS, 2006. [24] M. Soltanolkotabi, E. J. Candes, et al. A geometric analysis of subspace clustering with outliers. The Annals of Statistics, 40(4):2195-2238, 2012. [25] M. Soltanolkotabi, E. Elhamifa, and E. Candes. Robust subspace clustering. The Annals of Statistics, 42(2):669-699, 2014. [26] D. Su, J. Cao, N. Li, E. Bertino, and H. Jin. Differentially private k-means clustering. arXiv, 2015. [27] M. Tipping and C. Bishop. Mixtures of probabilistic principle component anlyzers. Neural computation, 11(2):443-482, 1999. [28] Y. Wang, Y.-X. Wang, and A. Singh. Clustering consistent sparse subspace clustering. arXiv, 2015. [29] Y. Wang, Y.-X. Wang, and A. Singh. A deterministic analysis of noisy sparse subspace clustering for dimensionality-reduced data. In ICML, 2015. [30] Y. Wang and J. Zhu. DP-space: Bayesian nonparametric subspace clustering with small-variance asymptotic analysis. In ICML, 2015. [31] Y.-X. Wang, S. Fienberg, and A. Smola. Privacy for free: Posterior sampling and stochastic gradient monte carlo. In ICML, 2015. [32] Y.-X. Wang and H. Xu. Noisy sparse subspace clustering. In ICML, pages 89-97, 2013. [33] A. Zhang, N. Fawaz, S. Ioannidis, and A. Montanari. Guess who rated this movie: Identifying users through subspace clustering. arXiv, 2012. [34] Z. Zhang, K. L. Chan, J. Kwok, and D.-Y. Yeung. Bayesian inference on principal component analysis using reversible jump markov chain monte carlo. In AAAI, 2004.
9

