Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path

Daniel Hsu Columbia University
djhsu@cs.columbia.edu

Aryeh Kontorovich Ben-Gurion University
karyeh@cs.bgu.ac.il

Csaba Szepesvari University of Alberta
szepesva@cs.ualberta.ca

Abstract

This article provides the first procedure for computing a fully data-dependent in-

terval that traps the mixing time tmix of a finite reversible ergodic Markov chain at a prescribed confidence level. The interval is computed from a single finite-length

sample path from the Markov chain, and does not require the knowledge of any

parameters of the chain. This stands in contrast to previous approaches, which ei-

ther only provide point estimates, or require a reset mechanism, or additional prior

knowledge. The interval is constructed around the relaxation time trelax, which is

strongly roughly

artelaatedntoratthee,

mixing time, and the width of the interval where n is the length of the sample path.

converges to zero Upper and lower

bounds are given on the number of samples required to achieve constant-factor

multiplicative accuracy. The lower bounds indicate that, unless further restric-

tions are placed on the chain, no procedure can achieve this accuracy level before

seeing each state at least (trelax) times on the average. Finally, future directions of research are identified.

1 Introduction

This work tackles the challenge of constructing fully empirical bounds on the mixing time of
Markov chains based on a single sample path. Let (Xt)t=1,2,... be an irreducible, aperiodic timehomogeneous Markov chain on a finite state space [d] := {1, 2, . . . , d} with transition matrix P . Under this assumption, the chain converges to its unique stationary distribution  = (i)di=1 regardless of the initial state distribution q:

lim
t

Prq

(Xt

=

i)

=

lim (qP
t

t)i

=

i

for each i  [d].

The mixing time tmix of the Markov chain is the number of time steps required for the chain to be within a fixed threshold of its stationary distribution:

tmix := min t  N : sup max |Prq (Xt  A) - (A)|  1/4 .
q A[d]

(1)

Here, (A) = iA i is the probability assigned to set A by , and the supremum is over all possible initial distributions q. The problem studied in this work is the construction of a non-trivial
confidence interval Cn = Cn(X1, X2, . . . , Xn, )  [0, ], based only on the observed sample path (X1, X2, . . . , Xn) and   (0, 1), that succeeds with probability 1 -  in trapping the value of the mixing time tmix.

This problem is motivated by the numerous scientific applications and machine learning tasks in which the quantity of interest is the mean (f ) = i if (i) for some function f of the states of a Markov chain. This is the setting of the celebrated Markov Chain Monte Carlo (MCMC) paradigm [1], but the problem also arises in performance prediction involving time-correlated data, as is common in reinforcement learning [2]. Observable bounds on mixing times are useful in the

1

design and diagnostics of these methods; they yield effective approaches to assessing the estimation quality, even when a priori knowledge of the mixing time or correlation structure is unavailable.

Main results. We develop the first procedure for constructing non-trivial and fully empirical con-
fidence intervals for Markov mixing time. Consider a reversible ergodic Markov chain on d states with absolute spectral gap  and stationary distribution minorized by . As is well-known [3, Theorems 12.3 and 12.4],

(trelax - 1) ln 2  tmix  trelax ln 4


(2)

where trelax := 1/ is the relaxation time. Hence, it suffices to estimate  and . Our main results are summarized as follows.

1. In Section 3.1, we show that in some problems n = ((d log d)/ + 1/) observations are necessary for any procedure to guarantee constant multiplicative accuracy in estimating  (Theorems 1 and 2). Essentially, in some problems every state may need to be visited about log(d)/ times, on average, before an accurate estimate of the mixing time can be provided, regardless of the actual estimation procedure used.
2. In Section 3.2, we give a point-estimator for , and prove in Theorem 3 that it achieves multiplicative accuracy from a single sample path of length O(1/(3)).1 We also provide a point-estimator for  that requires a sample path of length O(1/()). This establishes the feasibility of estimating the mixing time in this setting. However, the valid confidence intervals suggested by Theorem 3 depend on the unknown quantities  and . We also discuss the importance of reversibility, and some possible extensions to nonreversible chains.
3. In Section 4, the construction of valid fully empirical confidence intervals for  and  are considered. First, the difficulty of the task is explained, i.e., why the standard approach of turning the finite time confidence intervals of Theorem 3 into a fully empirical one fails. Combining several results from perturbation theory in a novel fashion we propose a new procedure and prove that it avoids slow convergence (Theorem 4). We also explain how to combine the empirical confidence intervals from Algorithm 1 with the non-empirical bounds from Theorem 3 to produce valid empirical confidence intervals. We prove in Theorem 5 that the width of these new intervals converge to zero asymptotically at least as fast as those from either Theorem 3 and Theorem 4.

Related work. There is a vast statistical literature on estimation in Markov chains. For instance, it

itsheknsaomwnpltehamteuanndernt(hfe)as:=sumn1ptiontn=s1ofn(X(Xt)t)ctofnrovmergaebsoavlem, tohset

law of surely

large numbers guarantees that to(f ) [4], while the central

limit theorem tells us that as n  , the distribution of the deviation n(n(f ) - (f )) will be

normal with mean zero and asymptotic variance limn n Var (n(f )) [5].

Although these asymptotic results help us understand the limiting behavior of the sample mean
over a Markov chain, they say little about the finite-time non-asymptotic behavior, which is often
needed for the prudent evaluation of a method or even its algorithmic design [6-13]. To address this need, numerous works have developed Chernoff-type bounds on Pr(|n(f ) - (f )| > ), thus providing valuable tools for non-asymptotic probabilistic analysis [6, 14-16]. These probability
bounds are larger than corresponding bounds for independent and identically distributed (iid) data due to the temporal dependence; intuitively, for the Markov chain to yield a fresh draw Xt that behaves as if it was independent of Xt, one must wait (tmix) time steps. Note that the bounds generally depend on distribution-specific properties of the Markov chain (e.g., P , tmix, ), which are often unknown a priori in practice. Consequently, much effort has been put towards estimating
these unknown quantities, especially in the context of MCMC diagnostics, in order to provide data-
dependent assessments of estimation accuracy [e.g., 11, 12, 17-19]. However, these approaches
generally only provide asymptotic guarantees, and hence fall short of our goal of empirical bounds
that are valid with any finite-length sample path.

Learning with dependent data is another main motivation to our work. Many results from statistical learning and empirical process theory have been extended to sufficiently fast mixing, dependent

1The O(*) notation suppresses logarithmic factors.

2

data [e.g., 20-26], providing learnability assurances (e.g., generalization error bounds). These results are often given in terms of mixing coefficients, which can be consistently estimated in some cases [27]. However, the convergence rates of the estimates from [27], which are needed to derive confidence bounds, are given in terms of unknown mixing coefficients. When the data comes from a Markov chain, these mixing coefficients can often be bounded in terms of mixing times, and hence our main results provide a way to make them fully empirical, at least in the limited setting we study.
It is possible to eliminate many of the difficulties presented above when allowed more flexible access to the Markov chain. For example, given a sampling oracle that generates independent transitions from any given state (akin to a "reset" device), the mixing time becomes an efficiently testable property in the sense studied in [28, 29]. On the other hand, when one only has a circuit-based description of the transition probabilities of a Markov chain over an exponentially-large state space, there are complexity-theoretic barriers for many MCMC diagnostic problems [30].

2 Preliminaries

2.1 Notations

We denote the set of positive integers by N, and the set of the first d positive integers {1, 2, . . . , d}

by [d]. The non-negative part of a real number x is [x]+ := max{0, x}, and x+ := max{0, x}. We use ln(*) for natural logarithm, and log(*) for logarithm with an arbitrary constant base. Bold-

face symbols are used for vectors and matrices (e.g., v, M ), and their entries are referenced by

subindexing (e.g., vi, Mi,j). For a vector v, v denotes its Euclidean norm; for a matrix M , M

denotes its spectral norm. We use Diag(v) to denote the diagonal matrix whose (i, i)-th entry is vi.

The probability simplex is denoted by d-1 = {p  [0, 1]d :

d i=1

pi

=

1},

and

we

regard

vectors

in d-1 as row vectors.

2.2 Setting

Let P  (d-1)d  [0, 1]dxd be a d x d row-stochastic matrix for an ergodic (i.e., irreducible and aperiodic) Markov chain. This implies there is a unique stationary distribution   d-1 with i > 0 for all i  [d] [3, Corollary 1.17]. We also assume that P is reversible (with respect to ):

iPi,j = j Pj,i, i, j  [d].

(3)

The minimum stationary probability is denoted by  := mini[d] i.

Define the matrices M := Diag()P and L := Diag()-1/2M Diag()-1/2 .

The (i, j)th entry of the matrix Mi,j contains the doublet probabilities associated with P : Mi,j = iPi,j is the probability of seeing state i followed by state j when the chain is started from its stationary distribution. The matrix M is symmetric on account of the reversibility of P , and hence
it follows that L is also symmetric. (We will strongly exploit the symmetry in our results.) Further, L = Diag()1/2P Diag()-1/2, hence L and P are similar and thus their eigenvalue systems are
identical. Ergodicity and reversibility imply that the eigenvalues of L are contained in the interval
(-1, 1], and that 1 is an eigenvalue of L with multiplicity 1 [3, Lemmas 12.1 and 12.2]. Denote and
order the eigenvalues of L as

1 = 1 > 2  * * *  d > -1.
Let  := max{2, |d|}, and define the (absolute) spectral gap to be  := 1-, which is strictly positive on account of ergodicity.

Let (Xt)tN be a Markov chain whose transition probabilities are governed by P . For each t  N, let (t)  d-1 denote the marginal distribution of Xt, so
(t+1) = (t)P , t  N.
Note that the initial distribution (1) is arbitrary, and need not be the stationary distribution .

The goal is to estimate  and  from the length n sample path (Xt)t[n], and also to construct fully empirical confidence intervals that  and  with high probability; in particular, the construction

3

of the intervals should not depend on any unobservable quantities, including  and  themselves. As mentioned in the introduction, it is well-known that the mixing time of the Markov chain tmix (defined in Eq. 1) is bounded in terms of  and , as shown in Eq. (2). Moreover, convergence
rates for empirical processes on Markov chain sequences are also often given in terms of mixing
coefficients that can ultimately be bounded in terms of  and  (as we will show in the proof of our first result). Therefore, valid confidence intervals for  and  can be used to make these rates
fully observable.

3 Point estimation

In this section, we present lower and upper bounds on achievable rates for estimating the spectral gap as a function of the length of the sample path n.

3.1 Lower bounds
The purpose of this section is to show lower bounds on the number of observations necessary to achieve a fixed multiplicative (or even just additive) accuracy in estimating the spectral gap . By Eq. (2), the multiplicative accuracy lower bound for  gives the same lower bound for estimating the mixing time. Our first result holds even for two state Markov chains and shows that a sequence length of (1/) is necessary to achieve even a constant additive accuracy in estimating .
Theorem 1. Pick any   (0, 1/4). Consider any estimator  that takes as input a random sample path of length n  1/(4) from a Markov chain starting from any desired initial state distribution. There exists a two-state ergodic and reversible Markov chain distribution with spectral gap   1/2 and minimum stationary probability    such that
Pr [| - |  1/8]  3/8.
Next, considering d state chains, we show that a sequence of length (d log(d)/) is required to estimate  up to a constant multiplicative accuracy. Essentially, the sequence may have to visit all d states at least log(d)/ times each, on average. This holds even if  is within a factor of two of the largest possible value of 1/d that it can take, i.e., when  is nearly uniform.
Theorem 2. There is an absolute constant c > 0 such that the following holds. Pick any positive integer d  3 and any   (0, 1/2). Consider any estimator  that takes as input a random sample path of length n < cd log(d)/ from a d-state reversible Markov chain starting from any desired initial state distribution. There is an ergodic and reversible Markov chain distribution with spectral gap   [, 2] and minimum stationary probability   1/(2d) such that
Pr [| - |  /2]  1/4.
The proofs of Theorems 1 and 2 are given in Appendix A.2

3.2 A plug-in based point estimator and its accuracy

Let us now consider the problem of estimating . For this, we construct a natural plug-in estimator. Along the way, we also provide an estimator for the minimum stationary probability, allowing one
to use the bounds from Eq. (2) to trap the mixing time.

Define the random matrix M  [0, 1]dxd and random vector   d-1 by

Mi,j

:=

|{t



[n

-

1]

:

(Xt, Xt+1) n-1

=

(i, j)}| ,

i

:=

|{t



[n] : Xt n

=

i}| ,

i  [d] .

Furthermore, define

Sym(L)

:=

1 (L

+


L

)

2

i, j  [d] ,

2A full version of this paper, with appendices, is available on arXiv [31].

4

to be the symmetrized version of the (possibly non-symmetric) matrix

L := Diag( )-1/2M Diag( )-1/2.

Let 1  2  * * *  d be the eigenvalues of Sym(L). Our estimator of the minimum stationary probability  is  := mini[d] i, and our estimator of the spectral gap  is  :=
1 - max{2, |d|}.

These estimators have the following accuracy guarantees:

Theorem 3. There exists an absolute constant C > 0 such that the following holds. Assume the

estimators  and  described above are formed from a sample path of length n from an ergodic and

reversible Markov chain. Let  > 0 denote the spectral gap and  > 0 the minimum stationary

probability. For any   (0, 1), with probability at least 1 - ,



| - |  C 



log

d  

n

+

log

d  

n



(4)

and  

| - |  C 

log

d 

*

log

n  

n

+

log

1 

n



.

(5)

Theorem 3 implies that the sequence lengths required to estimate  and  to within constant

multiplicative factors are, respectively, O

1  

and O

1  3

. By Eq. (2), the second of these is

also a bound on the required sequence length to estimate tmix.

The proof of Theorem 3 is based on analyzing the convergence of the sample averages M and  to their expectation, and then using perturbation bounds for eigenvalues to derive a bound on the error of . However, since these averages are formed using a single sample path from a (possibly) non-stationary Markov chain, we cannot use standard large deviation bounds; moreover applying
Chernoff-type bounds for Markov chains to each entry of M would result in a significantly worse sequence length requirement, roughly a factor of d larger. Instead, we adapt probability tail bounds for sums of independent random matrices [32] to our non-iid setting by directly applying a blocking technique of [33] as described in the article of [20]. Due to ergodicity, the convergence rate can be bounded without any dependence on the initial state distribution (1). The proof of Theorem 3 is given in Appendix B.

Note that because the eigenvalues of L are the same as that of the transition probability matrix P , we could have instead opted to estimate P , say, using simple frequency estimates obtained from
the sample path, and then computing the second largest eigenvalue of this empirical estimate P . In fact, this approach is a way to extend to non-reversible chains, as we would no longer rely on the symmetry of M or L. The difficulty with this approach is that P lacks the structure required by certain strong eigenvalue perturbation results. One could instead invoke the Ostrowski-Elsner theorem [cf. Theorem 1.4 on Page 170 of 34], which bounds the matching distance between the eigenvalues of a matrix A and its perturbation A + E by O( E 1/d). Since P - P is expected to be of size O(n-1/2), this approach will give a confidence interval for  whose width shrinks at a rate of O(n-1/(2d))--an exponential slow-down compared to the rate from Theorem 3. As demonstrated through an example from [34], the dependence on the d-th root of the norm of the perturbation cannot be avoided in general. Our approach based on estimating a symmetric matrix affords us the use of perturbation results that exploit more structure.

Returning to the question of obtaining a fully empirical confidence interval for  and , we notice that, unfortunately, Theorem 3 falls short of being directly suitable for this, at least without further
assumptions. This is because the deviation terms themselves depend inversely both on  and , and hence can never rule out 0 (or an arbitrarily small positive value) as a possibility for  or .3 In effect, the fact that the Markov chain could be slow mixing and the long-term frequency of some

3Using Theorem 3, it is possible to trap  in the union of two empirical confidence intervals--one around  and the other around zero, both of which shrink in width as the sequence length increases.

5

Algorithm 1 Empirical confidence intervals
Input: Sample path (X1, X2, . . . , Xn), confidence parameter   (0, 1). 1: Compute state visit counts and smoothed transition probability estimates:

Ni := |{t  [n - 1] : Xt = i}| , i  [d]; Ni,j := |{t  [n - 1] : (Xt, Xt+1) = (i, j)}| ,

Pi,j

:=

Ni,j + 1/d Ni + 1

,

(i, j)  [d]2.

2: Let A# be the group inverse of A := I - P .
3: Let   d-1 be the unique stationary distribution for P . 4: Compute eigenvalues 12 * * * d of Sym(L), where L := Diag( )1/2P Diag( )-1/2. 5: Spectral gap estimate:
 := 1 - max{2, |d|}.

6: Empirical bounds for |Pi,j-Pi,j| for (i, j)  [d]2: c := 1.01, n, := inf{t  0 : 2d2(1 +

logc

2n t

+

)e-t



},

 2

and

Bi,j := 

cn, 2Ni

+

cn, 2Ni

+

2cPi,j (1 - Pi,j )n, Ni

+

(5/3)n,

+ |Pi,j Ni

- 1/d| 

.

7: Relative sensitivity of :



:=

1 2

max

A#j,j - min

A#i,j : i  [d]

: j  [d]

.

8: Empirical bounds for maxi[d] |i - i| and max i[d]{| i/i - 1|, | i/i - 1|}:

b :=  max Bi,j : (i, j)  [d]2 ,

 :=

1 2

max

i[d]

b i

,

[i

b - b]+

.

9: Empirical bounds for | - |:

1/2

w := 2 + 2 + (1 + 2 + 2)

(i,j)[d]2

i j

Bi2,j

.

states could be small makes it difficult to be confident in the estimates provided by  and . This suggests that in order to obtain fully empirical confidence intervals, we need an estimator that is not subject to such effects--we pursue this in Section 4. Theorem 3 thus primarily serves as a point of comparison for what is achievable in terms of estimation accuracy when one does not need to provide empirical confidence bounds.
4 Fully empirical confidence intervals
In this section, we address the shortcoming of Theorem 3 and give fully empirical confidence intervals for the stationary probabilities and the spectral gap . The main idea is to use the Markov property to eliminate the dependence of the confidence intervals on the unknown quantities (including  and ). Specifically, we estimate the transition probabilities from the sample path using simple frequency estimates: as a consequence of the Markov property, for each state, the frequency estimates converge at a rate that depends only on the number of visits to the state, and in particular the rate (given the visit count of the state) is independent of the mixing time of the chain.
6

As discussed in Section 3, it is possible to form a confidence interval for  based on the eigenvalues of an estimated transition probability matrix by appealing to the Ostrowski-Elsner theorem. However, as explained earlier, this would lead to a slow O(n-1/(2d)) rate. We avoid this slow rate by using an estimate of the symmetric matrix L, so that we can use a stronger perturbation result
(namely Weyl's inequality, as in the proof of Theorem 3) available for symmetric matrices.

To form an estimate of L based on an estimate of the transition probabilities, one possibility is to estimate  using a frequency-based estimate for  as was done in Section 3, and appeal to the relation L = Diag()1/2P Diag()-1/2 to form a plug-in estimate. However, as noted in Section 3.2, confidence intervals for the entries of  formed this way may depend on the mixing time. Indeed, such an estimate of  does not exploit the Markov property.

We adopt a different strategy for estimating , which leads to our construction of empirical confi-
dence intervals, detailed in Algorithm 1. We form the matrix P using smoothed frequency estimates of P (Step 1), then compute the so-called group inverse A# of A = I - P (Step 2), followed by
finding the unique stationary distribution  of P (Step 3), this way decoupling the bound on the accuracy of  from the mixing time. The group inverse A# of A is uniquely defined; and if P defines an ergodic chain (which is the case here due to the use of the smoothed estimates), A# can be computed at the cost of inverting an (d-1)x(d-1) matrix [35, Theorem 5.2].4 Further, once given A#, the unique stationary distribution  of P can be read out from the last row of A# [35,
Theorem 5.3]. The group inverse is also be used to compute the sensitivity of . Based on  and P ,
we construct the plug-in estimate L of L, and use the eigenvalues of its symmetrization to form the estimate  of the spectral gap (Steps 4 and 5). In the remaining steps, we use perturbation analyses to relate  and , viewing P as the perturbation of P ; and also to relate  and , viewing L as a perturbation of Sym(L). Both analyses give error bounds entirely in terms of observable quantities (e.g., ), tracing back to empirical error bounds for the smoothed frequency estimates of P .

The most computationally expensive step in Algorithm 1 is the computation of the group inverse A#, which, as noted reduces to matrix inversion. Thus, with a standard implementation of matrix inversion, the algorithm's time complexity is O(n + d3), while its space complexity is O(d2).

To state our main theorem concerning Algorithm 1, we first define  to be analogous to  from Step 7, with A# replaced by the group inverse A# of A := I - P . The result is as follows.
Theorem 4. Suppose Algorithm 1 is given as input a sample path of length n from an ergodic and reversible Markov chain and confidence parameter   (0, 1). Let  > 0 denote the spectral gap,  the unique stationary distribution, and  > 0 the minimum stationary probability. Then, on an event of probability at least 1 - ,
i  [i - b, i + b] for all i  [d], and   [ - w,  + w].
Moreover, b and w almost surely satisfy (as n  )

b = O

max 
(i,j)[d]2

Pi,j log log n in

,

w = O

 

log log n

n

+

d log log n n

.5

The proof of Theorem 4 is given in Appendix C. As mentioned above, the obstacle encountered in Theorem 3 is avoided by exploiting the Markov property. We establish fully observable upper and lower bounds on the entries of P that converge at a n/ log log n rate using standard martingale tail inequalities; this justifies the validity of the bounds from Step 6. Properties of the group inverse [35, 36] and eigenvalue perturbation theory [34] are used to validate the empirical bounds on i and  developed in the remaining steps of the algorithm.
The first part of Theorem 4 provides valid empirical confidence intervals for each i and for , which are simultaneously valid at confidence level . The second part of Theorem 4 shows that the
4 The group inverse of a square matrix A, a special case of the Drazin inverse, is the unique matrix A# satisfying AA#A = A, A#AA# = A# and A#A = AA#.
5In Theorems 4 and 5, our use of big-O notation is as follows. For a random sequence (Yn)n and a (nonrandom) positive sequence (,n)n parameterized by , we say "Yn = O(,n) holds almost surely as n  " if there is some universal constant C > 0 such that for all , lim supn Yn/,n  C holds almost surely.

7

width of the intervals decrease as the sequence length increases. We show in Appendix C.5 that

  d/, and hence b = O

max(i,j)[d]2

d 

Pi,j log log n i n

, w = O

d  

log log n  n

.

It is easy to combine Theorems 3 and 4 to yield intervals whose widths shrink at least as fast
as both the non-empirical intervals from Theorem 3 and the empirical intervals from Theorem 4. Specifically, determine lower bounds on  and  using Algorithm 1,   mini[d][i - b]+ ,   [ - w]+; then plug-in these lower bounds for  and  in the deviation bounds in Eq. (5) from Theorem 3. This yields a new interval centered around the estimate of  from Theorem 3, and it no longer depends on unknown quantities. The interval is a valid 1 - 2 probability confidence interval for , and for sufficiently large n, the width shrinks at the rate given in Eq. (5). We can similarly construct an empirical confidence interval for  using Eq. (4), which is valid on the same 1 - 2 probability event.6 Finally, we can take the intersection of these new intervals with the
corresponding intervals from Algorithm 1. This is summarized in the following theorem, which we
prove in Appendix D.

Theorem 5. The following holds under the same conditions as Theorem 4. For any   (0, 1),

the confidence intervals U and V described above for  and , respectively, satisfy   U and   V with probability at least 1 - 2. Furthermore, the widths of these intervals almost surely

satisfy (as n  ) |U | = O



log

d  

 n

, |V | = O

min

log

d 

*log(n)

  n

,

w

, where w is

the width from Algorithm 1.

5 Discussion
The construction used in Theorem 5 applies more generally: Given a confidence interval of the form In = In(, , ) for some confidence level  and a fully empirical confidence set En() for (, ) for the same level, In = En()  (,)En()In(, , ) is a valid fully empirical 2level confidence interval whose asymptotic width matches that of In up to lower order terms under reasonable assumptions on En and In. In particular, this suggests that future work should focus on closing the gap between the lower and upper bounds on the accuracy of point-estimation. Another interesting direction is to reduce the computation cost: The current cubic cost in the number of states can be too high even when the number of states is only moderately large.
Perhaps more important, however, is to extend our results to large state space Markov chains: In most practical applications the state space is continuous or is exponentially large in some natural parameters. As follows from our lower bounds, without further assumptions, the problem of fully data dependent estimation of the mixing time is intractable for information theoretical reasons. Interesting directions for future work thus must consider Markov chains with specific structure. Parametric classes of Markov chains, including but not limited to Markov chains with factored transition kernels with a few factors, are a promising candidate for such future investigations. The results presented here are a first step in the ambitious research agenda outlined above, and we hope that they will serve as a point of departure for further insights in the area of fully empirical estimation of Markov chain parameters based on a single sample path.
References
[1] J. S. Liu. Monte Carlo Strategies in Scientific Computing. Springer Series in Statistics. Springer-Verlag, 2001.
[2] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction (Adaptive Computation and Machine Learning). A Bradford Book, 1998.
[3] D. Levin, Y. Peres, and E. Wilmer. Markov Chains and Mixing Times. AMS, 2008.
[4] S. P. Meyn and R. L. Tweedie. Markov Chains and Stochastic Stability. Springer, 1993.
[5] C. Kipnis and S. R. S. Varadhan. Central limit theorem for additive functionals of reversible markov processes and applications to simple exclusions. Comm. Math. Phys., 104(1):1-19, 1986.
6For the  interval, we only plug-in lower bounds on  and  only where these quantities appear as 1/ and 1/ in Eq. (4). It is then possible to "solve" for observable bounds on . See Appendix D for details.

8

[6] I. Kontoyiannis, L. A. Lastras-Montano, and S. P. Meyn. Exponential bounds and stopping rules for MCMC and general Markov chains. In VALUETOOLS, page 45, 2006.
[7] M.-F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In ICML, pages 65-72, 2006.
[8] V. Mnih, Cs. Szepesvari, and J.-Y. Audibert. Empirical Bernstein stopping. In ICML, pages 672-679, 2008.
[9] A. Maurer and M. Pontil. Empirical Bernstein bounds and sample-variance penalization. In COLT, 2009.
[10] L. Li, M. L. Littman, T. J. Walsh, and A. L. Strehl. Knows what it knows: a framework for self-aware learning. Machine Learning, 82(3):399-443, 2011.
[11] J. M. Flegal and G. L. Jones. Implementing MCMC: estimating with confidence. In Handbook of Markov chain Monte Carlo, pages 175-197. Chapman & Hall/CRC, 2011.
[12] B. M. Gyori and D. Paulin. Non-asymptotic confidence intervals for MCMC in practice. arXiv:1212.2016, 2014.
[13] A. Swaminathan and T. Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In ICML, 2015.
[14] D. Gillman. A Chernoff bound for random walks on expander graphs. SIAM Journal on Computing, 27(4):1203-1220, 1998.
[15] C. A. Leon and F. Perron. Optimal Hoeffding bounds for discrete reversible Markov chains. Annals of Applied Probability, pages 958-970, 2004.
[16] D. Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral methods. Electronic Journal of Probability, 20:1-32, 2015.
[17] S. T. Garren and R. L. Smith. Estimating the second largest eigenvalue of a Markov transition matrix. Bernoulli, 6:215-242, 2000.
[18] G. L. Jones and J. P. Hobert. Honest exploration of intractable probability distributions via markov chain monte carlo. Statist. Sci., 16(4):312-334, 11 2001.
[19] Y. Atchade. Markov Chain Monte Carlo confidence intervals. Bernoulli, 2015. (to appear).
[20] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. The Annals of Probability, 22(1):94-116, January 1994.
[21] R. L. Karandikar and M. Vidyasagar. Rates of uniform convergence of empirical means with mixing processes. Statistics and Probability Letters, 58(3):297-307, 2002.
[22] D. Gamarnik. Extension of the PAC framework to finite and countable Markov chains. IEEE Transactions on Information Theory, 49(1):338-345, 2003.
[23] M. Mohri and A. Rostamizadeh. Stability bounds for non-iid processes. In NIPS, 2008.
[24] M. Mohri and A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In NIPS, 2009.
[25] I. Steinwart and A. Christmann. Fast learning from non-i.i.d. observations. In NIPS, 2009.
[26] I. Steinwart, D. Hush, and C. Scovel. Learning from dependent observations. Journal of Multivariate Analysis, 100(1):175-194, 2009.
[27] D. McDonald, C. Shalizi, and M. Schervish. Estimating beta-mixing coefficients. In AISTATS, pages 516-524, 2011.
[28] T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing that distributions are close. In FOCS, pages 259-269. IEEE, 2000.
[29] T. Batu, L. Fortnow, R. Rubinfeld, W. D. Smith, and P. White. Testing closeness of discrete distributions. Journal of the ACM (JACM), 60(1):4:2-4:25, 2013.
[30] N. Bhatnagar, A. Bogdanov, and E. Mossel. The computational complexity of estimating MCMC convergence time. In RANDOM, pages 424-435. Springer, 2011.
[31] D. Hsu, A. Kontorovich, and C. Szepesvari. Mixing time estimation in reversible Markov chains from a single sample path. CoRR, abs/1506.02903, 2015.
[32] J. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine Learning, 2015.
[33] S. Bernstein. Sur l'extension du theoreme limite du calcul des probabilites aux sommes de quantites dependantes. Mathematische Annalen, 97:1-59, 1927.
[34] G. W. Stewart and J. Sun. Matrix perturbation theory. Academic Press, Boston, 1990.
[35] C. D. Meyer Jr. The role of the group generalized inverse in the theory of finite Markov chains. SIAM Review, 17(3):443-464, 1975.
[36] G. Cho and C. Meyer. Comparison of perturbation bounds for the stationary distribution of a Markov chain. Linear Algebra and its Applications, 335:137-150, 2001.
9

