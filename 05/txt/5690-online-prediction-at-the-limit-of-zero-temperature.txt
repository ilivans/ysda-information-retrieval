Online Prediction at the Limit of Zero Temperature

Mark Herbster Stephen Pasteris Department of Computer Science University College London London WC1E 6BT, England, UK
{m.herbster,s.pasteris}@cs.ucl.ac.uk

Shaona Ghosh ECS
University of Southampton Southampton, UK SO17 1BJ ghosh.shaona@gmail.com

Abstract
We design an online algorithm to classify the vertices of a graph. Underpinning the algorithm is the probability distribution of an Ising model isomorphic to the graph. Each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far. Computing these classifications is unfortunately based on a #P complete problem. This motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework. Our algorithm is optimal when the graph is a tree matching the prior results in [1]. For a general graph, the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound. The algorithm is efficient, as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph.
1 Introduction
Semi-supervised learning is now a standard methodology in machine learning. A common approach in semi-supervised learning is to build a graph [2] from a given set of labeled and unlabeled data with each datum represented as a vertex. The hope is that the constructed graph will capture either the cluster [3] or manifold [4] structure of the data. Typically, an edge in this graph indicates the expectation that the joined data points are more likely to have the same label. One method to exploit this representation is to use the semi-norm induced by the Laplacian of the graph [5, 4, 6, 7]. A shared idea of the Laplacian semi-norm based approaches is that the smoothness of a boolean labeling of the graph is measured via the "cut", which is just the number of edges that connect disagreeing labels. In practice the semi-norm is then used as a regularizer in which the optimization problem is relaxed from boolean to real values. Our approach also uses the "cut", but unrelaxed, to define an Ising distribution over the vertices of the graph.
Predicting with the vertex marginals of an Ising distribution in the limit of zero temperature was shown to be optimal in the mistake bound model [1, Section 4.1] when the graph is a tree. The exact computation of marginal probabilities in the Ising model is intractable on non-trees [8]. However, in the limit of zero temperature, a rich combinatorial structure called the Picard-Queyranne graph [9] emerges. We exploit this structure to give an algorithm which 1) is optimal on trees, 2) has a quadratic computational complexity, and 3) has a mistake bound on generic graphs that is stronger than previous bounds in many natural cases.
The paper is organized as follows. In the remainder of this section, we introduce the Ising model and lightly review previous work in the online mistake bound model for predicting the labeling of a graph. In Section 2 we review our key technical tool the Picard-Queyranne graph [9] and explain the required notation. In the body of Section 3 we provide a mistake bound analysis of our algorithm as well as the intractable 0-Ising algorithm and then conclude with a detailed comparison to the state of the art. In the appendices we provide proofs as well as preliminary experimental results.
Ising model in the limit zero temperature. In our setting, the parameters of the Ising model are an n-vertex graph G = (V (G), E(G)) and a temperature parameter  > 0, where V (G) =
1

{1, . . . , n} denotes the vertex set and E(G) denotes the edge set. Each vertex of this graph may

be labeled with one of two states {0, 1} and thus a labeling of a graph may be denoted by a vector

u  {0, 1}n where ui denotes the label of vertex i. The cutsize of a labeling u is defined as

G(u) := (i,j)E(G) |ui - uj|. The Ising probability distribution over labelings of G is then

defined as pG (u)



exp

-

1 

G

(u)

where  > 0 is the temperature parameter. In our online

setting at the beginning of trial t + 1 we will have already received an example sequence, St, of t

vertex-label pairs (i1, y1), . . . , pG (uv = y|ui1 = y1, . . . , uit

(it, =

yt) yt)

where pair (i, y)  V (G) x {0, 1}. We use pG (uv to denote the marginal probability that vertex v

= y|St) := has label y

given the previously labeled vertices of St. For convenience we also define the marginalized cutsize

G(u|St) to be equal to G(u) if ui1 = y1, . . . , uit = yt and equal to undefined otherwise. Our prediction yt+1 of vertex it+1 is then the label with maximal marginal probability in the limit of

zero temperature, thus

yt0+I 1(it+1|St)

:=

argmax
y{0,1}

lim
 0

pG

(uit+1

=

y|ui1

=

y1, . . . , uit

=

yt) .

[0-Ising] (1)

Note the prediction is undefined if the labels are equally probable. In low temperatures the mass
of the marginal is dominated by the labelings consistent with St and the proposed label of vertex it+1 of minimal cut; as we approach zero, yt+1 is the label consistent with the maximum number of labelings of minimal cut. Thus if k := min G(u|S) then we have that
u{0,1}n

(R) y0I(v|S) = 0
1

|u |u

 

{0, 1}n {0, 1}n

: :

G(u|(S, (v, 0))) = G(u|(S, (v, 0))) =

k| k|

> <

|u |u

 

{0, 1}n {0, 1}n

: :

G(u|(S, (v, 1))) G(u|(S, (v, 1)))

= k| = k|

.

The problem of counting minimum label-consistent cuts was shown to be #P-complete in [10] and further computing y0I(v|S) is also NP-hard (see Appendix G). In Section 2.1 we introduce the Picard-Queyranne graph [9] which captures the combinatorial structure of the set of minimum-cuts. We then use this simplifying structure as a basis to design a heuristic approximation to y0I(v|S) with a mistake bound guarantee.
Predicting the labelling of a graph in the mistake bound model. We prove performance guarantees for our method in the mistake bound model introduced by Littlestone [11]. On the graph this model corresponds to the following game. Nature presents a graph G; Nature queries a vertex i1  V (G) = INn; the learner predicts the label of the vertex y1  {0, 1}; nature presents a label y1; nature queries a vertex i2; the learner predicts y2; and so forth. The learner's goal is to minimize the total number of mistakes M = |{t : yt = yt}|. If nature is adversarial, the learner will always make a "mistake", but if nature is regular or simple, there is hope that a learner may incur only a few mistakes. Thus, a central goal of online learning is to design algorithms whose total mistakes can be bounded relative to the complexity of nature's labeling. The graph labeling problem has been studied extensively in the online literature. Here we provide a rough discussion of the two main approaches for graph label prediction, and in Section 3.3 we provide a more detailed comparison. The first approach is based on the graph Laplacian [12, 13, 14]; it provides bounds that utilize the additional connectivity of non-tree graphs, which are particularly strong when the graph contains uniformly-labeled clusters of small (resistance) diameter. The drawbacks of this approach are that the bounds are weaker on graphs with large diameter and that the computation times are slower. The second approach is to estimate the original graph with an appropriately selected tree or "path" graph [15, 16, 1, 17]; this leads to faster computation times, and bounds that are better on graphs with large diameters. The algorithm treeOpt [1] is optimal on trees. These algorithms may be extended to non-tree graphs by first selecting a spanning tree uniformly at random [16] and then applying the algorithm to the sampled tree. This randomized approach enables expected mistake bounds which exploit the cluster structure in the graph.
The bounds we prove for the NP-hard 0-Ising prediction and our heuristic are most similar to the "small p" bounds proven for the p-seminorm interpolation algorithm [14]. Although these bounds are not strictly comparable, a key strength of our approach is that the new bounds often improve when the graph contains uniformly-labeled clusters of varying diameters. Furthermore, when the graph is a tree we match the optimal bounds of [1]. Finally, the cumulative time required to compute the complete labeling of a graph is quadratic in the size of the graph for our algorithm, while [14] requires the minimization of a non-strongly convex function (on every trial) which is not differentiable when p  1.

2

2 Preliminaries
An (undirected) graph G is a pair of sets (V, E) such that E is a set of unordered pairs of distinct elements from V . We say that R is a subgraph R  G iff V (R)  V (G) and E(R) = {(i, j) : i, j  V (R), (i, j)  E(G)}. Given any subgraph R  G, we define its boundary (or inner border) 0(R), its neighbourhood (or exterior border) e(R) respectively as 0(R) := {j : i  V (R), j  V (R), (i, j)  E(G)}, and e(R) := {i : i  V (R), j  V (R), (i, j)  E(G)}, and its exterior edge border eE(R) := {(i, j) : i  V (R), j  V (R), (i, j)  E(G)}. The length of a subgraph P is denoted by |P| := |E(P)| and we denote the diameter of a graph by D(G). A pair of vertices v, w  V (G) are -connected if there exist  edge-disjoint paths connecting them. The connectivity of a graph, (G), is the maximal value of  such that every pair of points in G is -connected. The atomic number N(G) of a graph at connectivity level  is the minimum cardinality c of a partition of G into subgraphs {R1, . . . , Rc} such that (Ri)   for all 1  i  c.
Our results also require the use of directed-, multi-, and quotient- graphs. Every undirected graph also defines a directed graph where each undirected edge (i, j) is represented by directed edges (i, j) and (j, i). An orientation of an undirected graph is an assignment of a direction to each edge, turning the initial graph into a directed graph. In a multi-graph the edge set is now a multi-set and thus there may be multiple edges between two vertices. A quotient-graph G is defined from a graph G and a partition of its vertex set {Vi}Ni=1 so that V (G) := {Vi}Ni=1 (we often call these vertices supervertices to emphasize that they are sets) and the multiset E(G) := {(I, J) : I, J  V (G), I = J, i  I, j  J, (i, j)  E(G)}. We commonly construct a quotient-graph G by "merging" a collection of super-vertices, for example, in Figure 2 from 2a to 2b where 6 and 9 are merged to "6/9" and also the five merges that transforms 2c to 2d.
The set of all label-consistent minimum-cuts in a graph with respect to an example sequence S is UG(S) := argminu{0,1}n G(u|S). The minimum is typically non-unique. For example in Figure 2a, the vertex sets {v1, . . . , v4}, {v5, . . . , v12} correspond to one label-consistent minimum-cut and {v1, . . . , v5, v7, v8}, {v6, v9 . . . , v12} to another (the cutsize is 3). The (uncapacitated) maximum flow is the number of edge-disjoint paths between a source and target vertex. Thus in Figure 2b between vertex "1" and vertex "6/9" there are at most 3 simultaneously edge-disjoint paths; these are also not unique, as one path must pass through either vertices v11, v12 or vertices v11, v10, v12 . Figure 2c illustrates one such flow F (just the directed edges). For convenience it is natural to view the maximum flow or the label-consistent minimum-cut as being with respect to only two vertices as in Figure 2a transformed to Figure 2b so that H  merge(G, {v6, v9}). The "flow" and the "cut" are related by Menger's theorem which states that the minimum-cut with respect to a source and target vertex is equal to the max flow between them. Given a connected graph H and source and target vertices s, t the Ford-Fulkerson algorithm [18] can find k edge-disjoint paths from s to t in time O(k|E(H)|) where k is the value of the max flow.
2.1 The Picard-Queyranne graph
Given a set of labels there may be multiple label-consistent minimum-cuts as well as multiple maximum flows in a graph. The Picard-Queyranne (PQ) graph [9] reduces this multiplicity as far as is possible with respect to the indeterminacy of the maximum flow. The vertices of the PQ-graph are defined as a super-vertex set on a partition of the original graph's vertex set. Two vertices are contained in the same super-vertex iff they have the same label in every label-consistent minimum-cut. An edge between two vertices defines an analogous edge between two super-vertices iff that edge is conserved in every maximum flow. Furthermore the edges between super-vertices strictly orient the labels in any label-consistent minimum-cut as may be seen in the formal definition that follows.
First we introduce the following useful notations: let kG,S := min{G(u|S) : u  {0, 1}n} denote the minimum-cutsize of G with respect to S; let iS j denote an equivalence relation between vertices in V (G) where iS j iff u  UG(S) : ui = uj; and then we define,
Definition 1 ([9]). The Picard-Queyranne graph G(G, S) is derived from graph G and non-trivial example sequence S. The graph is an orientation of the quotient graph derived from the partition {, I2, . . . , IN-1, } of V (G) induced by S . The edge set of G is constructed of kG,S edge-disjoint paths starting at source vertex  and terminating at target vertex . A labeling u  {0, 1}n is in UG(S) iff
1. i   implies ui = 0 and i  implies ui = 1
3

2. i, j  H implies ui = uj 3. i  I, j  J, (I, J)  E(G), and ui = 1 implies uj = 1
where  and are the source and target vertices and H, I, J  V (G).
As G(G, S) is a DAG it naturally defines a partial order (V (G), G) on the vertex set where I G J if there exists a path starting at I and ending at J. The least and greatest elements of the partial order are  and . The notation R and R denote the up set and down set of R. Given the set U of all label-consistent minimum-cuts then if u  U there exists an antichain A  V (G) \ { } such that ui = 0 when i  I  A otherwise ui = 1; furthermore for every antichain there exists a labelconsistent minimum-cut. The simple structure of G(G, S) was utilized by [9] to enable the efficient algorithmic enumeration of minimum-cuts. However, the cardinality of this set of all label-consistent minimum-cuts is potentially exponential in the size of the PQ-graph and the exact computation of the cardinality was later shown #P-complete in [10]. In Figure 1 we give the algorithm from [9, 19]
PicardQueyranneGraph(graph: G; example sequence: S = (vk, yk)tk=1) 1. (H, s, t)  SourceTargetMerge(G, S) 2. F  MaxFlow(H, s, t) 3. I  (V (I), E(I)) where V (I) := V (H) and E(I) := {(i, j) : (i, j)  E(H), (j, i)  F} 4. G0  QuotientGraph(StronglyConnectedComponents(I), H) 5. E(G)  E(G0); V (G)  V (G0) except (G)  (G0)  {vk : k  INt, yk = 0}
and (G)  (G0)  {vk : k  INt, yk = 1} Return: directed graph: G
Figure 1: Computing the Picard-Queyranne graph

2 79

2 7 6/9

5 13

5

10 12

1

3

10 12

6

4 8 11
(a) Graph G and S = (v1, 0), (v6, 1), (v9, 1)

4 8 11
(b) Graph H (step 1 in Figure 1)



2 7 6/9

5 13

10 12



A

4 8 11

BC

(c) Graph I (step 3 in Figure 1)

(d) PQ Graph G (step 4 in Figure 1)

Figure 2: Building a Picard-Queyranne graph

to compute a PQ-graph. We illustrate the computation in Figure 2. The algorithm operates first on
(G, S) (step 1) by "merging" all vertices which share the same label in S to create H. In step 2 a
max flow graph F  H is computed by the Ford-fulkerson algorithm. It is well-known in the case
of unweighted graphs that a max flow graph F may be output as a DAG of k edge-disjoint paths
where k is the value of the flow. In step 3 all edges in the flow become directed edges creating I. The graph G0 is then created in step 4 from I where the strongly connected components become the super-vertices of G0 and the super-edges correspond to a subset of flow edges from F. Finally, in

4

step 5, we create the PQ-graph G by "fixing" the source and target vertices so that they also have as elements the original labeled vertices from S which were merged in step 1. The correctness of the algorithm follows from arguments in [9]; we provide an independent proof in Appendix B.
Theorem 2 ([9]). The algorithm in Figure 1 computes the unique Picard-Queyranne graph G(G, S) derived from graph G and non-trivial example sequence S.

3 Mistake Bounds Analysis

In this section we analyze the mistakes incurred by the intractable 0-Ising strategy (see (1)) and the strategy longest-path (see Figure 3). Our analysis splits into two parts. Firstly, we show (Section 3.1, Theorem 4) for a sufficiently regular graph label prediction algorithm, that we may analyze independently the mistake bound of each uniformly-labeled cluster (connected subgraph). Secondly, the per-cluster analysis then separates into three cases, the result of which is summarized in Theorem 10. For a given cluster C when its internal connectivity is larger than the number of edges in the boundary ((C) > |eE(C)|) we will incur no more than one mistake in that cluster. On the other hand for smaller connectivity clusters ((C)  |eE(C)|) we incur up to quadratically in mistakes via the edge boundary size. When C is a tree we incur O(|eE(C)| log D(C)) mistakes.
The analysis of smaller connectivity clusters separates into two parts. First, a sequence of trials in which the label-consistent minimum-cut does not increase, we call a PQ-game (Section 3.2) as in essence it is played on a PQ-graph. We give a mistake bound for a PQ-game for the intractable 0-Ising prediction and a comparable bound for the strategy longest-path in Theorem 8. Second, when the label-consistent minimum-cut increases the current PQ-game ends and a new one begins, leading to a sequence of PQ-games. The mistakes incurred over a sequence of PQ-games is addressed in the aforementioned Theorem 10 and finally Section 3.3 concludes with a discussion of the combined bounds of Theorems 4 and 10 with respect to other graph label prediction algorithms.

3.1 Per-cluster mistake bounds for regular graph label prediction algorithms

An algorithm is called regular if it is permutation-invariant, label-monotone, and Markov. An algorithm is permutation-invariant if the prediction at any time t does not depend on the order of the examples up to time t; label-monotone if for every example sequence if we insert an example "between" examples t and t+1 with label y then the prediction at time t+1 is unchanged or changed to y; and Markov with respect to a graph G if for any disjoint vertex sets P and Q and separating set R then the predictions in P are independent of the labels in Q given the labels of R. A subgraph is uniformly-labeled with respect to an example sequence iff the label of each vertex is the same and these labels are consistent with the example sequence. The following definition characterizes the "worst-case" example sequences for regular algorithms with respect to uniformly-labeled clusters.
Definition 3. Given an online algorithm A and a uniformly-labeled subgraph C  G, then BA(C; G) denotes the maximal mistakes made only in C for the presentation of any permutation of examples in e(C), each with label y, followed by any permutation of examples in C, each with label 1-y.

The following theorem enables us to analyze the mistakes incurred in each uniformly-labeled sub-

graph C independently of each other and independently of the remaining graph structure excepting

the subgraph's exterior border e(C).

Theorem 4 (Proof in Appendix D). Given an online permutation-invariant label-monotone Markov

algorithm A and a graph G which is covered by uniformly-labeled subgraphs C1, . . . , Cc the mistakes

incurred by the algorithm may be bounded by M 

c i=1

BA(Ci;

G)

.

The above theorem paired with Theorem 10 completes the mistake bound analysis of our algorithms.

3.2 PQ-games
Given a PQ-graph G = G(G, S), the derived online PQ-game is played between a player and an adversary. The aim of the player is to minimize their mistaken predictions; for the adversary it is to maximize the player's mistaken predictions. Thus to play the adversary proposes a vertex z  Z  V (G), the player then predicts a label y  {0, 1}, then the adversary returns a label y  {0, 1} and either a mistake is incurred or not. The only restriction on the adversary is to not return a label which increases the label-consistent minimum-cut. As long as the adversary does not give an example (z  , 1) or (z  , 0), the label-consistent minimum-cut does not increase

5

no matter the value of y; which also implies the player has a trivial strategy to predict the label of z    . After the example is given, we have an updated PQ-graph with new source and target super-vertices as seen in the proposition below. Proposition 5. If G(G, S) is a PQ-graph and (z, y = 0) ((z, y = 1)) is an example with z  Z  V (G) and z  (z  ) then let Z = {Z} (Z = {Z}) then G(G, S, (z, y) ) = merge(G(G, S), Z).
Thus given the PQ-graph G the PQ-game is independent of G and S, since a "play" z  V (G) induces a "play" Z  V (G) (with z  Z).

Mistake bounds for PQ-games. Given a single PQ-game, in the following we will discuss the
three strategies fixed-paths, 0-Ising, and longest-path that the player may adopt
for which we prove online mistake bounds. The first strategy fixed-paths is merely motiva-
tional: it can be used to play a single PQ-game, but not a sequence. The second strategy 0-Ising
is computationally infeasible. Finally, the longest-path strategy is "dynamically" similar to
fixed-paths but is also permutation-invariant. Common to all our analyses is a k-path cover
P of PQ-graph G which is a partitioning of the edge-set of G into k edge-disjoint directed paths P := {p1, . . . , pk} from  to . Note that the cover is not necessarily unique; for example, in Figure 2d, we have the two unique path covers P1 := {(, A, ), (, A, B, ), (, B, C, )} and P2 := {(, A, ), (, A, B, C, ), (, B, )}. We denote the set of all path covers as P and thus we have for Figure 2d that P := {P1, P2}. This cover motivates a simple mistake bound and strategy. Suppose we had a single path of length |p| where the first and last vertex are the "source"
and "target" vertices. So the minimum label-consistent cut-size is "1" and a natural strategy is simply to predict with the "nearest-neighbor" revealed label and trivially our mistake bound is log |p|.
Generalizing to multiple paths we have the following strategy.

Strategy fixed-paths(P): Given a PQ-graph choose a path cover {p1, . . . , pk} = P  P(G).

If the path cover is also vertex-disjoint except for the source and target vertex we may directly use the

"nearest-neighbor" strategy detailed above, achieving the mistake upper bound M 

k i=1

log

|pi

|.

Unsurprisingly, in the vertex-disjoint case it is a mistake-bound optimal [11] algorithm. If, however,

P is not vertex-disjoint and we need to predict a vertex V we may select a path in P containing V and predict with the nearest neighbour and also obtain the bound above. In this case, however, the bound may not be "optimal." Essentially the same technique was used in [20] in a related setting for learning "directed cuts." A limitation of the fixed-paths strategy is that it does not seem possible to extend into a strategy that can play a sequence of PQ-games and still meet the regularity properties, particularly permutation-invariance as required by Theorem 4.

Strategy 0-Ising: The prediction of the Ising model in the limit of zero temperature (cf. (1)), is equivalent to those of the well-known Halving algorithm [21, 22] where the hypothesis class U

is the set of label-consistent minimum-cuts. The mistake upper bound of the Halving algorithm is just M  log |U| where this bound follows from the observation that whenever a mistake is made at least "half" of concepts in U are no longer consistent. We observe that we may upper

bound |U |  argminP P(G)

k i=1

|pi|

since

the

product

of

path

lengths

from

any

path

cover

P

is an upper bound on the cardinality of U and hence we have the bound in (2). And in fact this

bound may be a significant improvement over the fixed-paths strategy's bound as seen in the

following proposition.

Proposition 6 (Proof in Appendix C). For every c  2 there exists a PQ-graph Gc, with a path cover P  P(Gc) and a PQ-game example sequence such that the mistakes Mfixed-paths(P ) = (c2),

while for all PQ-game example sequences on Gc the mistakes M0-Ising = O(c).

Unfortunately the 0-Ising strategy has the drawback that counting label-consistent minimum-cuts is #P-complete and computing the prediction (see (1)) is NP-hard (see Appendix G).

Strategy longest-path: In our search for an efficient and regular prediction strategy it seems natural to attempt to "dynamize" the fixed-paths approach and predict with a nearest neighbor along a dynamic path. Two such permutation-invariant methods are the longest-path and shortest-path strategies. The strategy shortest-path predicts the label of a super-vertex Z in a PQ-game G as 0 iff the shortest directed path (, . . . , Z) is shorter than the shortest directed path (Z, . . . , ). The strategy longest-path predicts the label of a super-vertex Z in a PQ-game G as 0 iff the longest directed path (, . . . , Z) is shorter than the longest directed path (Z, . . . , ). The strategy shortest-path seems to be intuitively favored over longest-path as it is just

6

Input: Graph: G, Example sequence: S = (i1, 0), (i2, 1), (i3, y3), . . . , (i , y )  (INn x {0, 1})

Initialization: G3 = PicardQueyranneGraph(G, S2)

for t = 3, . . . , do

Receive: it  {1, . . . , n}

It = V  V (Gt) with it  V

0 Predict (longest-path): yt = 1

|longest-path(Gt, t, It)|  |longest-path(Gt, It, otherwise

Predict (0-Ising):

yt = yI0(it|St-1)

Receive: yt

if (it  t or yt = 1) and (it  t or yt = 0) then

Gt+1

=

merge(Gt, merge(Gt,

{It}) {It})

yt = 0 yt = 1

else

Gt+1 = PicardQueyranneGraph(G, St) end

% as per equation (1) % cut unchanged
% cut increases

t)|

Figure 3: Longest-path and 0-Ising online prediction

the "nearest-neighbor" prediction with respect to the geodesic distance. However, the following proposition shows that it is strictly worse than any fixed-paths strategy in the worst case. Proposition 7 (Proof in Appendix C). For every c  4 there exists a PQ-graph Gc and a PQ-game example sequence such that the mistakes Mshortest-path = (c2 log(c)), while for every path cover P  P(Gc) and for all PQ-game example sequences on Gc the mistakes Mfixed-paths(P ) = O(c2).

In contrast, for the strategy longest-paths in the proof of Theorem 8 we show that there al-

ways exists some retrospective path cover Plp  P(G) such that Mlongest-paths 

k i=1

log

|pilp|.

Computing the "longest-path" has time complexity linear in the number of edges in a DAG.

Summarizing the mistake bounds for the three PQ-game strategies for a single PQ-game we have

the following theorem. Theorem 8 (Proof in Appendix C). The mistakes, M , of an online PQ-game for player strategies

fixed-paths(P), 0-Ising, and longest-path on PQ-graph G and k-path cover P  P(G) is bounded by


 

k i=1

log

|pi|

M  argminP P(G)

argmaxP P(G)

k i=1

log

|pi|

k i=1

log

|pi

|

fixed-paths(P)

0-Ising

.

longest-path

(2)

3.3 Global analysis of prediction at zero temperature
In Figure 3 we summarize the prediction protocol for 0-Ising and longest-path. We claim the regularity properties of our strategies in the following theorem. Theorem 9 (Proof in Appendix E). The strategies 0-Ising and longest-path are permutation-invariant, label-monotone, and Markov.

The technical hurdle here is to prove that label-monotonicity holds over a sequence of PQ-games. For this we need an analog of Proposition 5 to describe how the PQ-graph changes when the labelconsistent minimum-cut increases (see Proposition 19). The application of the following theorem along with Theorem 4 implies we may bound the mistakes of each uniformly-labeled cluster in potentially three ways.

Theorem 10 (Proof in Appendix D). Given either the 0-Ising or longest-path strategy A the mistakes on uniformly-labeled subgraph C  G are bounded by

O(1)  BA(C; G)  O |eE(C)|(1 + |eE(C)| - (C)) log N (C)
O(|eE(C)| log D(C))

(C) > |eE(C)| (C)  |eE(C)|
C is a tree

(3)

with the atomic number N (C) := N|eE(C)|+1(C)  |V (C)|.

7

First, if the internal connectivity of the cluster is high we will only make a single mistake in that cluster. Second, if the cluster is a tree then we pay the external connectivity of the cluster |eE(C)| times the log of the cluster diameter. Finally, in the remaining case we pay quadratically in the external
connectivity and logarithmically in the "atomic number" of the cluster. The atomic number captures
the fact that even a poorly connected cluster may have sub-regions of high internal connectivity. Computational complexity. If G is a graph and S an example sequence with a label-consistent
minimum-cut of  then we may implement the longest-path strategy so that it has a cumulative computational complexity of O(max(, n) |E(G)|). This follows because if on a trial the "cut" does not increase we may implement prediction and update in O(|E(G)|) time. On the other hand if the "cut" increases by  we pay O( |E(G)|) time. To do so we implement an online "Ford-Fulkerson"
algorithm [18] which starts from the previous "residual" graph to which it then adds the additional  flow paths with  steps of size O(|E(G)|).

Discussion. There are essentially five dominating mistake bounds for the online graph labeling problem: (I) the bound of treeOpt [1] on trees, (II) the bound in expectation of treeOpt on a random spanning tree sampled from a graph [1], (III) the bound of p-seminorm interpolation [14] tuned for "sparsity" (p < 2), (IV) the bound of p-seminorm interpolation as tuned to be equivalent to online label propagation [5] (p = 2), (V) this paper's longest-path strategy.

The algorithm treeOpt was shown to be optimal on trees. In Appendix F we show that longest-path also obtains the same optimal bound on trees. Algorithm (II) applies to generic graphs and is obtained from (I) by sampling a random spanning tree (RST). It is not directly comparable to the other algorithms as its bound holds only in expectation with respect to the RST.

We use [14, Corollary 10] to compare (V) to (III) and (IV). We introduce the following simpli-

fying notation to compare bounds. Let C1, . . . , Cc denote uniformly-labeled clusters (connected subgraphs) which cover the graph and set r := (Cr) and r := |eE(Cr)|. We define Dr(i) to be the wide diameter at connectivity level i of cluster Cr. The wide diameter Dr(i) is the minimum

value such that for all pairs of vertices v, w  Cr there exists i edge-disjoints of paths from v to w of

length at least Dr(i) in Cr (and if i > r then Dr(i) := +). Thus Dr(1) is the diameter of cluster

Cr and Dr(1)  Dr(2)  * * * . Let  denote the minimum label-consistent cutsize and observe that

if the cardinality of the cover |{C1, . . . , Cc}| is minimized then we have that 2 =

c r=1

r

.

Thus using [14, Corollary 10] we have the following upper bounds of (III): (/)2 log D + c and

(IV): (/)D + c where  := minr r and D := maxr Dr(). In comparison we have (V):

[

c r=1

max(0,

r

-

r

+ 1)r log Nr] + c with atomic numbers Nr

:=

Nr +1 (Cr ).

To contrast

the bounds, consider a double lollipop labeled-graph: first create a lollipop which is a path of n/4

vertices attached to a clique of n/4 vertices. Label these vertices 1. Second, clone the lollipop

except with labels 0. Finally join the two cliques with n/8 edges arbitrarily. For (III) and (IV) the bounds are O(n) independent of the choice of clusters. Whereas an upper bound for (V) is the exponentially smaller O(log n) which is obtained by choosing a four cluster cover consisting of the

two paths and the two cliques. This emphasizes the generic problem of (III) and (IV): parameters  and D are defined by the worst clusters; whereas (V) is truly a per-cluster bound. We consider

the previous "constructed" example to be representative of a generic case where the graph contains

clusters of many resistance diameters as well as sparse interconnecting "background" vertices.

On the other hand, there are cases in which (III,IV) improve on (V). For a graph with only small diameter clusters and if the cutsize exceeds the cluster connectivity then (IV) improves on (III,V) given the linear versus quadratic dependence on the cutsize. The log-diameter may be arbitrarily smaller than log-atomic-number ((III) improves on (V)) and also vice-versa. Other subtleties not accounted for in the above comparison include the fact a) the wide diameter is a crude upper bound for resistance diameter (cf. [14, Theorem 1]) and b) the clusters of (III,IV) are not required to be uniformly-labeled. Regarding "a)" replacing "wide" with "resistance" does not change the fact the bound now holds with respect to the worst resistance diameter and the example above is still problematic. Regarding "b)" it is a nice property but we do not know how to exploit this to give an example that significantly improves (III) or (IV) over a slightly more detailed analysis of (V). Finally (III,IV) depend on a correct choice of tunable parameter p.

Thus in summary (V) matches the optimal bound of (I) on trees, and can often improve on (III,IV) when a graph is naturally covered by label-consistent clusters of different diameters. However (III,IV) may improve on (V) in a number of cases including when the log-diameter is significantly smaller than log-atomic-number of the clusters.

8

References
[1] Nicolo Cesa-Bianchi, Claudio Gentile, and Fabio Vitale. Fast and optimal prediction on a labeled tree. In Proceedings of the 22nd Annual Conference on Learning. Omnipress, 2009.
[2] Avrim Blum and Shuchi Chawla. Learning from labeled and unlabeled data using graph mincuts. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML '01, pages 19-26, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.
[3] Olivier Chapelle, Jason Weston, and Bernhard Scholkopf. Cluster kernels for semi-supervised learning. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, pages 601-608. MIT Press, 2003.
[4] Mikhail Belkin and Partha Niyogi. Semi-supervised learning on riemannian manifolds. Mach. Learn., 56(1-3):209-239, 2004.
[5] Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty. Semi-supervised learning using gaussian fields and harmonic functions. In ICML, pages 912-919, 2003.
[6] Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Scholkopf. Learning with local and global consistency. In NIPS, 2003.
[7] Martin Szummer and Tommi Jaakkola. Partially labeled classification with markov random walks. In NIPS, pages 945-952, 2001.
[8] Leslie Ann Goldberg and Mark Jerrum. The complexity of ferromagnetic ising with local fields. Combinatorics, Probability & Computing, 16(1):43-61, 2007.
[9] Jean-Claude Picard and Maurice Queyranne. On the structure of all minimum cuts in a network and applications. In V.J. Rayward-Smith, editor, Combinatorial Optimization II, volume 13 of Mathematical Programming Studies, pages 8-16. Springer Berlin Heidelberg, 1980.
[10] J. Scott Provan and Michael O. Ball. The complexity of counting cuts and of computing the probability that a graph is connected. SIAM Journal on Computing, 12(4):777-788, 1983.
[11] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285-318, April 1988.
[12] Mark Herbster, Massimiliano Pontil, and Lisa Wainer. Online learning over graphs. In ICML '05: Proceedings of the 22nd international conference on Machine learning, pages 305-312, New York, NY, USA, 2005. ACM.
[13] Mark Herbster. Exploiting cluster-structure to predict the labeling of a graph. In Proceedings of the 19th International Conference on Algorithmic Learning Theory, pages 54-69, 2008.
[14] Mark Herbster and Guy Lever. Predicting the labelling of a graph via minimum p-seminorm interpolation. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT'09), 2009.
[15] Mark Herbster, Guy Lever, and Massimiliano Pontil. Online prediction on large diameter graphs. In Advances in Neural Information Processing Systems (NIPS 22), pages 649-656. MIT Press, 2009.
[16] Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, and Giovanni Zappella. Random spanning trees and the prediction of weighted graphs. In Proceedings of the 27th International Conference on Machine Learning (27th ICML), pages 175-182, 2010.
[17] Fabio Vitale, Nicolo Cesa-Bianchi, Claudio Gentile, and Giovanni Zappella. See the tree through the lines: The shazoo algorithm. In John Shawe-Taylor, Richard S. Zemel, Peter L. Bartlett, Fernando C. N. Pereira, and Kilian Q. Weinberger, editors, NIPS, pages 1584-1592, 2011.
[18] L. R. Ford and D. R. Fulkerson. Maximal Flow through a Network. Canadian Journal of Mathematics, 8:399-404, 1956.
[19] Michael O. Ball and J. Scott Provan. Calculating bounds on reachability and connectedness in stochastic networks. Networks, 13(2):253-278, 1983.
[20] Thomas Gartner and Gemma C. Garriga. The cost of learning directed cuts. In Proceedings of the 18th European Conference on Machine Learning, 2007.
[21] J. M. Barzdin and R. V. Frievald. On the prediction of general recursive functions. Soviet Math. Doklady, 13:1224-1228, 1972.
[22] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Inf. Comput., 108(2):212- 261, 1994.
9

