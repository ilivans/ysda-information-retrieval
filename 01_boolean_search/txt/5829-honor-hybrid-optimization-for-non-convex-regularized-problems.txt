HONOR: Hybrid Optimization for NOn-convex Regularized problems

Pinghua Gong

Jieping Ye

Univeristy of Michigan, Ann Arbor, MI 48109 Univeristy of Michigan, Ann Arbor, MI 48109

gongp@umich.edu

jpye@umich.edu

Abstract
Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.
1 Introduction
Sparse learning with convex regularization has been successfully applied to a wide range of applications including marker genes identification [19], face recognition [22], image restoration [2], text corpora understanding [9] and radar imaging [20]. However, it has been shown recently that many convex sparse learning formulations are inferior to their non-convex counterparts in both theory and practice [27, 12, 23, 25, 16, 26, 24, 11]. Popular non-convex sparsity-inducing penalties include Smoothly Clipped Absolute Deviation (SCAD) [10], Log-Sum Penalty (LSP) [6] and Minimax Concave Penalty (MCP) [23]. Although non-convex sparse learning reveals its advantage over the convex one, it remains a challenge to develop an efficient algorithm to solve the non-convex optimization problem especially for large-scale data.
DC programming [21] is a popular approach to solve non-convex problems whose objective functions can be expressed as the difference of two convex functions. However, a potentially non-trivial convex subproblem is required to solve at each iteration, which is not practical for large-scale problems. SparseNet [16] can solve a least squares problem with a non-convex penalty. At each step, SparseNet solves a univariate subproblem with a non-convex penalty which admits a closed-form solution. However, to establish the convergence analysis, the parameter of the non-convex penalty is required to be restricted to some interval such that the univariate subproblem (with a non-convex penalty) is convex. Moreover, it is quite challenging to extend SparseNet to non-convex problems with a non-least-squares loss, as the univariate subproblem generally does not admit a closed-form solution. The GIST algorithm [14] can solve a class of non-convex regularized problems by iteratively solving a possibly non-convex proximal operator problem, which in turn admits a closed-form solution. However, GIST does not well exploit the second-order information. The DC-PN algorithm
1

[18] can incorporate the second-order information to solve non-convex regularized problems but it requires to solve a non-trivial regularized quadratic subproblem at each iteration.
In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR), which incorporates the second-order information to speed up the convergence. HONOR adopts a hybrid optimization scheme which chooses either a Quasi-Newton (QN) step or a Gradient Descent (GD) step per iteration mainly depending on whether an iterate has very small components. If an iterate does not have any small component, the QN-step is adopted, which uses L-BFGS to exploit the second-order information. The key advantage of the QN-step is that it does not need to solve a regularized quadratic programming and only involves matrix-vector multiplications without explicitly forming the inverse Hessian matrix. If an iterate has small components, we switch to a GD-step. Our detailed theoretical analysis sheds light on the effect of such a hybrid scheme on the convergence of the algorithm. Specifically, we provide a rigorous convergence analysis for HONOR, which shows that every limit point of the sequence generated by HONOR is a Clarke critical point. It is worth noting that the convergence analysis for a non-convex problem is typically much more challenging than the convex one, because many important properties for a convex problem may not hold for non-convex problems. Empirical studies are also conducted on large-scale data sets which include up to millions of samples and features; results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.

2 Non-convex Sparse Learning

We focus on the following non-convex regularized optimization problem: min {f (x) = l(x) + r(x)} ,
xRn
where we make the following assumptions throughout the paper:

(1)

(A1) l(x) is coercive, continuously differentiable and l(x) is Lipschitz continuous with constant L. Moreover, l(x) > - for all x  Rn.

(A2)

r(x) cave

= with

rnie=sp1 ec(t|xtoi|)t,inwh[0e,re )(;t)(i0s)n=on-0daencrdeas(in0g),

continuously differentiable = 0 with (t) = (t)/t

and condenoting

the derivative of (t) at the point t.

Remark 1 Assumption (A1) allows l(x) to be non-convex. Assumption (A2) implies that (|xi|) is generally non-convex with respect to xi and the only convex case is (|xi|) = |xi| with  > 0. Moreover, (|xi|) is continuously differentiable with respect to xi in (-, 0)  (0, ) and nondifferentiable at xi = 0. In particular, (|xi|)/xi = (xi)(|xi|) for any xi = 0, where (xi) = 1, if xi > 0; (xi) = -1, if xi < 0 and (xi) = 0, otherwise. In addition, (0) > 0 must hold (Otherwise (0) < 0 implies (t)  (0) + (0)t < 0 for any t > 0, contradicting the fact
that (t) is non-decreasing). It is also easy to show that, under the assumptions above, both l(x)
and r(x) are locally Lipschitz continuous. Thus, the Clarke subdifferential [7] is well-defined.

The commonly used least squares loss and the logistic regression loss satisfy the assumption (A1); we can add a small term  x 2 to make them coercive. The following popular non-convex regularizers satisfy the assumption (A2), where  > 0 and  > 0 except that  > 2 for SCAD.

* LSP: (|xi|) =  log(1 + |xi|/).

 |xi|,

*

 SCAD: (|xi|) =

-x2i +2|xi|-2 2(-1)

,

 ( + 1)2/2,

if |xi|  ,
if  < |xi|  , if |xi| > .

* MCP: (|xi|) =

|xi| - x2i /(2), if |xi|  ,

2/2,

if |xi| > .

Due to the non-convexity and non-differentiability of problem (1), the traditional subdifferential
concept for the convex optimization is not applicable here. Thus, we use the Clarke subdifferential [7] to characterize the optimality of problem (1). We say x is a Clarke critical point of problem (1), if 0  of (x), where of (x) is the Clarke subdifferential of f (x) at x = x. To be self-contained,

2

we briefly review the Clarke subdifferential: for a locally Lipschitz continuous function f (x), the Clarke generalized directional derivative of f (x) at x = x along the direction d is defined as

f

o (x ;

d)

=

lim sup
xx,0

f

(x

+

d) 

-

f

(x)

.

Then, the Clarke subdifferential of f (x) at x = x is defined as

of (x) = {  Rn : f o(x; d)  dT , d  Rn}.

Interested readers may refer to Proposition 4 in the Supplement A for more properties about the Clarke Subdifferential. We want to emphasize that some basic properties of the subdifferential of a convex function may not hold for the Clarke Subdifferential of a non-convex function.

3 Proposed Optimization Algorithm: HONOR

Since each decomposable component function of the regularizer is only non-differentiable at the origin, the objective function is differentiable, if the segment between any two consecutive iterates do not cross any axis. This motivates us to design an algorithm which can keep the current iterate in the same orthant of the previous iterate. Before we present the detailed HONOR algorithm, we introduce two functions as follows:

Define a function  : Rn  Rn with the i-th entry being:

i(xi; yi) =

xi, if (xi) = (yi), 0, otherwise,

where y  Rn (yi is the i-th entry of y) is the parameter of the function ; (*) is the sign function defined as follows: (xi) = 1, if xi > 0; (xi) = -1, if xi < 0 and (xi) = 0, otherwise.
Define the pseudo-gradient f (x) whose i-th entry is given by:

 il(x) + (|xi|),

   

il(x) - (|xi|),

if (x) = il(x) + (0),

 

il(x) - (0),

 

0,

if xi > 0, if xi < 0, if xi = 0, il(x) + (0) < 0, if xi = 0, il(x) - (0) > 0, otherwise,

where (t) is the derivative of (t) at the point t.

Remark 2 If r(x) is convex, f (x) is the minimum-norm sub-gradient of f (x) at x. Thus, -  f (x) is a descent direction. However, f (x) is not even a sub-gradient of f (x) if r(x) is non-convex. This indicates that some obvious concepts and properties for a convex problem may not hold in the
non-convex case. Thus, it is significantly more challenging to develop and analyze algorithms for a
non-convex problem.

Interestingly, we can still show that vk = -  f (xk) is a descent direction at the point xk (refer to Supplement D and replace pk = (dk; vk) with vk). To utilize the second-order information, we may perform the optimization along the direction dk = Hkvk, where Hk is a positive definite matrix containing the second-order information. However, dk is not necessarily a descent direction. To address this issue, we use the following slightly modified direction pk:

pk = (dk; vk).

We can show that pk is a descent direction (proof is provided in Supplement D). Thus, we can perform the optimization along the direction pk. Recall that we need to keep the current iterate in
the same orthant of the previous iterate. So the following iterative scheme is proposed:

xk() = (xk + pk; k),

(2)

where

ik =

(xki ), if xki = 0, (vik), if xki = 0,

(3)

3

and  is a step size chosen by the following line search procedure: for constants 0 > 0, ,   (0, 1) and m = 0, 1, * * * , find the smallest integer m with  = 0m such that the following inequality holds:

f (xk())  f (xk) - (vk)T dk.

(4)

However, only using the above iterative scheme may not guarantee the convergence. The main challenge is: if there exists a subsequence K such that {xki }K converges to zero, it is possible that for sufficiently large k  K, |xki | is arbitrarily small but never equal to zero (refer to the proof of Theorem 1 for more details). To address this issue, we propose a hybrid optimization scheme. Specifically, for a small constant  > 0, if Ik = {i  {1, * * * , n} : 0 < |xki |  min( vk , ), xki vik < 0} is not empty, we switch the iteration to the following gradient descent step (GD-step):

xk() = arg min
x

l(xk )T

(x

-

xk )

+

1 2

x - xk

2 + r(x)

,

where  is a step size chosen by the following line search procedure: for constants 0 > 0, ,   (0, 1) and m = 0, 1, * * * , find the smallest integer m with  = 0m such that the following inequality holds:

f (xk())



f (xk)

-

 2

xk() - xk

2.

(5)

The detailed steps of the algorithm are presented in Algorithm 1.

Remark 3 Algorithm 1 is similar to OWL-QN-type algorithms in [1, 3, 4, 17, 13]. However,
HONOR is significantly different from them: (1) The OWL-QN-type algorithms can only handle 1regularized convex problems while HONOR is applicable to a class of non-convex problems beyond
1-regularized ones. (2) The convergence analyses of the OWL-QN-type algorithms heavily rely on the convexity of the 1-regularized problem. In contrast, the convergence analysis for HONOR is applicable to non-convex cases beyond the convex ones, which is a non-trivial extension.

Algorithm 1: HONOR: Hybrid Optimization for NOn-convex Regularized problems

1 Initialize x0, H0 and choose ,   (0, 1),  > 0, 0 > 0; 2 for k = 0 to maxiter do

3 Compute vk  -  f (xk) and Ik = {i  {1, * * * , n} : 0 < |xki |  k, xki vik < 0}, where k = min( vk , );

4 Initialize   0; 5 if Ik =  then

6 (QN-step)

7 Compute dk  Hkvk with a positive definite matrix Hk using L-BFGS;

8 Alignment: pk  (dk; vk);

9 while Eq. (4) is not satisfied do

10   ; xk()  (xk + pk; k);

11 end

12 else

13 (GD-step)

14 while Eq. (5) is not satisfied do

15   ;

16

xk()  arg minx

l(xk)T (x

-

xk )

+

1 2

x - xk 2 + r(x) ;

17 end

18 end
19 xk+1  xk(); 20 if some stopping criterion is satisfied then 21 stop and return xk+1;
22 end

23 end

4

4 Convergence Analysis
We first present a few basic propositions and then provide the convergence theorem based on the propositions; all proofs of the presented propositions are carefully handled due to the lack of convexity. First of all, an optimality condition is presented (proof is provided in Supplement B), which will be directly used in the proof of Theorem 1.
Proposition 1 Let x = limkK,k xk, vk = -  f (xk) and v = -  f (x), where K is a subsequence of {1, 2, * * * , k, k + 1, * * * }. If lim infkK,k |vik| = 0 for all i  {1, * * * , n}, then v = 0 and x is a Clarke critical point of problem (1).
We subsequently show that we have a Lipschitz-continuous-like inequality in the following proposition (proof is provided in Supplement C), which is crucial to prove the final convergence theorem.

Proposition 2

Let vk

= -f (xk), xk() = (xk +pk; k) and qk =

1 

((xk

+

pk

;

k

)

-

xk

)

with  > 0. Then under assumptions (A1) and (A2), we have

(i) l(xk)T (xk() - xk) + r(xk()) - r(xk)  -(vk)T (xk() - xk),

(ii)

f (xk())



f (xk)

-

(vk )T

qk

+

2L 2

qk

2.

(6) (7)

We next show that both line search criteria in the QN-step [Eq. (4)] and the GD-step [Eq. (5)] at any iteration k is satisfied in a finite number of trials (proof is provided in Supplement D).
Proposition 3 At any iteration k of the HONOR algorithm, if xk is not a Clarke critical point of problem (1), then (a) for the QN-step, there exists an   [k, 0] with 0 < k  0 such that the line search criterion in Eq. (4) is satisfied; (b) for the GD-step, the line search criterion in Eq. (5) is satisfied whenever    min(0, (1 - )/L). That is, both line search criteria at any iteration k are satisfied in a finite number of trials.

We are now ready to provide the convergence proof for the HONOR algorithm: Theorem 1 The sequence {xk} generated by the HONOR algorithm has at least a limit point and every limit point of {xk} is a Clarke critical point of problem (1).

Proof It follows from Proposition 3 that both line search criteria in the QN-step [Eq. (4)] and the GD-step [Eq. (5)] at each iteration can be satisfied in a finite number of trials. Let k be the accepted
step size at iteration k. Then we have

f (xk) - f (xk+1)  k(vk)T dk = k(vk)T Hkvk (QN-step),

or

f (xk)

- f (xk+1)



 2k

xk+1 - xk

2  20

xk+1 - xk

2 (GD-step).

(8) (9)

Recall that Hk is positive definite and  > 0, k > 0, which together with Eqs.(8), (9) imply that {f (xk)} is monotonically decreasing. Thus, {f (xk)} converges to a finite value f, since f is bounded from below (note that l(x) > - and r(x)  0 for all x  Rn). Due to the boundedness of {xk} (see Proposition 7 in Supplement F), the sequence {xk} generated by the HONOR algorithm has at least a limit point x. Since f is continuous, there exists a subsequence K of {1, 2 * * * , k, k + 1, * * * } such that

lim xk = x,
kK,k

lim f (xk) = lim f (xk) = f = f (x).

k

kK,k

(10) (11)

In the following, we prove the theorem by contradiction. Assume that x is not a Clarke critical point of problem (1). Then by Proposition 1, there exists at least one i  {1, * * * , n} such that

lim inf
kK,k

|vik |

>

0.

(12)

5

We next consider the following two cases:

(a) There exist a subsequence K of K and an integer k > 0 such that for all k  K, k  k, the GD-step is adopted. Then for all k  K, k  k, we have

xk+1 = arg min
x

l(xk)T (x

-

xk )

+

1 2k

x - xk

2 + r(x)

.

Thus, by the optimality condition of the above problem and properties of the Clarke subdifferential (Proposition 4 in Supplement A), we have

0



l(xk )

+

1 k

(xk+1

-

xk )

+

 o r(xk+1 ).

Taking limits with k  K for Eq. (9) and considering Eqs. (10), (11), we have

(13)

lim xk+1 - xk 2  0  lim xk = lim xk+1 = x.

kK ,k

kK ,k

kK ,k

(14)

Taking limits with k  K for Eq. (13) and considering Eq. (14), k   min(0, (1 - )/L) [Proposition 3] and or(*) is upper-semicontinuous (upper-hemicontinuous) [8] (see Proposition 4 in the Supplement A), we have

0  l(x) + or(x) = of (x),

which contradicts the assumption that x is not a Clarke critical point of problem (1).

(b) There exists an integer k > 0 such that for all k  K, k  k, the QN-step is adopted. According to Remark 7 (in Supplement F), we know that the smallest eigenvalue of Hk is uniformly bounded from below by a positive constant, which together with Eq. (12) implies

lim inf (vk)T Hkvk > 0.
kK,k

(15)

Taking limits with k  K for Eq. (8), we have

lim k(vk)T Hkvk  0,
kK,k

which together with   (0, 1), k  (0, 0] and Eq. (15) implies that

lim k = 0.
kK,k

(16)

Eq. (12) implies that there exist an integer k > 0 and a constant  > 0 such that k = min( vk , )   for all k  K, k  k. Notice that for all k  K, k  k, the QN-step is adopted. Thus, we obtain that Ik = {i  {1, * * * , n} : 0 < |xki |  k, xki vik < 0} =  for all k  K, k  k. We also notice that, if |xki |  , then there exists a constant i > 0 such that xki () = i(xki + pki ; ik) = xki + pki for all   (0, i], as {pki } is bounded (Proposition 8 in Supplement F). Therefore, we conclude that, for all k  K, k  k = max(k, k) and for all
i  {1, * * * , n}, at least one of the following three cases must happen:

xki = 0  xki () = i(xki + pki ; ik) = xki + pki ,  > 0, or |xki | > k    xki () = i(xki + pki ; ik) = xki + pki ,   (0, i], or xki vik  0  xki pki  0  xki () = i(xki + pki ; ik) = xki + pki ,  > 0.

It follows that there exists a constant  > 0 such that

qk

=

1 

(xk

()

- xk)

=

pk ,

k



K, k



k, 



(0, ].

(17)

Thus, considering |pki | = |i(dki ; vik)|  |dki | and vikpki  vikdki for all i  {1, * * * , n}, we have

qk 2 = pk 2  dk 2 = (vk)T (Hk)2vk, k  K, k  k,   (0, ], (vk)T qk = (vk)T pk  (vk)T dk = (vk)T Hkvk, k  K, k  k,   (0, ].

(18) (19)

6

According to Proposition 8 (in Supplement F), we know that the largest eigenvalue of Hk is uniformly bounded from above by some positive constant M . Thus, we have

(vk)T (Hk)2vk



2 L

(vk

)T

H

k

vk

-

2 L

-

M

(vk)T Hkvk, k,

which together with Eqs. (18), (19) and dk = Hkvk implies

qk

2

2 L

(vk )T

qk

-

2 L

-

M

(vk)T dk, k  K, k  k,   (0, ].

(20)

Considering Eqs. (7), (20), we have

f (xk())  f (xk) - 

1

-

LM 2

(vk)T dk, k  K, k  k,   (0, ],

which together with (vk)T dk = (vk)T Hkvk  0 implies that the line search criterion in the QN-step [Eq. (4)] is satisfied if

1-

LM 2

  , 0 <   0 and 0 <   , k  K, k  k.

Considering the backtracking form of the line search in QN-step [Eq. (4)], we conclude that the line search criterion in the QN-step [Eq. (4)] is satisfied whenever

k   min(min(, 0), 2(1 - )/(LM )) > 0, k  K, k  k.

This leads to a contradiction with Eq. (16). By (a) and (b), we conclude that x = limkK,k xk is a Clarke critical point of problem (1).

5 Experiments

In this section, we evaluate the efficiency of HONOR on solving the non-convex regularized lo-

gistic regression problem1 by setting l(x) = 1/N Rn is the i-th sample associated with the label yi

N i=1

log(1

+

exp(-yiaTi x)),

where ai



 {1, -1}. Three non-convex regulariz-

ers (LSP, MCP and SCAD) are included in experiments, where the parameters are set as  =

1/N and  = 10-2 ( is set as 2 + 10-2 for SCAD as it requires  > 2). We com-

pare HONOR with the non-convex solver2 GIST [14] on three large-scale, high-dimensional

and sparse data sets which are summarized in Table 1. All data sets can be downloaded from

http://www.csie.ntu.edu.tw/cjlin/libsvmtools/datasets/.

All algorithms are implemented in Mat-

Table 1: Data set statistics.

lab 2015a under a Linux operating system and executed on an Intel Core i7-4790 CPU (@3.6GHz) with 32GB memory. We choose the starting points

datasets

kdd2010a kdd2010b

url

 samples N

510,302

748,401 2,396,130

dimensionality n 20,216,830 29,890,095 3,231,961

x0 for the compared algorithms using the same random vector whose entries are i.i.d. sampled from

the standard Gaussian distribution. We terminate the compared algorithms if the relative change of two consecutive objective function values is less than 10-5 or the number of iterations exceeds 1000 (HONOR) or 10000 (GIST). For HONOR, we set  = 10-5,  = 0.5, 0 = 1 and the number of unrolling steps in L-BFGS as m = 10. For GIST, we use the non-monotone line search in exper-

iments as it usually performs better than its monotone counterpart. To show how the convergence behavior of HONOR varies over the parameter , we use three values:  = 10-10, 10-6, 10-2.

We report the objective function value (in log-scale) vs. CPU time (in seconds) plots in Figure 1. We can observe from Figure 1 that: (1) If  is set to a small value, the QN-step is adopted at almost all steps in HONOR and HONOR converges significantly faster than GIST for all three non-convex

1We do not include the term  x 2 in the objective and find that the proposed algorithm still works well. 2We do not involve SparseNet, DC programming and DC-PN in comparison, because (1) adapting
SparseNet to the logistic regression problem is challenging; (2) DC programming is shown to be much in-
ferior to GIST; (3) The objective function value of DC-PN is larger than GIST in most cases [18].

7

regularizers on all three data sets. This shows that using the second-order information greatly speeds up the convergence. (2) When  increases, the ratio of the GD-step adopted in HONOR increases. Meanwhile, the convergence performance of HONOR generally degrades. In some cases, setting a slightly larger  and adopting a small number of GD steps even sligtly boosts the convergence performance of HONOR (the green curves in the first row). But setting  to a very small value is always safe to guarantee the fast convergence of HONOR. (3) When  is large enough, the GD steps dominate all iterations of HONOR and HONOR converge much slower. In this case, HONOR con-
verges even slower than GIST. The reason is that, at each iteration of HONOR, extra computational
cost is required in addition to the basic computation in the GD-step. Moreover, the non-monotone
line search is used in GIST while the monotone line search is adopted in the GD-step. (4) In some
cases (the first row), GIST is trapped in a local solution which has a much larger objective function value than HONOR with a small . This implies that HONOR may have a potential of escaping from high error plateau which often exists in high dimensional non-convex problems. These results
show the great potential of HONOR for solving large-scale non-convex sparse learning problems.

LSP (kdd2010a)

LSP (kdd2010b)

LSP (url)

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST
101 101

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST

101

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST

100

Objective function value (logged scale)

Objective function value (logged scale)

Objective function value (logged scale)

100 0

200 400 600 800 1000 1200 CPU time (seconds)
MCP (kdd2010a)

100 0

200 400 600 800 1000 1200 CPU time (seconds)
MCP (kdd2010b)

10-1 0

2000

4000 6000 8000 10000 12000 14000 CPU time (seconds)
MCP (url)

Objective function value (logged scale)

Objective function value (logged scale)

Objective function value (logged scale)

100 10-1 10-2

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST

100 10-1 10-2 10-3

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST

100 10-1 10-2

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST

0 500 1000 1500 2000 2500 3000 CPU time (seconds)
SCAD (kdd2010a)

10-3

0 500 1000 1500 2000 2500 3000 3500 4000

0 0.5 1 1.5 2 2.5 3

CPU time (seconds)

CPU time (seconds)

x 10 4

SCAD (kdd2010b)

SCAD (url)

Objective function value (logged scale)

Objective function value (logged scale)

Objective function value (logged scale)

100 10-1

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST

10-2 0

500

1000

1500

2000

2500

CPU time (seconds)

100 10-1 10-2 10-3
0

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST

1000

2000

3000

CPU time (seconds)

4000

100 10-1

HONOR(=1e-10) HONOR(=1e-6) HONOR(=1e-2) GIST

10-2

10-3 0

0.5 1 1.5 2 CPU time (seconds)

2.5 x 10 4

Figure 1: Objective function value (in log-scale) vs. CPU time (in seconds) plots for differ-
ent non-convex regularizers and different large-scale and high-dimensional data sets. The ratios of the GD-step adopted in HONOR are: LSP (kdd2010a): 0%, 1%, 34%; LSP (kdd2010b): 0%, 2%, 27%; LSP (url): 0.1%, 2%, 35%; MCP (kdd2010a): 0%, 88%, 100%; MCP (kdd2010b): 0%, 89%, 100%; MCP (url): 0%, 97%, 100%; SCAD (kdd2010a): 0%, 43%, 100%; SCAD (2010b): 0%, 32%, 99.5%; SCAD (url): 0%, 79%, 100%.

6 Conclusions

In this paper, we propose an efficient optimization algorithm called HONOR for solving non-convex regularized sparse learning problems. HONOR incorporates the second-order information to speed up the convergence in practice and uses a carefully designed hybrid optimization scheme to guarantee the convergence in theory. Experiments are conducted on large-scale data sets and results show that HONOR converges significantly faster than state-of-the-art algorithms. In our future work, we plan to develop parallel/distributed variants of HONOR to tackle much larger data sets.

Acknowledgements
This work is supported in part by research grants from NIH (R01 LM010730, U54 EB020403) and NSF (IIS- 0953662, III-1539991, III-1539722).

8

References
[1] G. Andrew and J. Gao. Scalable training of 1-regularized log-linear models. In ICML, pages 33-40, 2007.
[2] J. Bioucas-Dias and M. Figueiredo. A new TwIST: two-step iterative shrinkage/thresholding algorithms for image restoration. IEEE Transactions on Image Processing, 16(12):2992-3004, 2007.
[3] R. H. Byrd, G. M. Chin, J. Nocedal, and F. Oztoprak. A family of second-order methods for convex 1-regularized optimization. Technical report, Industrial Engineering and Management Sciences, Northwestern University, Evanston, IL, 2012.
[4] R. H. Byrd, G. M. Chin, J. Nocedal, and Y. Wu. Sample size selection in optimization methods for machine learning. Mathematical Programming, 134(1):127-155, 2012.
[5] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained optimization. SIAM Journal on Scientific Computing, 16(5):1190-1208, 1995.
[6] E. Candes, M. Wakin, and S. Boyd. Enhancing sparsity by reweighted 1 minimization. Journal of Fourier Analysis and Applications, 14(5):877-905, 2008.
[7] F. Clarke. Optimization and Nonsmooth Analysis. John Wiley&Sons, New York, 1983.
[8] J. Dutta. Generalized derivatives and nonsmooth optimization, a finite dimensional tour. Top, 13(2):185- 279, 2005.
[9] L. El Ghaoui, G. Li, V. Duong, V. Pham, A. Srivastava, and K. Bhaduri. Sparse machine learning methods for understanding large text corpora. In CIDU, pages 159-173, 2011.
[10] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96(456):1348-1360, 2001.
[11] J. Fan, L. Xue, and H. Zou. Strong oracle optimality of folded concave penalized estimation. Annals of Statistics, 42(3):819, 2014.
[12] G. Gasso, A. Rakotomamonjy, and S. Canu. Recovering sparse signals with a certain family of nonconvex penalties and dc programming. IEEE Transactions on Signal Processing, 57(12):4686-4698, 2009.
[13] P. Gong and J. Ye. A modified orthant-wise limited memory quasi-newton method with convergence analysis. In ICML, 2015.
[14] P. Gong, C. Zhang, Z. Lu, J. Huang, and J. Ye. A general iterative shrinkage and thresholding algorithm for non-convex regularized optimization problems. In ICML, volume 28, pages 37-45, 2013.
[15] N. Jorge and J. Stephen. Numerical Optimization. Springer, 1999.
[16] R. Mazumder, J. Friedman, and T. Hastie. Sparsenet: Coordinate descent with nonconvex penalties. Journal of the American Statistical Association, 106(495), 2011.
[17] P. Olsen, F. Oztoprak, J. Nocedal, and S. Rennie. Newton-like methods for sparse inverse covariance estimation. In Advances in Neural Information Processing Systems (NIPS), pages 764-772, 2012.
[18] A. Rakotomamonjy, R. Flamary, and G. Gasso. Dc proximal newton for non-convex optimization problems. 2014.
[19] S. Shevade and S. Keerthi. A simple and efficient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19(17):2246, 2003.
[20] X. Tan, W. Roberts, J. Li, and P. Stoica. Sparse learning via iterative minimization with application to mimo radar imaging. IEEE Transactions on Signal Processing, 59(3):1088-1101, 2011.
[21] P. Tao and L. An. The dc (difference of convex functions) programming and dca revisited with dc models of real world nonconvex optimization problems. Annals of Operations Research, 133(1-4):23-46, 2005.
[22] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. Robust face recognition via sparse representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(2):210-227, 2008.
[23] C. Zhang. Nearly unbiased variable selection under minimax concave penalty. The Annals of Statistics, 38(2):894-942, 2010.
[24] C. Zhang and T. Zhang. A general theory of concave regularization for high-dimensional sparse estimation problems. Statistical Science, 27(4):576-593, 2012.
[25] T. Zhang. Analysis of multi-stage convex relaxation for sparse regularization. JMLR, 11:1081-1107, 2010.
[26] T. Zhang. Multi-stage convex relaxation for feature selection. Bernoulli, 2012.
[27] H. Zou and R. Li. One-step sparse estimates in nonconcave penalized likelihood models. Annals of Statistics, 36(4):1509, 2008.
9

