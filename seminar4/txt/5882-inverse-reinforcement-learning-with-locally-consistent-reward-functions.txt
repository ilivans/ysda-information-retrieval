Inverse Reinforcement Learning with Locally Consistent Reward Functions
Quoc Phong Nguyen, Kian Hsiang Low, and Patrick Jaillet Dept. of Computer Science, National University of Singapore, Republic of Singapore Dept. of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, USA
{qphong,lowkh}@comp.nus.edu.sg, jaillet@mit.edu
Abstract
Existing inverse reinforcement learning (IRL) algorithms have assumed each expert's demonstrated trajectory to be produced by only a single reward function. This paper presents a novel generalization of the IRL problem that allows each trajectory to be generated by multiple locally consistent reward functions, hence catering to more realistic and complex experts' behaviors. Solving our generalized IRL problem thus involves not only learning these reward functions but also the stochastic transitions between them at any state (including unvisited states). By representing our IRL problem with a probabilistic graphical model, an expectation-maximization (EM) algorithm can be devised to iteratively learn the different reward functions and the stochastic transitions between them in order to jointly improve the likelihood of the expert's demonstrated trajectories. As a result, the most likely partition of a trajectory into segments that are generated from different locally consistent reward functions selected by EM can be derived. Empirical evaluation on synthetic and real-world datasets shows that our IRL algorithm outperforms the state-of-the-art EM clustering with maximum likelihood IRL, which is, interestingly, a reduced variant of our approach.
1 Introduction
The reinforcement learning problem in Markov decision processes (MDPs) involves an agent using its observed rewards to learn an optimal policy that maximizes its expected total reward for a given task. However, such observed rewards or the reward function defining them are often not available nor known in many real-world tasks. The agent can therefore learn its reward function from an expert associated with the given task by observing the expert's behavior or demonstration, and this approach constitutes the inverse reinforcement learning (IRL) problem. Unfortunately, the IRL problem is ill-posed because infinitely many reward functions are consistent with the expert's observed behavior. To resolve this issue, existing IRL algorithms have proposed alternative choices of the agent's reward function that minimize different dissimilarity measures defined using various forms of abstractions of the agent's generated optimal behavior vs. the expert's observed behavior, as briefly discussed below (see [17] for a detailed review): (a) The projection algorithm [1] selects a reward function that minimizes the squared Euclidean distance between the feature expectations obtained by following the agent's generated optimal policy and the empirical feature expectations observed from the expert's demonstrated state-action trajectories; (b) the multiplicative weights algorithm for apprentice learning [24] adopts a robust minimax approach to deriving the agent's behavior, which is guaranteed to perform no worse than the expert and is equivalent to choosing a reward function that minimizes the difference between the expected average reward under the agent's generated optimal policy and the expert's empirical average reward approximated using the agent's reward weights; (c) the linear programming apprentice learning algorithm [23] picks its reward function by minimizing the same dissimilarity measure but incurs much less time empirically; (d) the policy matching algorithm [16] aims to match the agent's generated optimal behavior to the expert's observed behavior by choosing a reward function that minimizes the sum of
1

squared Euclidean distances between the agent's generated optimal policy and the expert's estimated policy (i.e., from its demonstrated trajectories) over every possible state weighted by its empirical state visitation frequency; (e) the maximum entropy IRL [27] and maximum likelihood IRL (MLIRL) [2] algorithms select reward functions that minimize an empirical approximation of the KullbackLeibler divergence between the distributions of the agent's and expert's generated state-action trajectories, which is equivalent to maximizing the average log-likelihood of the expert's demonstrated trajectories. The log-likelihood formulations of the maximum entropy IRL and MLIRL algorithms differ in the use of smoothing at the trajectory and action levels, respectively. As a result, the former's log-likelihood or dissimilarity measure does not utilize the agent's generated optimal policy, which is consequently questioned by [17] as to whether it is considered an IRL algorithm. Bayesian IRL [21] extends IRL to the Bayesian setting by maintaining a distribution over all possible reward functions and updating it using Bayes rule given the expert's demonstrated trajectories. The work of [5] extends the projection algorithm [1] to handle partially observable environments given the expert's policy (i.e., represented as a finite state controller) or observation-action trajectories. All the IRL algorithms described above have assumed that the expert's demonstrated trajectories are only generated by a single reward function. To relax this restrictive assumption, the recent works of [2, 6] have, respectively, generalized MLIRL (combining it with expectation-maximization (EM) clustering) and Bayesian IRL (integrating it with a Dirichlet process mixture model) to handle trajectories generated by multiple reward functions (e.g., due to many intentions) in observable environments. But, each trajectory is assumed to be produced by a single reward function. In this paper, we propose a new generalization of the IRL problem in observable environments, which is inspired by an open question posed in the seminal works of IRL [19, 22]: If behavior is strongly inconsistent with optimality, can we identify "locally consistent" reward functions for specific regions in state space? Such a question implies that no single reward function is globally consistent with the expert's behavior, hence invalidating the use of all the above-mentioned IRL algorithms. More importantly, multiple reward functions may be locally consistent with the expert's behavior in different segments along its state-action trajectory and the expert has to switch/transition between these locally consistent reward functions during its demonstration. This can be observed in the following real-world example [26] where every possible intention of the expert is uniquely represented by a different reward function: A driver intends to take the highway to a food center for lunch. An electronic toll coming into effect on the highway may change his intention to switch to another route. Learning of the driver's intentions to use different routes and his transitions between them allows the transport authority to analyze, understand, and predict the traffic route patterns and behavior for regulating the toll collection. This example, among others (e.g., commuters' intentions to use different transport modes, tourists' intentions to visit different attractions, Section 4), motivate the practical need to formalize and solve our proposed generalized IRL problem. This paper presents a novel generalization of the IRL problem that, in particular, allows each expert's state-action trajectory to be generated by multiple locally consistent reward functions, hence catering to more realistic and complex experts' behaviors than that afforded by existing variants of the IRL problem (which all assume that each trajectory is produced by a single reward function) discussed earlier. At first glance, one may straightaway perceive our generalization as an IRL problem in a partially observable environment by representing the choice of locally consistent reward function in a segment as a latent state component. However, the observation model cannot be easily specified nor learned from the expert's state-action trajectories, which invalidates the use of IRL for POMDP [5]. Instead, we develop a probabilistic graphical model for representing our generalized IRL problem (Section 2), from which an EM algorithm can be devised to iteratively select the locally consistent reward functions as well as learn the stochastic transitions between them in order to jointly improve the likelihood of the expert's demonstrated trajectories (Section 3). As a result, the most likely partition of an expert's demonstrated trajectory into segments that are generated from different locally consistent reward functions selected by EM can be derived (Section 3), thus enabling practitioners to identify states in which the expert transitions between locally consistent reward functions and investigate the resulting causes. To extend such a partitioning to work for trajectories traversing through any (possibly unvisited) region of the state space, we propose using a generalized linear model to represent and predict the stochastic transitions between reward functions at any state (i.e., including states not visited in the expert's demonstrated trajectories) by exploiting features that influence these transitions (Section 2). Finally, our proposed IRL algorithm is empirically evaluated using both synthetic and real-world datasets (Section 4).
2

2 Problem Formulation

A Markov decision process (MDP) for an agent is defined as a tuple (S, A, t, r, ) consisting of a finite set S of its possible states such that each state s 2 S is associated with a column vector s of realized feature measurements, a finite set A of its possible actions, a state transition function t : S  A  S ! [0, 1] denoting the probability t(s, a, s0) , P (s0|s, a) of moving to state s0 by performing action a in state s, a reward function r : S ! R mapping each state s 2 S to its reward r(s) , > s where  is a column vector of reward weights, and constant factor
2 (0, 1) discounting its future rewards. When  is known, the agent can compute its policy
 : S  A ! [0, 1] specifying the probability (s, a) , P (a|s, r) of performing action a in state s. However,  is not known in IRL and to be learned from an expert (Section 3).

LtioetnRchdoesnenotaerbaifitrnairtielysefrtoomf lRocpalrliyorctoonlseisatrennint gre.wDaerfidnfeunacttriaonnssitoiofnthfeunacgteionnt an!d:rRe beSa

reward R !

func[0, 1]

for switching between these reward functions as the probability !(r, s, r0 ) , P (r0 |s, r, !)

of switching from reward function r to reward function r0 in state s where the set ! ,

{!rr0 }r r0 2 R \

2{Rre,r}0i2fRth\e{rfee}atcuornetsaiinnsfluceonlucminng

vectors of transition weights !rr0 for all r 2 R the stochastic transitions between reward functions

and can

be additionally observed by the agent during the expert's demonstration, and ! , ; otherwise.

In our generalized IRL problem, ! is not known and to be learned from the expert (Section 3).

Specifically, in the former 
!(r, s, r0 ) ,

case, we propose using a generalized linear model

exp(!r>Pr0 's)/(1 + 1/(1 + r2R\{re}

P expr(!2Rr>\r{r'e}s

exp(!r> ))

r

's

))

to represent ! if r0 6= re, otherwise;

:

(1)

where 's is a column vector of random feature measurements influencing the stochastic transitions

between reward functions (i.e., !) in state s.

Remark 1. Different from s whose feature measurements are typically assumed in IRL algorithms to be realized/known to the agent for all s 2 S and remain static over time, the feature measurements

of 's are, in practice, often not known to the agent a priori and can only be observed when the expert (agent) visits the corresponding state s 2 S during its demonstration (execution), and may vary

over time according to some unknown distribution, as motivated by the real-world examples given

in Section 1. Without prior observation of the feature measurements of 's for all s 2 S (or knowledge of their distributions) necessary for computing ! (1), the agent cannot consider exploiting ! for switching between reward functions within MDP or POMDP planning, even after learning its

weights !; this eliminates the possibility of reducing our generalized IRL problem to an equivalent

conventional IRL problem (Section 1) with only a single reward function (i.e., comprising a mixture

of locally consistent reward functions). Furthermore, the observation model cannot be easily speci-

fied nor learned from the expert's trajectories of states, actions, and 's, which invalidates the use of IRL for POMDP [5]. Instead of exploiting ! within planning, during the agent's execution, when it visits some state s and observes the feature measurements of 's, it can then use and compute ! for state s to switch between reward functions, each of which has generated a separate MDP policy

prior to execution, as illustrated in a simple example in Fig. 1 below. !(r, s, r) !(r0 , s, r0 )

Remark 2. Using a generalized linear model to represent ! (1) allows learning of the stochastic transitions between reward functions (specifically, by learning ! (Section 3)) to be generalized across dif-

!(r, s, r0 )
r r0

ferent states. After learning, (1) can then be exploited for predicting the stochastic transitions between reward functions at any state (i.e., including states not visited in the expert's demonstrated state-action trajectories). Consequently, the agent can choose to traverse a trajectory through any region (i.e., possibly not visited by the expert) of the state space during its execution and the most likely partition of its trajectory into segments that are generated from different locally consistent reward functions selected by EM can still be derived (Section 3). In contrast, if the feature measurements of 's cannot be observed by

!(r0 , s, r)
Figure 1: Transition function ! of an agent in state s for switching between two reward functions r and r0 with their respective policies  and 0 generated prior to execution.

the agent during the expert's demonstration (i.e., ! = ;, as defined above), then such a generaliza-

tion is not possible; only the transition probabilities of switching between reward functions at states

visited in the expert's demonstrated trajectories can be estimated (Section 3). In practice, since the

number |S| of visited states is expected to be much larger than the length L of any feature vector 's,

3

the number O(|S||R|2) of transition probabilities to be estimated is bigger than |!| = O(L|R|2) in (1). So, observing 's offers a further advantage of reducing the number of parameters to be learned.

Fig. 2 shows the probabilistic graphical model for representing our generalized IRL problem. To describe our

R0n

R1n

R2n

***

RTnn

model, some notations are necessary: Let N be the num-

ber of the expert's demonstrated trajectories and Tn be the length (i.e., number of time steps) of its n-th trajectory for

An1 An2 * * * AnTn

n= note

1, . . . , N . its reward

fLuentctriotnn,2acRtio, na,nt an2d

A, and state at

stntim2e

S destep t

in be

its n-th random

trajectory, variables

respectively. corresponding

LtoetthReirtn

,reAspnte,catinvde

Sretn-

alizations rtn , ant , and snt where Rtn is a latent variable,

and Ant and Stn are observable variables. Define rn ,

(orfatnl)lTt=ints0

, an , reward

(fauntn)cTtt=nio1n, sa,nadcstinon,s,

(asnntd)Tts=nta1teass

sequences in its n-th

trajectory, respectively. Finally, define r1:N , (rn )Nn=1,

ait1s:Nrew,ard(afunn)cNnt=io1n, asenqduse1n:cNes,,ac(tisonn)Nnse=q1ueanscteusp,laensdosftaatlel

sequences in its N trajectories, respectively.

It can be observed from Fig. 2 that our probabilistic graphical model of the expert's n-th demonstrated trajectory en-

S1n S2n * * * STnn

Figure 2: Probabilistic graphical model

of the expert's n-th demonstrated trajectory encoding its stochastic transi-

tions between reward functions with

solid edges (i.e., !(rtn 1 , snt , rtn ) =

Psta(tretn

t|rsantn,srititnon1s,

!) for with

t = 1, . . dashed

.

, Tn), edges

(i.e., for t

t(snt =

,

ant , snt+1) 1, . . . , Tn

=

P1)(,snta+n1d|spnt o, laintcy)

with dotted edges P (ant |snt , rtn ) for

(i.e., tn (snt , t = 1, . . . , Tn

ant ).

)

=

codes its stochastic transitions between reward functions, state transitions, and policy. Through our

model, the Viterbi algorithm [20] can be applied to derive the most likely partition of the expert's

trajectory into segments that are generated from different locally consistent reward functions se-

lected by EM, as shown in Section 3. Given the state transition function t(*, *, *) and the number

|R| of reward functions, our model allows tractable learning of the unknown parameters using EM

(Section 3), which include the reward weights vector  for all reward functions r 2 R, transition function ! for switching between reward functions, initial state probabilities (s) , P (S1n = s) for all s 2 S, and initial reward function probabilities (r) , P (R0n = r) for all r 2 R.

3 EM Algorithm for Parameter Learning

A straightforward approach to learning the unknown parameters  , (, , {|r 2 R}, !) is to select the value of  that directly maximizes the log-likelihood of the expert's demonstrated trajec-

tories. Computationally, such an approach is prohibitively expensive due to a large joint parameter

space to be searched for the optimal value of . To ease this computational burden, our key idea is

to devise an EM algorithm that iteratively refines the estimate for  to improve the expected log-

likelihood instead, which is guaranteed to improve the original log-likelihood by at least as much:

Expectation

(E)

step.

Q(, i)

,

P
r1:N

P (r1:N |s1:N , a1:N , i) log P (r1:N , s1:N , a1:N |).

Maximization (M) step. i+1 = argmax Q(, i)

where i denotes an estimate for  at iteration i. The Q function of EM can be reduced to the

fQo(l+++low,PPPiniNnNnNn)g====s111uPPPmPTtTtTto=Nn==nnnf=111fi1P1Pvlleoorrggte,2rtrR((m0ss2sPntn1R,,)(aaR+Psnt s(,PtnhRsont=Nn+wtn=1nr)11iP.n=|sAnrr,p2ap,Rnse,nntPd, Ri(ix)RlAtno0ng:==r(rs0|nt|ss,nna,,ntaa)nn,,ii))loglog(r!

) (r

,

snt

,

r0

)

(2) (3) (4) (5)

Interestingly, each of the first four terms in (2), (3), and (4) contains a unique unknown parameter

type (respectively, , , {|r 2 R}, and !) and can therefore be maximized separately in the M step to be discussed below. As a result, the parameter space to be searched can be greatly re-

duced. Note that the third term (3) generalizes the log-likelihood in MLIRL [2] (i.e., assuming all

trajectories to be produced by a single reward function) to that allowing each expert's trajectory to

be generated by multiple locally consistent reward functions. The last term (5), which contains the

known state transition function t, is independent of unknown parameters .1

1If the state transition function is unknown, then it can be learned by optimizing the last term (5).

4

Lu0bs(eoesat)hrthen=eriwnm(gi1see/itn.NhioSt)diianPlocfesNntL=baat1cegIar1pnnarnbfogoebercamaoblmliullispttiuip2etlesi.deSrTdswoirwhemicettrahlexytiIhmf1nreoizcimseonatthhnsteerianefiixdnrpistcteaPrttteo'rssrm2dvSeaimnri(aotshbn)elset=Qroaft1feuvdtanotlcrutoaeijboe1tncatii(onf2r)sitehn1osef=ienEsMOtsi,m,(aNwantdee) time, it does not have to be refined.

LEMea,rwnienugtiinliizteiatlhreemweatrhdodif+uo1nf(crLtiaoig)nr=apnrg(o1eb/maNbu)illtPiitpielNnise=.r1Ts Pow(mitRhaxt0nhime=iczorents|hsterna,sienactnoP,ndri)t2erRm

in Q (r )

function (2) of = 1 to derive
(6)

for all ri 2 R where

iteration i, time using

aanpdroPce(Rdurtne

i+1 denotes = r|sn, an inspired by

,Banauie)mst(i-imnWatehtleicshfocaarlsgeo,arttiti=htemr0a)[t3ico]an,naisb+esh1co,owminpdiuentneAodtpeipsneaOnnd(eiPxstiBNnm=. a1te|Rfo|r2

 at Tn)

Learning reward functions. The third term in the Q function (3) of EM is maximized using

gradient ascent and its gradient g1() with respect to  is derived to be

g1()

,

XN
n=1

XTn
t=1

P

(Rtn = r|sn, an, (snt , ant )

i)

d(snt , d

ant )

(7)

for all  2 {0|r0 2 R}. For (snt , ant ) to be differentiable in , we define the Q function

oth>faMt sD+P(su, saPi)nsg,02aSnetxo(pps(e, raaQ,tso0r()sth, aaat)0)bQ/lePn(dsas00,2tahA0e)eQwxph(evreaQlue(assQ,vai0a()s)B, aios)ltdz,emfianPnenda2eaAxspQalorB(aostil,otazn)m[a2n]:nQe(xsp,(laso,)raas)tuioc,hn policy, and > 0 is a temperature parameter. Then, we update i+1 i + g1(i) where is the learning step size. We use backtracking line search method to improve the performance of gradient

ascent. Similar to MLIRL, the time incurred in each iteration of gradient ascent depends mostly on

that of value iteration, which increases with the size of the MDP's state and action space.

Learning transition function for switching between reward functions. To maximize the fourth

term in the Q function (4) of EM, if the feature measurements of 's cannot be observed by the agent dwuirthintgh!etih+ce1o(enrxsptir,easrit,n'rstsd0Pie)mr=o0n2(sPRtraNnt!=io(1nrP(,is.Tte=,n.r,1!0 )n=,=t,r;1)i,,fsot,hrrea0nill)w/re(Pu2triRliiz2eaRntdhPesmNn2=e1tShPotodTt=oonb1ftLanian,gt,rranig,se,rmi u)ltipli(e8rs)

for ri , r0i 2 R and s 2 S where S is the set of states visited by the expert, !i+1 is an estimate

for ! at iteration i + 1, and n,t,ri ,s,ri , P (Rtn 1 = r, Stn = s, Rtn = r|sn, an, i) can be

computed efficiently by exploiting the intermediate described previously, as detailed in Appendix B.

results

from

evaluating

P (Rtn

=

r|sn, an, i)

On the other hand, if the feature measurements of 's can be observed by the agent during the expert's

demonstration, then recall that we use a generalized linear model to represent ! (1) (Section 2) and

! is the unknown parameter to be estimated. Similar to learning the reward weights vector  for

reward function r, we maximize the fourth term (4) in the Q function of EM by using gradient

ascent and its gradient g2(!rr0 ) with respect to !rr0 is derived to be

g2 (!r r0

)

,

XN
n=1

XTn
t=1

X
r2R

n,t,r ,snt ,r
!(r, snt , r)

d!(r, snt , d!r r0

r)

(9)

for all !rr0

2

!.

Let

!i
r r0

denote an estimate for !rr0

at iteration i.

Then, it is updated

umseinthgo!d rii+sr1a0lso

us!erid rto0

i+mprgo2v(e!rithre0p)ewrfhoermreancies

the learning step size. Backtracking line search of gradient ascent here. In both cases, the time

incurred in each O(PNn=1 |R|2|S

iteration i |Tn) time.

is

proportional

to

the

number

of

n,t,ri ,s,ri to be computed, which is

Viterbi algorithm for partitioning a trajectory into segments with different locally consistent

reward functions. Given the final estimate rameters  produced by EM, the most likely

pbart=itio(nbo, fbt,h{eb|erxbpe2rt'sRn}-,th!bd)emfoorntshterautendkntroawjenctopray-

into segments generated by different locally consistent reward functions is rn = (rtn )Tt=n0 ,

aVrigtemrbaixarlgnoPrit(hrmn[|2sn0],.anS,pbec)ifi=callay,rgdmefianxervnrbP,T(rfonr,

sn, an|b), T = 1, . . .

which , Tn as

can be derived using the the probability of the most

5

likely reward function sequence (rtn )Tt=01 from time steps 0 to T 1 ending with reward function

rb at time step T that produce state and action sequences (snt )Tt=1 and (ant )Tt=1:

vrb,T

, =

mt(sanTx(r1,tna)TtnT=011P, s(nT(r)tnb)(Tt=snT01,,aRnT)Tnm=axrrb,0

(snt )Tt=1, (ant )Tt=1|b) vrb0 ,T 1 !b (rb0 , snT ,

rb)

,

TTsahvm=ernbe,,11w,r,.a.0ny.mt,=oTanpxaarrrgt0ni1mt,Pioaa(nxnrrdab0nn0r,baTnR(gnreb1nn=0t)='sar!tbrgr(amr,jesbac0n1x,t,osrarn1byn1,v|rrtbrba1,n)Tv)n=e, r.rsTbinT(nhgsen1=ta)hbraoorbvug(egsmhVn1 a,iatxanern1rybb)0rimevagrlagibox0o,nrTrbi0(thib.!bme((.r,rcbpba00on),ssbsnT!bieb+(alr1ypb,p0nr,loiseTntn1d+v,1iirns)bi)ttfheo.der

by the expert) of the state space during its execution in O(|R|2T ) time.

4 Experiments and Discussion

This section evaluates the empirical performance of our IRL algorithm using 3 datasets featuring

experts' demonstrated trajectories in two simulated grid worlds and real-world taxi trajectories. The

average log-likelihood of the expert's demonstrated trajectories is used as the performance metric

because it inherently accounts for the fidelity of our IRL algorithm in learning the locally consistent

reward where

Nfutnotctiisonthse(it.oet.a, lRn)uamndbLetrh(eo)fst,tohceh(1ae/sxtNpicetortttr)'asPnsdiNnet=imtoot1nolsnosbgterPatwt(esednen,traathnjee|cmt)o(rii.ees.,

! ): available

in

the

(10) dataset.

As proven in [17], maximizing L() with respect to  is equivalent to minimizing an empirical ap-

proximation of the Kullback-Leibler divergence between the distributions of the agent's and expert's

generated state-action trajectories. Note that when the final estimate b produced by EM (Section 3)

is plugged into (10), the resulting P (sn, an|b) in (10) can be computed efficiently using a procedure

similar to that in Section 3, as detailed in Appendix C. To avoid local maxima in gradient ascent,

we initialize our EM algorithm with 20 random 0 values and report the best result based on the Q

value of EM (Section 3).

01 234

01 234

To demonstrate the importance of modeling and learning stochastic transitions between locally consistent reward functions, the performance of our IRL algorithm is compared with that of its reduced variant assuming no change/switching of reward function within each trajectory, which is implemented by initializing !(r, s, r) = 1 for all r 2 R and s 2 S and deactivating the learning of !. In fact, it can be shown (Appendix D) that such a reduction, interestingly, is equivalent to EM clustering with MLIRL [2]. So, our IRL algorithm generalizes EM clustering with MLIRL, the latter of which has been empirically demonstrated in [2] to outperform many existing IRL algorithms, as discussed in Section 1.

0 0O

11

2O 3

D2 3

4 4D
AB Figure 3: Grid worlds A (states (0, 0), (1, 1), and (2, 2) are, respectively, examples of water, land, and obstacle), and B (state (2, 2) is an example of barrier). `O' and `D' denote origin and destination.

Simulated grid world A. The environment (Fig. 3) is modeled as a 5  5 grid of states, each of which is either land, water, water and destination, or obstacle associated with the respective feature vectors (i.e., s) (0, 1, 0)>, (1, 0, 0)>, (1, 0, 1)>, and (0, 0, 0)>. The expert starts at origin (0, 2) and any of its actions can achieve the desired state with 0.85 probability. It has two possible reward functions, one of which prefers land to water and going to destination (i.e.,  = (0, 20, 30)>), and the other of which prefers water to land and going to destination (i.e., 0 = (20, 0, 30)>). The expert will only consider switching its reward function at states (2, 0) and (2, 4) from r0 to r with 0.5 probability and from r to r0 with 0.7 probability; its reward function remains unchanged at all other states. The feature measurements of 's cannot be observed by the agent during the expert's demonstration. So, ! = ; and ! is estimated using (8). We set to 0.95 and the number |R| of reward functions of the agent to 2.

Fig. 4a shows results of the average log-likelihood L (10) achieved by our IRL algorithm, EM clustering with MLIRL, and the expert averaged over 4 random instances with varying number N of expert's demonstrated trajectories. It can be observed that our IRL algorithm significantly outperforms EM clustering with MLIRL and achieves a L performance close to that of the expert, especially when N increases. This can be explained by its modeling of ! and its high fidelity in learning and predicting !: While our IRL algorithm allows switching of reward function within each trajectory, EM clustering with MLIRL does not.

6

Average log-likelihood Average log-likelihood

We also observe that the accuracy of estimating the transition probabilities !(r, s, .) (!(r0 , s, .)) using (8) depends on the frequency and distribution of trajectories demon-

-15 -17 -19 -21

-22 -23 -24 -25 -26 -27

strated by the expert with its reward

faut nticmtieonsteRpttn at time step

t1,1=wanhrdiciht(sRissttanete1xps=ent c=rted0s).

Those transition probabilities that

are poorly estimated due to few rel-

evant expert's demonstrated trajec-

tories, however, do not hurt the L

-23

Our IRL algorithm

-28 Our IRL algorithm

-25

EM clustering with MLIRL Expert

-29

EM clustering with MLIRL Expert

0 200 400 600 800 1000 1200 1400 1600

0 100 200 300 400 500 600

(a)No. of demonstrated trajectories

(b)No. of demonstrated trajectories

Figure 4: Graphs of average log-likelihood L achieved by our

IRL algorithm, EM clustering with MLIRL, and the expert vs.

number N of expert's demonstrated trajectories in simulated

grid worlds (a) A (Ntot = 1500) and (b) B (Ntot = 500).

performance of our IRL algorithm by much because such trajectories tend to have very low prob-

ability of being demonstrated by the expert. In any case, this issue can be mitigated by using the

generalized linear model (1) to represent ! and observing the feature measurements of 's necessary for learning and computing !, as shown next.

Simulated grid world B. The environment (Fig. 3) is also modeled as a 5  5 grid of states, each

of which is either the origin, destination, or land associated with the respective feature vectors (i.e.

s) (0, 1)>, (1, 0)>, and (0, 0)>. The expert starts at origin (4, 0) and any of its actions can achieve the desired state with 0.85 probability. It has two possible reward functions, one of which prefers

going to destination (i.e.,  = (30, 0)>), and the other of which prefers returning to origin (i.e., 0 =

(0, 30)>). While moving to the destination, the expert will encounter barriers at some states with

corresponding feature vectors 's = (1, 1)> and no barriers at all other states with 's = (0, 1)>; the

second component of 's is used as an offset value in the generalized linear model (1). The expert's

behavior of switching between reward functions is governed by a generalized linear model ! (1)

wit iwthilrl,e

= for

rex0aamnpdlet,racnosnitsiiodnerwsewigithcthsi!ngrirts

= ( 11, 12)> and reward function at

!strat0ers

= (13, 12)>. As a with barriers from r

result, to r0

with 0.269 probability. We estimate ! using (9) and set to 0.95 and the number |R| of reward

functions of the agent to 2. To assess the fidelity of learning and predicting the stochastic transitions

between reward functions at unvisited states, we intentionally remove all demonstrated trajectories

that visit state (2, 0) with a barrier.

Fig. 4b shows results of L (10) performance achieved by our IRL algorithm, EM clustering with MLIRL, and the expert averaged over 4 random instances with varying N . It can again be observed that our IRL algorithm outperforms EM clustering with MLIRL and achieves an L performance comparable to that of the expert due to its modeling of ! and its high fidelity in learning and predicting !: While our IRL algorithm allows switching of reward function within each trajectory, EM clustering with MLIRL does not. Besides, the estimated transition function !b using (9) is very close to that of the expert, even at unvisited state (2, 0). So, unlike using (8), the learning of ! with (9) can be generalized well across different states, thus allowing ! to be predicted accurately at any state. Hence, we will model ! with (1) and learn it using (9) in the next experiment. Real-world taxi trajectories. The Comfort taxi company in Singapore has provided GPS traces of 59 taxis with the same origin and destination that are map-matched [18] onto a network (i.e., comprising highway, arterials, slip roads, etc) of 193 road segments (i.e., states). Each road segment/state is specified by a 7-dimensional feature vector s: Each of the first six components of s is an indicator describing whether it belongs to Alexandra Road (AR), Ayer Rajah Expressway (AYE), Depot Road (DR), Henderson Road (HR), Jalan Bukit Merah (JBM), or Lower Delta Road (LDR), while the last component of s is the normalized shortest path distance from the road segment to destination. We assume that the 59 map-matched trajectories are demonstrated by taxi drivers with a common set R of 2 reward functions and the same transition function ! (1) for switching between reward functions, the latter of which is influenced by the normalized taxi speed constituting the first component of 2-dimensional feature vector 's; the second component of 's is used as an offset of value 1 in the generalized linear model (1). The number |R| of reward functions is set to 2 because when we experiment with |R| = 3, two of the learned reward functions are similar. Every driver can deterministically move its taxi from its current road segment to the desired adjacent road segment.

7

Fig. 5a shows results of L (10) performance achieved by our IRL algorithm and EM clustering with MLIRL averaged over 3 random instances with varying N . Our IRL algorithm outperforms EM clustering with MLIRL due to its modeling of ! and its high fidelity in learning and predicting !. To see this, our IRL algorithm is able to learn that a taxi driver is likely to switch between reward functions representing different in-

-4 1 b!(rb, s, rb0 )
-4.5 b!(rb0 , s, rb0 ) 0.8
-5

Average log-likelihood
Probability

-5.5 0.6
-6
0.4
-6.5

-7

0.2

-7.5

Our IRL algorithm EM clustering with MLIRL

-8 10

20 30 40 50 60
(a)No. of demonstrated trajectories

0 0 0.2 0.4 0.6 0.8 1
(b)Normalized taxi speed

Figure 5: Graphs of (a) average log-likelihood L achieved by

our IRL algorithm and EM clustering with MLIRL vs. no. N

of taxi trajectories (Ntot = 59) and (b) transition probabilities

of switching between reward functions vs. taxi speed.

tentions directly

within its demonstrated trajectory: to the destination (Fig. 6a) due to

aRheuwgaerdpefnuanlctytio(ni.er.,b

denotes his intention of driving reward weight -49) on being far

from destination and a large reward (i.e., reward weight 35.7) for taking the shortest path from ori-

g(Finigt.o6dbe) sdtiuneattioonla,rwgehircehwaisrdvsiafoJrBtMrav, ewlihnigleornb0thdeemno(treesspheisctiivnetelyn,tiroenwoafrddweteoiugrhintsg

to DR or JBM 30.5 and 23.7).

As an example, Fig. 6c shows the most likely partition of a demonstrated trajectory into segments

gaAalleYgononEergr,iatsDhtwemRdit,c(fHShroeeRmsc,tfirlaooonncmda3lrJ)lB.yb0MIcttoocntraosbnisdutbepeesnottinonbrtaeustweironravnirne.dgdOfitunhnntaocttthAiteohRneosttdhorreibdvreeahtrnoadiunsrdri,tbno0ti,htDiewaRlhrl,eyiawcnihandridrsrebdf0mueonarniicvntetishodiennussrsllibienpawgrrnhooeiauldder

Viterbi exiting driving by EM

clustering with MLIRL are both associated with his intention of driving directly to destination (i.e.,

similar to rb); it is not able to learn his intention of detouring to DR or JBM.

Fig. 5b shows the influence of normalized taxi

speed (i.e., first component of 's) on the estimated transition function !b using (9). It can be observed that when the driver is in

rvebr(yi.eu.n, ldikreivlyintgo dcihreacntglye

to destination), he is his intention regard-

O

AR

AYE DR

JBM

(a)

HR

LDR

D

less of taxi speed. detouring to DR

But, when he is or JBM), he is

liinkerlby0

(i.e., (un-

O

AR

JBM

likely) to remain in this intention if taxi speed

is low (high). The demonstrated trajectory in

Fig. 6c in fact supports this observation: The

driver initially remains slip road exiting AYE,

winhircbh0

on the causes

upslope the low

taxi speed. Upon turning into AR to detour to

O

AR

AYE DR
(b)
JBM

HR D
LDR

DdrRiv,ehaetsrweliatctihveeslyfrhoimghrsbp0 etoedrbonbeflcaatutseerrhaeinc.an
5 Conclusion
This paper describes an EM-based IRL algorithm that can learn the multiple reward functions being locally consistent in different segments along a trajectory as well as the stochastic transitions between them. It generalizes EM-clustering with MLIRL and has been empirically demonstrated to outperform it on both synthetic and real-world datasets.

AYE

DR
(c)

HR D
LDR

Figure 6: Reward (a) rb(s) and (b) rb0 (s) for each road segment s with b = (7.4, 3.9, 16.3, 20.3, 35.7, 21.5, 49.0)> and b0 = (5.2, 9.2, 30.5, 15.0, 23.7, 21.5, 9.2)> such that more red road segments give higher rewards. (c) Most likely partition of a demonstrated trajectory from origin `O' to destination `D' into red and green segments generated by rb and rb0 , respectively.

For our future work, we plan to extend our IRL algorithm to cater to an unknown number of reward

functions [6], nonlinear reward functions [12] modeled by Gaussian processes [4, 8, 13, 14, 15, 25],

other dissimilarity measures described in Section 1, linearly-solvable MDPs [7], active learning with

Gaussian processes [11], and interactions with self-interested agents [9, 10].

Acknowledgments. This work was partially supported by Singapore-MIT Alliance for Research and Technology Subaward Agreement No. 52 R-252-000-550-592.

8

References
[1] P. Abbeel and A. Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc. ICML, 2004.
[2] M. Babes-Vroman, V. Marivate, K. Subramanian, and M. Littman. Apprenticeship learning about multiple intentions. In Proc. ICML, pages 897-904, 2011.
[3] J. Bilmes. A gentle tutorial of the EM algorithm and its application to parameter estimation for Gaussian mixture and Hidden Markov models. Technical Report ICSI-TR-97-02, University of California, Berkeley, 1998.
[4] J. Chen, N. Cao, K. H. Low, R. Ouyang, C. K.-Y. Tan, and P. Jaillet. Parallel Gaussian process regression with low-rank covariance matrix approximations. In Proc. UAI, pages 152-161, 2013.
[5] J. Choi and K. Kim. Inverse reinforcement learning in partially observable environments. JMLR, 12:691- 730, 2011.
[6] J. Choi and K. Kim. Nonparametric Bayesian inverse reinforcement learning for multiple reward functions. In Proc. NIPS, pages 314-322, 2012.
[7] K. Dvijotham and E. Todorov. Inverse optimal control with linearly-solvable MDPs. In Proc. ICML, pages 335-342, 2010.
[8] T. N. Hoang, Q. M. Hoang, and K. H. Low. A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data. In Proc. ICML, pages 569-578, 2015.
[9] T. N. Hoang and K. H. Low. A general framework for interacting Bayes-optimally with self-interested agents using arbitrary parametric model and model prior. In Proc. IJCAI, pages 1394-1400, 2013.
[10] T. N. Hoang and K. H. Low. Interactive POMDP Lite: Towards practical planning to predict and exploit intentions for interacting with self-interested agents. In Proc. IJCAI, pages 2298-2305, 2013.
[11] T. N. Hoang, K. H. Low, P. Jaillet, and M. Kankanhalli. Nonmyopic -Bayes-optimal active learning of Gaussian processes. In Proc. ICML, pages 739-747, 2014.
[12] S. Levine, Z. Popovic, and V. Koltun. Nonlinear inverse reinforcement learning with Gaussian processes. In Proc. NIPS, pages 19-27, 2011.
[13] K. H. Low, J. Chen, T. N. Hoang, N. Xu, and P. Jaillet. Recent advances in scaling up Gaussian process predictive models for large spatiotemporal data. In S. Ravela and A. Sandu, editors, Proc. Dynamic Data-driven Environmental Systems Science Conference (DyDESS'14). LNCS 8964, Springer, 2015.
[14] K. H. Low, N. Xu, J. Chen, K. K. Lim, and E. B. O zgul. Generalized online sparse Gaussian processes with application to persistent mobile robot localization. In Proc. ECML/PKDD Nectar Track, 2014.
[15] K. H. Low, J. Yu, J. Chen, and P. Jaillet. Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation. In Proc. AAAI, pages 2821-2827, 2015.
[16] G. Neu and C. Szepesvari. Apprenticeship learning using inverse reinforcement learning and gradient methods. In Proc. UAI, pages 295-302, 2007.
[17] G. Neu and C. Szepesvari. Training parsers by inverse reinforcement learning. Machine Learning, 77(2- 3):303-337, 2009.
[18] P. Newson and J. Krumm. Hidden Markov map matching through noise and sparseness. In Proc. 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 336-343, 2009.
[19] A. Y. Ng and S. Russell. Algorithms for inverse reinforcement learning. In Proc. ICML, 2000. [20] L. R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc.
IEEE, 77(2):257-286, 1989. [21] D. Ramachandran and E. Amir. Bayesian inverse reinforcement learning. In Proc. IJCAI, pages 2586-
2591, 2007. [22] S. Russell. Learning agents for uncertain environments. In Proc. COLT, pages 101-103, 1998. [23] U. Syed, M. Bowling, and R. E. Schapire. Apprenticeship learning using linear programming. In Proc.
ICML, pages 1032-1039, 2008. [24] U. Syed and R. E. Schapire. A game-theoretic approach to apprenticeship learning. In Proc. NIPS, pages
1449-1456, 2007. [25] N. Xu, K. H. Low, J. Chen, K. K. Lim, and E. B. O zgul. GP-Localize: Persistent mobile robot localization
using online sparse Gaussian process observation model. In Proc. AAAI, pages 2585-2592, 2014. [26] J. Yu, K. H. Low, A. Oran, and P. Jaillet. Hierarchical Bayesian nonparametric approach to modeling and
learning the wisdom of crowds of urban traffic route planning agents. In Proc. IAT, pages 478-485, 2012. [27] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning.
In Proc. AAAI, pages 1433-1438, 2008.
9

