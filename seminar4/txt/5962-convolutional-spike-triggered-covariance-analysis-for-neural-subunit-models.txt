Convolutional Spike-triggered Covariance Analysis for Neural Subunit Models

Anqi Wu1

Il Memming Park2

Jonathan W. Pillow1

1 Princeton Neuroscience Institute, Princeton University {anqiw, pillow}@princeton.edu
2 Department of Neurobiology and Behavior, Stony Brook University memming.park@stonybrook.edu

Abstract

Subunit models provide a powerful yet parsimonious description of neural responses to complex stimuli. They are defined by a cascade of two linear-nonlinear (LN) stages, with the first stage defined by a linear convolution with one or more filters and common point nonlinearity, and the second by pooling weights and an output nonlinearity. Recent interest in such models has surged due to their biological plausibility and accuracy for characterizing early sensory responses. However, fitting poses a difficult computational challenge due to the expense of evaluating the log-likelihood and the ubiquity of local optima. Here we address this problem by providing a theoretical connection between spike-triggered covariance analysis and nonlinear subunit models. Specifically, we show that a "convolutional" decomposition of a spike-triggered average (STA) and covariance (STC) matrix provides an asymptotically efficient estimator for class of quadratic subunit models. We establish theoretical conditions for identifiability of the subunit and pooling weights, and show that our estimator performs well even in cases of model mismatch. Finally, we analyze neural data from macaque primary visual cortex and show that our moment-based estimator outperforms a highly regularized generalized quadratic model (GQM), and achieves nearly the same prediction performance as the full maximum-likelihood estimator, yet at substantially lower cost.

1 Introduction
A central problem in systems neuroscience is to build flexible and accurate models of the sensory encoding process. Neurons are often characterized as responding to a small number of features in the high-dimensional space of natural stimuli. This motivates the idea of using dimensionality reduction methods to identify the features that affect the neural response [1-9]. However, many neurons in the early visual pathway pool signals from a small population of upstream neurons, each of which integrates and nolinearly transforms the light from a small region of visual space. For such neurons, stimulus selectivity is often not accurately described with a small number of filters [10]. A more accurate description can be obtained by assuming that such neurons pool inputs from an earlier stage of shifted, identical nonlinear "subunits" [11-13]. Recent interest in subunit models has surged due to their biological plausibility and accuracy for characterizing early sensory responses. In the visual system, linear pooling of shifted rectified linear filters was first proposed to describe sensory processing in the cat retina [14, 15], and more recent work has proposed similar models for responses in other early sensory areas [16-18]. Moreover, recent research in machine learning and computer vision has focused on hierarchical stacks of such subunit models, often referred to as Convolutional Neural Networks (CNN) [19-21]. The subunit models we consider here describe neural responses in terms of an LN-LN cascade, that is, a cascade of two linear-nonlinear (LN) processing stages, each of which involves linear projection and a nonlinear transformation. The first LN stage is convolutional, meaning it is formed from one or
1

more banks of identical, spatially shifted subunit

filters, with outputs transformed by a shared sub-

stimulus

unit nonlinearity. The second LN stage consists

of a set of weights for linearly pooling the nonlinear subunits, an output nonlinearity for mapping

subunit filter

1st LN stage

the output into the neuron's response range, and

finally, an noise source for capturing the stochasticity of neural responses (typically assumed to be

subunit nonliearity

Gaussian, Bernoulli or Poisson). Vintch et al pro-

posed one variant of this type of subunit model, and

pooling

W4

showed that it could account parsimoniously for the multi-dimensional input-output properties revealed

weights

W1

W3 W2

W5 W6 W7

by spike-triggered analysis of V1 responses [12, 13].

output

However, fitting such models remains a challeng-

nonlinearity

2nd LN stage

ing problem. Simple LN models with Gaussian or

Poisson

Poisson noise can be fit very efficiently with spike-

spiking

triggered-moment based estimators [6-8], but there

is no equivalent theory for LN-LN or subunit mod-

response

els. This paper aims to fill that gap. We show that a convolutional decomposition of the spike-triggered average (STA) and covariance (STC) provides an asymptotically efficient estimator for a Poisson sub-

Figure 1: Schematic of subunit LN-LNP cascade model. For simplicity, we show only 1 subunit type.

unit model under certain technical conditions: the

stimulus is Gaussian, the subunit nonlinearity is well

described by a second-order polynomial, and the final nonlinearity is exponential. In this case, the

subunit model represents a special case of a canonical Poisson generalized quadratic model (GQM),

which allows us to apply the expected log-likelihood trick [7, 8] to reduce the log-likelihood to a

form involving only the moments of the spike-triggered stimulus distribution. Estimating the subunit

model from these moments, an approach we refer to as convolutional STC, has fixed computational

cost that does not scale with the dataset size after a single pass through the data to compute sufficient

statistics. We also establish theoretical conditions under which the model parameters are identifi-

able. Finally, we show that convolutional STC is robust to modest degrees of model mismatch, and

is nearly as accurate as the full maximum likelihood estimator when applied to neural data from V1

simple and complex cells.

2 Subunit Model

We begin with a general definition of the Poisson convolutional subunit model (Fig. 1). The model is specified by:

subunit outputs: spike rate:
spike count:

smi =fX(kmX* xi)



= g wmi smi

mi

y|  Poiss( ),

(1) (2)
(3)

where km is the filter for the m'th type of subunit, xi is the vectorized stimulus segment in the i'th position of the shifted filter during convolution, and f is the nonlinearity governing subunit outputs. For the second stage, wmi is a linear pooling weight from the m'th subunit at position i, and g is the neuron's output nonlinearity. Spike count y is conditionally Poisson with rate .

Fitting subunit models with arbitrary g and f poses significant computational challenges. However,

if we set g to exponential and f takes the form of second-order polynomial, the model reduces to

X

X



=

exp

1 2

wmi (km * xi)2+ wmi (km * xi) + a

(4)



= exp

1 2

x>C[w,k]x

+

b>[w,k]x

+a

(5)

where

X C[w,k] = Km> diag(wm)Km,
m

X

b[w,k] =

Km> wm ,

m

(6)

2

and Km is a Toeplitz matrix consisting of shifted copies of km satisfying Kmx = [x1, x2, x3, . . .]>km. In essence, these restrictions on the two nonlinearities reduce the subunit model to a (canonicalform) Poisson generalized quadratic model (GQM) [7, 8, 22], that is, a model in which the Poisson spike rate takes the form of an exponentiated quadratic function of the stimulus. We will pursue the implications of this mapping below. We assume that k is a spatial filter vector without time expansion. If we have a spatio-temporal stimulus-response, k should be a spatial-temporal filter, but the subunit convolution (across filter position i) involves only the spatial dimension(s). From (eqs. 4 and 5) it can be seen that the subunit model contains fewer parameters than a full GLM, making it a more parsimonious description for neurons with multi-dimensional stimulus selectivity.

3 Estimators for Subunit Model

With the above definitions and formulations, we now present three estimators for the model parameters {w, k}. To simplify the notation, we omit the subscript in C[w,k] and b[w,k], but their dependence on the model parameters is assumed throughout.

Maximum Log-Likelihood Estimator

The maximum log-likelihood estimator (MLE) has excellent asymptotic properties, though it comes with the high computational cost. The log-likelihood function can be written:

XX

LMLE() =

yi log i

i

(7)

Xi i

X

=

yi(

1 2

x>i Cxi

+

b>xi

+ a) "

exp(

1 2

x>i

C

xi

+

b>xi

+ #

a)

X

= Tr[C] + b> + ansp

exp(

1 2

x>i Cxi

+

b>xi

+

a)

(8) (9)

i

where  = covariance

(PSTi Cyi)xainids

the nsp

s=pikPe-itryiiggiserthede

taovtearlanguem(SbTeAr o)fasnpdikes=. WPeidyeinxoitxe>ithies

the spike-triggered MLE as MLE.

Moment-based Estimator with Expected Log-Likelihood Fitting

If the stimuli are drawn from x  N (0, ), a zero-mean Gaussian with covariance , then the expression in square brackets divided by N in (eq. 9) will converge to its expectation, given by

E

 exp(

1 2

x>i Cxi

+

b>xi

+

 a)

=

|I

C|

1
2 exp

1 2

b>

(

1

C) 1b + a

(10)

Substituting this expectation into (9) yields a quantity called expected log-likelihood, with the objective function as,

LELL() = Tr[C] + b> + ansp N |I

C|

1
2 exp

1 2

b>(

1

C) 1b + a

(11)

where N is the number of time bins. We refer to MELE mum expected log-likelihood estimator) [7, 8, 22].

=

arg max

LELL()

as

the

MELE

(maxi-

Moment-based Estimator with Least Squares Fitting

Maximizing (11) w.r.t {C, b, a} yields analytical expected maximum likelihood estimates [7]:

Cmele =

1



1, bmele = 

1,

amele

=

log(

nsp N

|



1

|

1 2

)

1 2

>

1 1

(12)

With these analytical estimates, it is straightforward and to optimize w and k by directly minimizing squared error:

LLS() = ||Cmele K> diag(w)K||22 + ||bmele K>w||22

(13)

which corresponds to an optimal "convolutional" decomposition of the moment-based estimates. This formulation shows that the eigenvectors of Cmele are spanned by shifted copies of k. We denote this estimate LS.

All three estimators, MLE, MELE and LS should provide consistent estimates for the subunit model parameters due to consistency of ML and MELE estimates. However, the moment-based estimates

3

(MELE and LS) are computationally much simpler, and scale much better to large datasets, due to the fact that they depend on the data only via the spike-triggered moments. In fact their only dependence on the dataset size is the cost of computing the STA and STC in one pass through the data. As for efficiency, LS has the drawback of being sensitive to noise in the Cmele estimate, which has far more free parameters than in the two vectors w and k (for a 1-subunit model). Therefore, accurate estimation of Cmele should be a precondition for good performance of LS, and we expect MELE to perform better for small datasets.
4 Identifiability

The equality C = C[w,k] = K> diag(w)K is a core assumption to bridge the theoretical connection between a subunit model and the spike-triggered moments (STA & STC). In case we care about recovering the underlying biological structure, we maybe interested to know when the solution is unique and naively interpretable. Here we address the identifiability of the convolution decomposition of C for k and w estimation. Specifically, we briefly study the uniqueness of the form C = K> diag(w)K for a single subunit and multiple subunits respectively. We provide the proof for the single subunit case in the main text, and the proof for multiple subunits sharing the same pooling weight w in the supplement. Note that failure of identifiability only indicates that there are possible symmetries in the solution space so that there are multiple equivalent optima, which is a question of theoretical interest, but it holds no implications for practical performance. 4.1 Identifiability for Single Subunit Model

We will frequently make use of frequency domain representation. Let B 2 Rdd denote the discrete

Fourier transform (DFT) matrix with j-th column is,

h bj = 1, e

2 d

(j

1), e

2 d

2(j

1), e

2 d

3(j

1), . . . , e

2 d

(d

1)(j

1)i> .

(14)

Let ke be a d-dimensional vector resulting from a discrete Fourier transform, that is, ke = Bkk where Bk is a d  dk DFT matrix, and similarly we 2 Rd be a Fourier representation of w.
We assume that k and w have full support in the frequency domain. Assumption 1. No element in ke or we is zero. Theorem. Suppose Assumption 1 holds, the convolution decomposition C = K> diag(w)K is uniquely identifiable up to shift and scale, where C 2 Rdd and d = dk + dw 1.

Proof. We fix k (and thus ke) to be a unit vector to deal with the obvious scale invariance. First note that we can rewrite the convolution operator K using DFT matrices as,

K = BH diag(Bkk)Bw

(15)

where B 2 Rdd is the DFT matrix and (*)H denotes conjugate transpose operation. Thus,

C = BH diag(ke)H Bw diag(w)BwH diag(ke)B

Note that Wf := Bw diag(w)BwH is a circulant matrix,

0 we1

wed * * * we3 we21

Wf := circulant(we ) = BBBB@wewed...2 1

we...1 wed 2

*** ... ***

we...4 we1

wwee...d3CCCCA

wed wed 1 * * * we2 we1

Hence, we can rewrite (16) in the frequency domain as,

(16) (17)

Ce = BCBH = diag(ke)H Wf diag(ke) = Wf (kekeH )>

(18)

Since B is invertible, the uniqueness of the original C decomposition is equivalent to the uniqueness of Ce decomposition. The newly defined decomposition is

Ce = Wf (kekeH )>.

(19)

4

Suppose there are two distinct decompositions {Wf, ke} and {Ve , ge}, where both {k, ke} and {g, ge} are unit vectors, such that Ce = Wf (kekeH )> = Ve (gegeH )>. Since both Wf and Ve have no zero, define the element-wise ratio R := (Wf./Ve )> 2 Rdd, then we have

R kekeH = gegeH

(20)

Note that rank(R kekeH ) = rank(gegeH ) = 1.

R is also a circulant matrix We can express R as R =

wPhdich
i=1

can be diagonalized ribibHi . Using the

by DFT identity

[23]: R = B diag (r1, for Hadamard product

..., that

rd)BH . for any

vector a and b, (aaH ) (bbH ) = (a b)(a b)H , we get

R

kekeH

=

Xd ri(bibHi )

(kekeH ) = Xd ri(bi

ke)(bi

ke)H

i=1 i=1

By Lemma 1 (in the appendix), {b1 ke, b2 ke, . . . , bd ke} is a linearly independent set.

(21)

Therefore, to satisfy the rank constraint rank(R kekeH ) = 1, ri can be non-zero at most a single i. Without loss of generality, let ri 6= 0 and all other r* to be zero, then we have,

ri(bibHi ) kekeH = gegeH =) ri diag(bi)kekeH diag(bi)H = gegeH 

(22)

Because bi, ke and ge are unit vectors, ri = 1. By recognizing that diag(bi)ke is the Fourier

transform of i 1 positions shifted k, denoted as ki 1, we have, ki 1(ki 1)> = gg>. Therefore,

g= be a

ki 1. Moreover, shifted version of

from w.

(20)

and

(22),

(bibHi

)

Ve = Wf thus, vi 1 = w. that is, v must also

If restricting k and g to be unit vectors, then any solution v and g would satisfy w = vi 1 and g = ki 1. Therefore, the two decompositions are identical up to scale and shift.

4.2 Identifiability for Multiple Subunits Model
Multiple subunits model (with m > 1 subunits) is far more complicated to analyze due to large degree of hidden invariances. In this study, we only provide the analysis under a specific condition when all subunits share a common pooling weight w. Assumption 2. All models share a common w.

We make a few additional assumptions. We would like to consider a tight parameterization where no combination of subunits can take over another subunit's task. Assumption 3. K := [k1, k2, k3, . . . , km] spans an m-dimensional subspace where ki is the subunit filter for i-th subunit and K 2 Rdkm. In addition, K has orthogonal columns.

We denote K with p positions shifted along the column as that trivially, m  dk < dk + dw 1 < d since dw > 1.

Kp

:=

[kp1, kp2, kp3, . . . , kpm].

Also,

note

To allow arbitrary scale corresponding to each unit vector ki, we introduce coefficient i to the i-th

subunit, thus extending (19) to,

C = Xm Wf (ikeikeHi )> = Wf

Xm

!> ikeikeHi

=

Wf

(Ke AKe H )>

(23)

i=1 i=1

where A 2 Rmm is a diagonal matrix of i and Ke 2 Rdm is the DFT of K. Assumption 4. @ 2 Rmm such that Ki = P Ki, 8i, where P 2 Rdkdk is the permutation matrix from Ki to Kj by shifting rows, namely Kj = P Ki, 8i, j, and  is a linear projection coefficient matrix satisfying Kj = Ki. Assumption 5. A has all positive or all negative values on the diagonal.

Given these assumptions, we establish the proposition for multiple subunits model. Proposition. Under Assumptions (1-5), the convolutional decomposition C = Wf (Ke AKe H )> is uniquely identifiable up to shift and scale.

The proof for the proposition and illustrations of Assumption 4-5 are in the supplement.

5

a) 0.4
0.2 0 0
c)

10 20 30

0.4

0

-0.4 0

10 20

exponential

true parameters MELE smoothMELE
30
output nonlinearity

1.13

0.76

0.7

run time (sec)

b) 7 smoothLS smoothMELE smoothMLE 3.5
0103 104 105
sample size
soft-rectifier
0.6

quadratic

subunit nonlinearity

0.64

0.56

0.14

0.37

0.88

0.74

0.38

0.39

0.07

0.17

0.7 0.71

MSE

sigmoid

0.45

0.39

0.4 0.43

0.01303

104 105 0.05

sample size

0.09

0.14

smoothLS

smoothMELE

smoothMLE

Figure 2: a) True parameters and MELE and smoothMELE estimations. b) Speed performance for smoothLS, smoothMELE and smoothMLE. The slightly decreasing running time along with a larger size is resulted from a more and more fully supported subspace, which makes optimization require fewer iterations. c) Accuracy performance for all combinations of subunit and output nonlinearities for smoothLS, smoothMELE and smoothMLE. Top left is the subunit model matching the data; others are model mismatch.

5 Experiments

5.1 Initialization

All three estimators are non-convex and contain many local optima, thus the selection of model initialization would affect the optimization substantially. Similar to [12] using `convolutional STC' for initialization, we also use a simple moment based method with some assumptions. For simplicity, we assume all subunit models sharing the same w with different scaling factors as in eq. (23). Our initializer is generated from a shallow bilinear regression. Firstly, initialize w with a wide Gaussian profile, then estimate Ke AKe H from element-wise division of Cmele by Wf. Secondly, use SVD to decompose Ke AKe H into an orthogonal base set Ke and a positive diagonal matrix A, where Ke and A contain information about ki's and 's respectively, hypothesizing that k's are orthogonal to each other and 's are all positive (Assumptions 3 and 5). Based on the ki's and i's we estimated from the rough Gaussian profile of w, now we fix those and re-estimate w with the same elementwise division for Wf. This bilinear iterative procedure proceeds only a few times in order to avoid overfitting to Cmele which is a coarse estimate of C.
5.2 Smoothing prior

Neural receptive fields are generally smooth, thus a prior smoothing out high frequency fluctuations

would improve the performance of estimators, unless the data likelihood provides sufficient evidence

for jaggedness. We apply automatic smoothness determination (ASD [24]) to both w and k, each

with an associated balancing hyper parameter w and k. Assuming w  N (0, Cw) with

 Cw = exp w

k k2 

2

2 w

(24)

where and length

is the scale

vector of Cw

of differences that belong to

between neighboring locations in w. w and the hyper parameter set. k also has the same

Aw2SDareprviaorriawnicthe

hyper parameters k and

2 k

.

For

multiple

subunits,

each

wi

and

ki

would

have

its

own

ASD

prior.

6

low-rank, smooth, expected GQM low-rank, smooth GQM smoothLS(#1)
smoothLS(#2)
smoothMELE(#1)
smoothMELE(#2)
smoothMLE(#1)
smoothMLE(#2)

goodness-of-fit (nats/spk)
running time (sec)

performance
0

-1 0

-0.06

-2

-0.12

-0.18
104
-3
104 training size

105
105

speed
250 200 150 100
50
104 training size105

Figure 3: Goodness-of-model fits from various estimators and their running speeds (without GQM comparisons). Black curves are regularized GQM (with and without expected log-likelihood trick); blue is smooth LS; green is smooth MELE; red is smooth MLE. All the subunit estimators have results for 1 subunit and 2 subunits. The inset figure in performance is the enlarged view for large goodness-of-fit values. The right figure is the speed result showing that MLE-based methods require exponentially increasing running time when increasing the training size, but our moment-based ones have quite consistent speed.

Fig. 2a shows the true w and k and the estimations from MELE and smoothMELE (MELE with smoothing prior). From now on, we use smoothing prior by default. 5.3 Simulations To illustrate the performance of our moment-based estimators, we generated Gaussian stimuli from an LNP neuron with exponentiated-quadratic nonlinearity and 1 subunit model with 8-element filter k and 33-element pooling weights w. Mean firing rate is 0.91 spk/s. In our estimation, each time bin stimulus with 40 dimensions is treated as one sample to generate spike response. Fig. 2 b and c show the speed and accuracy performance of three estimators LS, MELE and MLE (with smoothing prior). LS and MELE are comparable with baseline MLE in terms of accuracy but are exponentially faster. Although LNP with exponential nonlinearity has been widely adapted in neuroscience for its simplicity, the actual nonlinearity of neural systems is often sub-exponential, such as soft-rectifier nonlinearity. But exponential is favored as a convenient approximation of soft-rectifier within a small regime around the origin. Also generally, LNP neuron leans towards sigmoid subunit nonlinearity rather than quadratic. Quadratic could well approximate a sigmoid within a small nonlinear regime before the linear regime of the sigmoid. Therefore, in order to check the generalization performance of LS and MELE on mismatch models, we stimulated data from a neuron with sigmoid subunit nonlinearity or soft-rectifier output nonlinearity as shown in Fig. 2c. All the full MLEs formulated with no model mismatch provide a baseline for inspecting the performance of the ELL methods. Despite the model-mismatch, our estimators (LS and MELE) are on par with MLE when the subunit nonlinearity is quadratic, but the performance is notably worse for the sigmoid nonlinearity. Even so, in real applications, we will explore fits with different subunit nonlinearities using full MLE, where the exponential and quadratic assumption is thus primarily useful for a reasonable and extremely fast initializer. Moreover, the running time for moment-based estimators is always exponentially faster. 5.4 Application to neural data In order to show the predictive performance more comprehensively in real neural dataset, we applied LS, MELE and MLE estimators to data from a population of 57 V1 simple and complex cells (data published in [11]). The stimulus consisted of oriented binary white noise ("flickering bar") aligned with the cell's preferred orientation. The size of receptive field was chosen to be # of bars d 10 time bins, yielding a 10d-dimensional stimulus space. The time bin size is 10 ms and the number of bars (d) is 16 in our experiment. We compared moment-based estimators and MLE with smoothed low-rank expected GQM and smoothed low-rank GQM [7, 8]. Models are trained on stimuli with size varying from 6.25  103 to 105 and tested on 5  104 samples. Each subunit filter has a length of 5. All hyper parameters are chosen by cross validation. Fig. 3 shows that GQM is weakly better than LS but its running time is far more than LS (data not shown). Both MELE and MLE (but not LS) outfight GQM and
7

a) subunit #1
b) STA V1
responses

subunit #1
0
-0.1
-0.2 0 10 20
excitatory STC filters

subunit #2

subunit #2
0.2 0.1
00 10 20
suppressive STC filters

subunit model
Figure 4: Estimating visual receptive fields from a complex cell (544l029.p21). a) k and w by fitting smoothMELE(#2). Subunit #1 is suppressive (negative w) and #2 is excitatory (positive w). Form the y-axis we can tell from w that both imply that middle subunits contribute more than the ends. b) Qualitative analysis. Each image corresponds to a normalized 24 dimensions spatial pixels (horizontal) by 10 time bins (vertical) filter. Top row: STA/STC from true data; Bottom row: simulated response from 2-subunit MELE model given true stimuli and applied the same subspace analysis.

expected GQM with both 1 subunit and 2 subunits. Especially the improvement is the greatest with 1 subunit, which results from the average over all simple and complex cells. Generally, the more "complex" the cell is, the higher probability that multiple subunits would fit better. Outstandingly, MELE outperforms others with best goodness-of-fit and flat speed curve. The goodness-of-fit is defined to be the log-likelihood on the test set divided by spike count. For qualitative analysis, we ran smoothMELE(#2) for a complex cell and learned the optimal subunit filters and pooling weights (Fig. 4a), and then simulated V1 response by fitting 2-subunit MELE generative model given the optimal parameters. STA/STC analysis is applied to both neural data and simulated V1 response data. The quality of the filters trained on 105 stimuli are qualitatively close to that obtained by STA/STC (Fig. 4b). Subunit models can recover STA, the first six excitatory STC filters and the last four suppressive ones but with a considerably parsimonious parameter space.
6 Conclusion
We proposed an asymptotically efficient estimator for quadratic convolutional subunit models, which forges an important theoretical link between spike-triggered covariance analysis and nonlinear subunit models. We have shown that the proposed method works well even when the assumptions about model specification (nonlinearity and input distribution) were violated. Our approach reduces the difficulty of fitting subunit models because computational cost does not depend on dataset size (beyond the cost of a single pass through the data to compute the spike-triggered moments). We also proved conditions for identifiability of the convolutional decomposition, which reveals that for most cases the parameters are indeed identifiable. We applied our estimators to the neural data from macaque primary visual cortex, and showed that they outperform a highly regularized form of the GQM and achieve similar performance to the subunit model MLE at substantially lower computational cost.
References
[1] R. R. de Ruyter van Steveninck and W. Bialek. Real-time performance of a movement-senstivive neuron in the blowfly visual system: coding and information transmission in short spike sequences. Proc. R. Soc. Lond. B, 234:379-414, 1988.
8

[2] J. Touryan, B. Lau, and Y. Dan. Isolation of relevant visual features from random stimuli for cortical complex cells. Journal of Neuroscience, 22:10811-10818, 2002.
[3] B. Aguera y Arcas and A. L. Fairhall. What causes a neuron to spike? Neural Computation, 15(8):1789- 1807, 2003.
[4] Tatyana Sharpee, Nicole C. Rust, and William Bialek. Analyzing neural responses to natural signals: maximally informative dimensions. Neural Comput, 16(2):223-250, Feb 2004.
[5] O. Schwartz, J. W. Pillow, N. C. Rust, and E. P. Simoncelli. Spike-triggered neural characterization. Journal of Vision, 6(4):484-507, 7 2006.
[6] J. W. Pillow and E. P. Simoncelli. Dimensionality reduction in neural models: An information-theoretic generalization of spike-triggered average and covariance analysis. Journal of Vision, 6(4):414-428, 4 2006.
[7] Il Memming Park and Jonathan W. Pillow. Bayesian spike-triggered covariance analysis. In J. ShaweTaylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 1692-1700, 2011.
[8] Il M. Park, Evan W. Archer, Nicholas Priebe, and Jonathan W. Pillow. Spectral methods for neural characterization using generalized quadratic models. In Advances in Neural Information Processing Systems 26, pages 2454-2462, 2013.
[9] Ross S. Williamson, Maneesh Sahani, and Jonathan W. Pillow. The equivalence of information-theoretic and likelihood-based methods for neural dimensionality reduction. PLoS Comput Biol, 11(4):e1004141, 04 2015.
[10] Kanaka Rajan, Olivier Marre, and Gasper Tkacik. Learning quadratic receptive fields from neural responses to natural stimuli. Neural Computation, 25(7):1661-1692, 2013/06/19 2013.
[11] Nicole C. Rust, Odelia Schwartz, J. Anthony Movshon, and Eero P. Simoncelli. Spatiotemporal elements of macaque v1 receptive fields. Neuron, 46(6):945-956, Jun 2005.
[12] B Vintch, A Zaharia, J A Movshon, and E P Simoncelli. Efficient and direct estimation of a neural subunit model for sensory coding. In Adv. Neural Information Processing Systems (NIPS*12), volume 25, Cambridge, MA, 2012. MIT Press. To be presented at Neural Information Processing Systems 25, Dec 2012.
[13] Brett Vintch, Andrew Zaharia, J Movshon, and Eero P Simoncelli. A convolutional subunit model for neuronal responses in macaque v1. J. Neursoci, page in press, 2015.
[14] HB Barlow and W Ro Levick. The mechanism of directionally selective units in rabbit's retina. The Journal of physiology, 178(3):477, 1965.
[15] S. Hochstein and R. Shapley. Linear and nonlinear spatial subunits in y cat retinal ganglion cells. J. Physiol., 262:265-284, 1976.
[16] Jonathan B Demb, Kareem Zaghloul, Loren Haarsma, and Peter Sterling. Bipolar cells contribute to nonlinear spatial summation in the brisk-transient (y) ganglion cell in mammalian retina. The Journal of neuroscience, 21(19):7447-7454, 2001.
[17] Joanna D Crook, Beth B Peterson, Orin S Packer, Farrel R Robinson, John B Troy, and Dennis M Dacey. Y-cell receptive field and collicular projection of parasol ganglion cells in macaque monkey retina. The Journal of neuroscience, 28(44):11277-11291, 2008.
[18] PX Joris, CE Schreiner, and A Rees. Neural processing of amplitude-modulated sounds. Physiological reviews, 84(2):541-577, 2004.
[19] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193-202, 1980.
[20] T. Serre, L. Wolf, S. Bileschi, M. Riesenhuber, and T. Poggio. Robust object recognition with cortex-like mechanisms. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 29(3):411-426, 2007.
[21] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
[22] AlexandroD. Ramirez and Liam Paninski. Fast inference in generalized linear models via expected loglikelihoods. Journal of Computational Neuroscience, pages 1-20, 2013.
[23] Philip J Davis. Circulant matrices. American Mathematical Soc., 1979. [24] M. Sahani and J. Linden. Evidence optimization techniques for estimating stimulus-response functions.
NIPS, 15, 2003.
9

