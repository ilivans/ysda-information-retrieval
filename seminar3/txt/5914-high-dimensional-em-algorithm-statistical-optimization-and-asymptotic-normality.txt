High Dimensional EM Algorithm: Statistical Optimization and Asymptotic Normality

Zhaoran Wang

Quanquan Gu

Yang Ning

Han Liu

Princeton University University of Virginia Princeton University Princeton University

Abstract
We provide a general theory of the expectation-maximization (EM) algorithm for inferring high dimensional latent variable models. In particular, we make two contributions: (i) For parameter estimation, we propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure into parameter estimation. With an appropriate initialization, this algorithm converges at a geometric rate and attains an estimator with the (near-)optimal statistical rate of convergence. (ii) Based on the obtained estimator, we propose a new inferential procedure for testing hypotheses for low dimensional components of high dimensional parameters. For a broad family of statistical models, our framework establishes the first computationally feasible approach for optimal estimation and asymptotic inference in high dimensions.
1 Introduction
The expectation-maximization (EM) algorithm [12] is the most popular approach for calculating the maximum likelihood estimator of latent variable models. Nevertheless, due to the nonconcavity of the likelihood function of latent variable models, the EM algorithm generally only converges to a local maximum rather than the global one [30]. On the other hand, existing statistical guarantees for latent variable models are only established for global optima [3]. Therefore, there exists a gap between computation and statistics. Significant progress has been made toward closing the gap between the local maximum attained by the EM algorithm and the maximum likelihood estimator [2, 18, 25, 30]. In particular, [30] first establish general sufficient conditions for the convergence of the EM algorithm. [25] further improve this result by viewing the EM algorithm as a proximal point method applied to the Kullback-Leibler divergence. See [18] for a detailed survey. More recently, [2] establish the first result that characterizes explicit statistical and computational rates of convergence for the EM algorithm. They prove that, given a suitable initialization, the EM algorithm converges at a geometric rate to a local maximum close to the maximum likelihood estimator. All these results are established in the low dimensional regime where the dimension d is much smaller than the sample size n. In high dimensional regimes where the dimension d is much larger than the sample size n, there exists no theoretical guarantee for the EM algorithm. In fact, when d n, the maximum likelihood estimator is in general not well defined, unless the models are carefully regularized by sparsity-type assumptions. Furthermore, even if a regularized maximum likelihood estimator can be obtained in a computationally tractable manner, establishing the corresponding statistical properties, especially asymptotic normality, can still be challenging because of the existence of high dimensional nuisance parameters. To address such a challenge, we develop a general inferential theory of the EM algorithm for parameter estimation and uncertainty assessment of high dimensional latent variable models. In particular, we make two contributions in this paper: * For high dimensional parameter estimation, we propose a novel high dimensional EM algorithm by
attaching a truncation step to the expectation step (E-step) and maximization step (M-step). Such a Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910, NSF IIS1546482-BIGDATA, NSF DMS1454377-CAREER, NIH R01GM083084, NIH R01HG06841, NIH R01MH102339, and FDA HHSF223201000072C.
1

truncation step effectively enforces the sparsity of the attained estimator and allows us to establish significantly improved statistical rate of convergence. * Based upon the estimator attained by the high dimensional EM algorithm, we propose a decorrelated score statistic for testing hypotheses related to low dimensional components of the high dimensional parameter.

Under a unified analytic framework, we establish simultaneous statistical and computational guarantees for the proposed high dimensional EM algorithm and the respective uncertainty assessment

procedure. Let  solution sequence

2 of

Rd the

be the true parameter, high dimensional EM

s be its sparsity algorithm with T

level and being the

tota(tl)nuTt=m0bbere

the iterative of iterations.

In particular, we prove that:

* Given an appropriate initialization init with relative error upper bounded by a constant  2 (0, 1),

i.e., init



2/k

k2



,

the

iterative

solution

sequence p

(t)

T t=0

satisfies

(t)  2  | 1 {*zt/}2 + | 2 * s{z* log d/n}

Optimization Error Statistical Error: Optimal Rate

(1.1)

with high probability. Here  2 (0, 1), and 1, 2 are quantities that possibly depend on ,  and

t,

. As the optimization error term in the overall estimation error achieves

(t1h.e1)pdsecr*elaosgesd/tonzsetarotisatticaalgeraotme eotfricconravteergweinthcere(usppetcot

to an

extra factor of log n), which is (near-)minimax-optimal. See Theorem 3.4 for details.

* The proposed decorrelated score statistic is asymptotically normal. Moreover, its limiting variance

is optimal in the sense that it attains the semiparametric information bound for the low dimensional

components of interest in the presence of high dimensional nuisance parameters. See Theorem 4.6

for details.

Our framework allows two implementations of the M-step: the exact maximization versus approximate maximization. The former one calculates the maximizer exactly, while the latter one conducts an approximate maximization through a gradient ascent step. Our framework is quite general. We illustrate its effectiveness by applying it to two high dimensional latent variable models, that is, Gaussian mixture model and mixture of regression model.

Comparison with Related Work: A closely related work is by [2], which considers the low dimen-

sional regime where d that the EM algorithm

cisonmvuercghessmatalalegretohmanetnri.cUrantdeetrocseormtaeinloinciatliaolpiztiamtiounmctohnadt iattitoanins,stthheeypprdo/vne

statistical rate of convergence. They cover both maximization and gradient ascent implementations of

the M-step, and establish the consequences for the two latent variable models considered in our paper

under low dimensional settings. Our framework adopts their view of treating the EM algorithm as

a perturbed version of gradient methods. However, to handle the challenge of high dimensionality,

the key ingredient of our framework is the truncation step that enforces the sparsity structure along

the solution path. Such a truncation operation poses significant challenges for both computational

and statistical analysis. In detail, for computational analysis we need to carefully characterize the

evolution of each intermediate solution's support and its effects on the evolution of the entire iterative

solution sequence. For statistical analysis, we need to establish a fine-grained characterization of the

eerarnrtteoryrowefmicsopenlosvtyeaertdgisebtniycca[el2, e]w.rrIhonirch,hiwgihshimdcihumcieshntsseihcoahnrnapilecrraeltglhyiamnmetosh,reewircehpnaeldele/dnntgorinaetgsetatwhbalhinsehnjutdhsteepstnasb.lIi*nslhoaigdndgdi/ttinhoenstta2ot-inpstooicrinmatl

estimation, we further construct hypothesis tests for latent variable models in the high dimensional

regime, which have not been established before.

High dimensionality poses significant challenges for assessing the uncertainty (e.g., testing hypotheses) of the constructed estimators. For example, [15] show that the limiting distribution of the Lasso estimator is not Gaussian even in the low dimensional regime. A variety of approaches have been proposed to correct the Lasso estimator to attain asymptotic normality, including the debiasing method [13], the desparsification methods [26, 32] as well as instrumental variable-based methods [4]. Meanwhile, [16, 17, 24] propose the post-selection procedures for exact inference. In addition, several authors propose methods based on data splitting [20, 29], stability selection [19] and 2-confidence sets [22]. However, these approaches mainly focus on generalized linear models rather than latent variable models. In addition, their results heavily rely on the fact that the estimator is a global optimum of a convex program. In comparison, our approach applies to a much broader family of statistical models with latent structures. For these latent variable models, it is computationally infeasible to

2

obtain the global maximum of the penalized likelihood due to the nonconcavity of the likelihood

function. Unlike existing approaches, our inferential theory is developed for the estimator attained

by the proposed high dimensional EM algorithm, which is not necessarily a global optimum to any

optimization formulation.

Another line of research for the estimation of latent variable models is the tensor method, which

exploits the structures of third or higher order moments. See [1] and the references therein. However,

existing tensor methods primarily focus on the low dimensional regime where d  n. In addition, since the high order sample moments generally have a slow statistical rate of convergence, the

weFsohtriimcehxaatiosmrspsulobebo, tp[a9tii]nmeeasdtlabcbyolmitshhpeathtreeendpsworditm6h/etnhthesotpadtsidsu/tisncuaamlllriyantihemaovafxecaloonswvueebrrogbpeotniumcneadfl.osSrtiammtiisixltaitcurlaryel,

rate even for d  n. of regression model,
in high dimensional

settings, the statistical rates of convergence attained by tensor methods are significantly slower than

the statistical rate obtained in this paper.

The latent variable models considered in this paper have been well studied. Nevertheless, only a

few works establish theoretical guarantees for the EM algorithm. In particular, for Gaussian mixture

model, [10, 11] establish parameter estimation guarantees for the EM algorithm and its extensions. For

mixture of regression model, [31] establish exact parameter recovery guarantees for the EM algorithm

under a noiseless setting. For high dimensional mixture of regression model, [23] analyze the gradient

EM algorithm for the 1-penalized log-likelihood. They establish support recovery guarantees for the attained local optimum but have no parameter estimation guarantees. In comparison with existing

works, this paper establishes a general inferential framework for simultaneous parameter estimation

and uncertainty assessment based on a novel high dimensional EM algorithm. Our analysis provides

the first theoretical guarantee of parameter estimation and asymptotic inference in high dimensional

regimes for the EM algorithm and its applications to a broad family of latent variable models.

Notation: The matrix (p, q)-norm, i.e., k * kp,q, is obtained by taking the p-norm of each row and then taking the q-norm of the obtained row norms. We use C, C0, . . . to denote generic constants. Their values may vary from line to line. We will introduce more notations in 2.2.

2 Methodology

We first introduce the high dimensional EM Algorithm and then the respective inferential procedure. As examples, we consider their applications to Gaussian mixture model and mixture of regression model. For compactness, we defer the details to A of the appendix. More models are included in the longer version of this paper.

Algorithm 1 High Dimensional EM Algorithm

1: Parameter: Sparsity Parameter sb, Maximum Number of Iterations T

2: Initialization: Sbinit supp init, sb , (0) trunc init, Sbinit

supp(*, *) and trunc(*, *) are defined in (2.2) and (2.3)

3: For t = 0 to T 1

4: E-step: Evaluate Qn ; (t)

5: M-step: (t+0.5) Mn (t)

Mn(*) is implemented as in Algorithm 2 or 3

6: T-step: Sb(t+0.5) supp (t+0.5), sb , (t+1) trunc (t+0.5), Sb(t+0.5)

7: End For

8: Output: b

(T )

Algorithm 2 Maximization Implementation of the M-step

1: Input: (t), Qn ; (t)

Output: Mn (t)

argmax Qn ; (t)

Algorithm 3 Gradient Ascent Implementation of the M-step

1: Input: (t), Qn ; (t)

Parameter: Stepsize  > 0

2: Output: Mn (t)

(t) +  * rQn (t); (t)

2.1 High Dimensional EM Algorithm

Before we introduce the proposed high dimensional EM Algorithm (Algorithm 1), we briefly review

the classical EM algorithm. Let h (y) be the probability density function of Y 2 Y, where 2 Rd is

the model parameter. For over an unobserved latent

latent variable models, variable Z 2 Z, i.e., h

w(ye )as=suRmZeftha(tyh, z)(ydz) .isLoetbktain(zed|

by y)

marginalizing be the density

3

of Z conditioning on the observed variable Y = y, i.e., k (z | y) = f (y, z)/h (y). We define

Qn(

;

0)

=

1 n

Xn Z
i=1 Z

k

0 (z | yi) * log f

(yi, z) dz.

(2.1)

See B of the appendix for a detailed derivation. At the t-th iteration of the classical EM algorithm, we

evaluate Qn ; (t) at the E-step and then perform max Qn ; (t) at the M-step. The proposed high dimensional EM algorithm (Algorithm 1) is built upon the E-step and M-step (lines 4 and 5)

of the classical EM algorithm. In addition to the exact maximization implementation of the M-step

(Algorithm 2), we allow the gradient ascent implementation of the M-step (Algorithm 3), which

performs an approximate maximization via a gradient ascent step. To handle the challenge of high

dimensionality, in line 6 of Algorithm 1 we perform a truncation step (T-step) to enforce the sparsity

structure. In detail, we define

supp( , s): The set of index j's corresponding to the top s largest | j|'s.

Also,

for

an

index

set

S



{1, . . ., d}, we trunc(

define 
,S) j

the =

trunc(*, *) function j * 1{j 2 S}.

in

line

6

as

(2.2) (2.3)

Note that (t+0.5) is the output of the M-step (line 5) at the t-th iteration of the high dimensional EM algorithm. To obtain (t+1), the T-step (line 6) preserves the entries of (t+0.5) with the top sb large magnitudes and sets the rest to zero. Here sb is a tuning parameter that controls the sparsity level (line 1). By iteratively performing the E-step, M-step and T-step, the high dimensional EM algorithm attains an sb-sparse estimator b = (T ) (line 8). Here T is the total number of iterations.

2.2 Asymptotic Inference

Notation: Let r1Q( ; 0) be the gradient with respect to and r2Q( ; 0) be the gradient with respect to 0. If there is no confusion, we simply denote rQ( ; 0) = r1Q( ; 0) as in the previous sections. We define the higher order derivatives in the same manner, e.g., r21,2Q( ; 0) is calculated

by first taking derivative with respect to and then with respect to 0. For =

1>,

> 2

>

2 Rd with

1 2 Rd1 , 2 2 Rd2 and d1 + d2 = d, we use notations such as v 1 2 Rd1 and A 1, 2 2 Rd1d2 to denote the corresponding subvector of v 2 Rd and the submatrix of A 2 Rdd.

We aim to conduct asymptotic inference for low dimensional components of the high dimensional

para=meter

,

. (

W)>ith>ou, twlohsesreofgen2eraRlitiys,

we the

consider entry of

a single entry of interest, while

. In particular, we assume  2 Rd 1 is treated as the

nuisance parameter. In the following, we construct a high dimensional score test named decorrelated

score test. It is worth noting that, our method and theory can be easily generalized to perform statistical

inference for an arbitrary low dimensional subvector of .

Decorrelated Score Test: For score test, we are primarily interested in testing H0 :  = 0, since this null hypothesis characterizes the uncertainty in variable selection. Our method easily generalizes to H0 :  = 0 with 0 6= 0. For notational simplicity, we define the following key quantity

Tn( ) = r21,1Qn( ; ) + r21,2Qn( ; ) 2 Rdd.

(2.4)

Let

=

,

>

>. We Sn(

d,efi)n=e thre d1eQcno(rre;late)dscorwe (fun, ct)io>n*Srn(1*,Q*)n

2 (

R ;

as  )

.

(2.5)

Here

w( w(

,

,

)

2 Rd 1 is obtained
) = argmin kwk1,
w2Rd 1

using the following

subject to

 Tn(

Dantzig  ) ,

selector [8]  Tn( ) ,

*w

1

,

(2.6)

where > 0 is a tuning parameter. Let b = b, b> >, where b is the estimator attained by the high

dimensional EM algorithm (Algorithm 1). We define the decorrelated score statistic as

p n * Sn

b0,

 Tn

b0


|

1/2,

(2.7)

where

b0 =

0, b>

>,

and

 Tn

b0


|

 = 1,

w b0,

> * Tn

b0

 * 1,

w b0,

>>.

Here we use b0 instead of b since we are interested in the null hypothesis H0 :  = 0. We can also replace b0 with b and the theoretical results will remain the same. In 4 we will prove the proposed decorrelated score statistic in (2.7) is asymptotically N (0, 1). Consequently, the decorrelated score

4

test with significance level 2 (0, 1) takes the form

S( ) = 1

p n * Sn

b0,

 Tn

b0


|

1/2 2/ 

1(1

/2),

1(1

 /2) ,

where 1(*) is the inverse function of the Gaussian cumulative distribution function. If S( ) = 1,

we reject the null hypothesis H0 :  = 0. The intuition of this decorrelated score test is explained

in D of the appendix. The key theoretical observation is Theorem 2.1, which connects r1Qn(*; *)

in (2.5) and Tn(*) in (2.7) with the score function and Fisher information in the presence of latent

structures. I( ) =

Let E

nr( 2)nb(e

the)long-,lwikheleirheoEod.I(t*s)sicsotrheefeuxnpceticotnatiisonrunnd(er)thanedmtohdeeFliwshiethr

information parameter

is .

Theorem 2.1. r1Qn( ;

For the true ) = rn(

parameter )/n, and

EandaTnny(

2)

Rd =

,

it holds I( )

that =E

 r2n(



 )

n.

(2.8)

Proof. See I.1 of the appendix for a detailed proof.

Based on the decorrelated score test, it is easy to establish the decorrelated Wald test, which allows us to construct confidence intervals. For compactness we defer it to the longer version of this paper.

3 Theory of Computation and Estimation

Before we present the main results, we introduce three technical conditions, which will significantly ease our presentation. They will be verified for specific latent variable models in E of the appendix. The first two conditions, proposed by [2], characterize the properties of the population version lower bound function Q(*; *), i.e., the expectation of Qn(*; *) defined in (2.1). We define the respective population version M-step as follows. For the M-step in Algorithm 2, we define

M ( ) = argmax Q( 0; ).
0

(3.1)

For the M-step in Algorithm 3, we define

M ( ) = +  * r1Q( ; ),

(3.2)

where  > 0 is the stepsize in Algorithm 3. We use B to denote the basin of attraction, i.e., the local

region where the high dimensional EM algorithm enjoys desired guarantees.

Condition 3.1. We define two versions of this condition.

*

Lipschitz-Gradient-1( 1,B). For r1Q M ( );

thetruerp1aQramMet(er

);



and
2

any 

1

*

2 k

B,

we have k2,

(3.3)

where M (*) is the population version M-step (maximization implementation) defined in (3.1).

* Lipschitz-Gradient-2( 2, B). For the true parameter  and any 2 B, we have

r1Q( ; ) r1Q( ; ) 2  2 * k

k2.

(3.4)

Condition 3.1 defines a variant of Lipschitz continuity for r1Q(*; *). In the sequel, we will use (3.3) and (3.4) in the analysis of the two implementations of the M-step respectively.

Condition 3.2 Concavity-Smoothness(, , B). For any 1, 2 2 B, Q(*; ) is -smooth, i.e.,

Q( 1; ) Q( 2; ) + ( 1 and -strongly concave, i.e.,

2)> * r1Q( 2; ) /2 * k 2

1k22,

(3.5)

Q( 1; )  Q( 2; ) + ( 1 2)> * r1Q( 2; ) /2 * k 2 1k22.

(3.6)

This condition indicates that, when the second variable of Q(*; *) is fixed to be , the function is

`sandwiched' between two quadratic functions. The third condition characterizes the statistical error

between the sample version and population version M-steps, i.e., Mn(*) defined in Algorithms 2 and 3, and M (*) in (3.1) and (3.2). Recall k * k0 denotes the total number of nonzero entries in a vector.

Condition 3.3 Statistical-Error(, , s, n, B). For any fixed 2 B with k k0  s, we have that

holds with probability at least 1

M ( ) Mn( ) 1  

(3.7)

. Here  > 0 possibly depends on , sparsity level s, sample size

n, dimension d, as well as the basin of attraction B.

In (3.7) the statistical error  quantifies the 1-norm of the difference between the population version and sample version M-steps. Particularly, we constrain the input of M (*) and Mn(*) to be s-sparse. Such a condition is different from the one used by [2]. In detail, they quantify the statistical error

5

with the 2-norm and do not constrain the input of M (*) and Mn(*) to be sparse. Consequently, our

subsequent statistical characterizes the more

analysis is different from theirs. refined entrywise statistical error,

The reason we use which converges at

athfeast1ra-tneoormf pislothgadt,/nit

(eprroosrsicbolnyvwerigtheseaxttraasflaocwtorrastedeopfepnddi/nng,ownhsipcehcdifioecsmnoodt edlesc)r.eIansecotomzpearroisaosnn, thinecre2a-nseosrmwisthtadtisticanl.

Furthermore, the fine-grained entrywise statistical error is crucial to our key proof for quantifying the

effects of the truncation step (line 6 of Algorithm 1) on the iterative solution sequence.

3.1 Main Results

To simplify the technical analysis of the high dimensional EM algorithm, we focus on its resampling

version, which is illustrated in Algorithm 4 in C of the appendix.

Theorem 3.4. We define B = : k

k2  R , where R =  * k k2 for some  2 (0, 1).

We assume Condition Concavity-Smoothness(, , B) holds and init * For the maximization implementation of the M-step (Algorithm 2),

we

sup2posRe t/h2a.t

Condition

Lipschitz-Gradient-1( 1, B)

sb = C * max

p sb

+

C

0

p /1

16/(1p/1  * s

holds with 1 := 1/ 2
1)2, 4 * (1 + )2/(1 *   min (1 p1)2 *

(0, 1) )2 *
R, (1

and s ,
)2

/[2

*

(1

+

)]

*

k

k2

.

(3.8) (3.9)

Here C 1 and C0 > 0 are constants. Under Condition Statistical-Error(, /T, sb, n/T, B) we

have that, for t = 1, . . . , T , (t)  2  |t1/{2z* R}

+

p sb

+

C

0/p1

|

p  * s /(1
{z

p1)

*

 }

(3.10)

Optimization Error

Statistical Error

holds with probability at least 1 , where C0 is the same constant as in (3.9).

* For the gradient ascent implementation of the M-step (Algorithm 3), we suppose that Condition Lipschitz-Gradient-2( 2, B) holds with 2 := 1 2 * ( 2)/( + ) 2 (0, 1) and the stepsize in Algorithm 3 is set to  = 2/( + ). Meanwhile, we assume (3.8) and (3.9) hold with 1 replaced by 2. Under Condition Statistical-Error(, /T, sb, n/T, B) we have that, for t = 1, . . . , T , (3.10) holds with probability at least 1 , in which 1 is replaced with 2.

Proof. See G.1 of the appendix for a detailed proof.

The assumption in (3.8) states that the sparsity parameter sb is chosen to be sufficiently large and also

of the same order as the true sparsity level s. This assumption ensures that the error incurred by the

truncation step can be upper bounded. In addition, as is shown for specific latent variable models in

sapaEsmspopfs*leth.sieTsizhasepuenrpffieeficnnoidcereirnxe,t,altyhstheessema. esBasrlurylo.mtrThptheeteiraosmsensuainmsisn(pu3tmC.i9oop)nntsidiouningt(sigo3eg.ns8ut)Sas,rttaahpnteitsesbsteia+cmtahClpa-ltE0e/trhpsreioz1ree(nnt,iries/*iTstpue,frssbfia,tcniive/iensTtsol,oyfBlutl)hatiredogensecasrsmeeuqaecsuhoeerstndhcaeaesrt

remains within the basin of attraction B in the presence of statistical error.

Theorem 3.4 illustrates that, the upper bound of the overall estimation error can be decomposed

into two terms. The first term is the upper bound of optimization error, which decreases to zero at a

geometric rate of convergence, because we have boorduenrdaosfpstsati,stthiciaslteerrrmor,iswphriocphodrotieosnnaol ttodeppesnd*

1, on

2 < 1. t. Since

Mpesban+wChi0l/ep, t1he

, where  is the entrywise

seco* pndstermis

is the upper of the same

statistical error between

Mrmoou(dg*)ehlla.y)noTdfhMtehrneef(oo*r)rd.ee,IrnthpeElsotoagftidtsh/tienc.aalp(epTrehrnoedrreitxemrwmayeispberrooevuxegtrhtahlyafato,cftfoothrrseeaoatrctadhcehsrpepdecstiofic*

latent variable model,  is depending on each specific
log d/n. Consequently, for

a sufficiently large t = T such that same order, the final estimator b =

the
(T

)oaptttiaminiszaati(onneaarn-)dopsttaimtisatlicpalser*rolorgtedr/mns

in (3.10) (possibly

are of the with extra

factors) statistical rate. For compactness, we give the following example and defer the details to E.

Implications for Gaussian Mixture Model: We assume y1, . . . , yn are the n i.i.d. realizations of Y = Z *  + V . Here Z is a Rademacher random variable, i.e., P(Z = +1) = P(Z = 1) = 1/2, and V  N (0, 2 * Id) is independent of Z, where is the standard deviation. Suppose that we have k k2/ r, where r > 0 is a sufficiently large constant that denotes the minimum signal-to-noise ratio. In E of the appendix we prove that there exists some constant C > 0 such that Conditions

6

Lipschitz-Gradient-1( 1, B) and Concavity-Smoothness(, , B) hold with

1 = exp C * r2 ,  =  = 1, B = : k

k2  R with R =  * k k2,  = 1/4.

For a sufficiently large n, we have that Condition Statistical-Error(, , s, n, B) holds with

 = C * k k1 +

q  * log d + log(2/ ) n.

Then large

the first part of Theorem T , which is near-optimal

3.4 implies with respect

b to

the

 2C minimax

p l*owesrb*oluongddp* lsog

n/n for log d/n.

a

sufficiently

4 Theory of Inference

To simplify the presentation of the unified framework, we lay out several technical conditions, which will be verified for each model. Let EM, G, T and L be four quantities that scale with s, d and n. These conditions will be verified for specific latent variable models in F of the appendix.

Condition 4.1 Parameter-Estimation EM . We have b Condition 4.2 Gradient-Statistical-Error G . We have

 1 = OP EM .

r1Qn( ; ) Condition 4.3 Tn(*)-Concentration T .

r1Q( We have

; ) 1 Tn( )

=EOPTnG(

.



 )

1,1 = OP

T

.

Condition 4.4 Tn(*)-Lipschitz L . For any , we have

Tn( ) Tn( ) 1,1 = OP L * k

k1.

In the sequel, we lay out an assumption on several population quantities and the sample size n. Recall

that  = parameter.

[, ( By the

no)t>at]i>o,nws ihner2e.2,I2(

R


is )

the
,

entry 2 R(d

o1f)in(tdere1s)ta, nwdhiIle(

)2

Rd , 2

1 is R(d

the nuisance 1)1 denote

the We

submatrices of the Fisher information matrix I( )

w

=

 I(

define

 1 I(



 )



 )

1 ,

*

and

Id(I()),

2 as

Rd the

1, sw = largest and

2 Rdd. We define w, sw and Sw kwk0, and Sw = supp(w). smallest eigenvalues of I( ), and

as

(4.1)

 I(



 ) |

 = I(



 ) ,

 I(

)>,

*

 I

(



 )

1 ,

 * I(



 )

,

2 R.

(4.2)

According to (4.1) and (4.2), we can easily verify that

 I(



 ) |

 = 1,

The following assumption ensures that

Also, according to (4.3) and the fact that

(w)> * I(

)

*

 1,

(w)>>.

 d I( d I(

))

> >

0. Hence, 0, we have

 I ( I(

)),|

(4.3)
in (4.1) is invertible. > 0.

Assumption 4.5 . We impose the following assumptions.

*

For positive constants

max

 1 I(



 )

mdaxIa(nd)min,

we assume 
min, I(



 ) |

= O(1),

 I(

)|1

= O(1).

(4.4)

* The tuning parameter of the Dantzig selector in (2.6) is set to

= C * T + L * EM * 1 + kwk1 ,

(4.5)

where C 1 is a sufficiently large constant. The sample size n is sufficiently large such that

max kwk1, 1 * sw *

= o(1),

EM = o(1),

sw *

*

G

=

p o(1/ n),

(4.6)

*

 EM

=

p o(1/ n),

max 1, kwk1

* L *

 EM

2

=

p o(1/ n).

The assumption on

 d I(

other assumptions in (4.4)



 )

guarantees

that

the

Fisher

information

guarantee the existence of the asymptotic

matrix is variance

positive definite.

of

p n

*

Sn

b0,

The in

the score statistic defined in (2.7). Similar assumptions are standard in existing asymptotic inference

results. For example, for mixture of regression model, [14] impose variants of these assumptions.

For specific models, we will show that EM, G, T and all decrease with n, while L increases

with n at a slow rate. Therefore, the assumptions in (4.6) ensure that the sample size n is sufficiently

large. We will make these assumptions more explicit after we specify EM, G, T and L for each

7

model. Note the assumptions in (4.6) imply that sw = kwk0 needs to be small. For instance, for

specified in (4.5), we will prove that

max T is

kwk1, 1 of the order

*psw * log

= o(1) in (4.6) implies sw *  d/n. Hence, we require that sw

T= =o

op(1). In the n/ log d

following,  d 1,

ita.hsees.u,dmwepfitni2oitniRosndugog1feiwsstsspiaInr(s(e4..1)S)u,c,whealihesaspvaweristihtIiy(natshse)usmp,apnti*oownf acaf=enwbceIo(ulunmd)enrsst,oofo.dTI(ahserfeo)flolor,ew,.ss.SuAucchchcaoarssdppiaanrrgssiitttoyy assumption on w is necessary, because otherwise it is difficult to accurately estimate w in high

dimensional regimes. In the context of high dimensional generalized linear models, [26, 32] impose

similar sparsity assumptions.

4.1 Main Results

Decorrelated Score Test: The next theorem establishes the asymptotic normality of the decorrelated

score statistic defined in (2.7). Theorem 4.6. We consider  = , ( )>> with  = 0. Under Assumption 4.5 and Conditions

4.1-4.4, we have that for n ! 1,

p n * Sn

b0,

 Tn

b0


|

1/2 D! N (0, 1),

(4.7)

where

b0

and

 Tn

function pn * Sn

b0 b0,


|
is

2  I

R (

are defined in



 ) |

, which

(2.7). The is defined

limiting in (4.2).

variance

of

the

decorrelated

score

Proof. See G.2 of the appendix for a detailed proof. Optimality: [27] prove that for inferring  in the presence of nuisance parameter

,

 I

(



 ) |

is

the semiparametric efficient information, i.e., the minimum limiting variance of the (rescaled) score

function. Our proposed decorrelated score function achieves such a semiparametric information lower

bound and is therefore in this sense optimal.

In the following, we use Gaussian mixture model to illustrate the effectiveness of Theorem 4.6. We

defer the details and the implications for mixture of regression to F of the appendix.

Implications for Gaussian Mixture Model: Under the same model considered in 3.1, if we assume

alElMqu=anstitpiesloegxcde*pltosgwn,/sn,,

d and G =

pn are log

dco/nns,taTnt,=thpenlowgedh/anveanthdat

Conditions 4.1-4.4 L = log d + log n

hold with 3/2. Thus,

under holds

Assumption if max sw,

4.5, (4.7) holds when s 2 * (s)2 * (log d)5

n=!on1/.(lAoglsno,)2we.

can

verify

that

(4.6)

in

Assumption

4.5

5 Conclusion

We propose a novel high dimensional EM algorithm which naturally incorporates sparsity structure. Our theory shows that, with a suitable initialization, the proposed algorithm converges at a geometric rate and achieves an estimator with the (near-)optimal statistical rate of convergence. Beyond point estimation, we further propose the decorrelated score and Wald statistics for testing hypotheses and constructing confidence intervals for low dimensional components of high dimensional parameters. We apply the proposed algorithmic framework to a broad family of high dimensional latent variable models. For these models, our framework establishes the first computationally feasible approach for optimal parameter estimation and asymptotic inference under high dimensional settings.

References

[1] A N A N D K U M A R , A ., G E , R ., H S U , D ., K A K A D E , S . M . and T E L G A R S K Y, M . (2014). Tensor decompositions for learning latent variable models. Journal of Machine Learning Research 15 2773-2832.
[2] B A L A K R I S H N A N , S ., WA I N W R I G H T, M . J . and Y U , B . (2014). Statistical guarantees for the EM algorithm: From population to sample-based analysis. arXiv preprint arXiv:1408.2156 .

[3] B A R T H O L O M E W, D . J ., K N O T T, M . and M O U S TA K I , I . (2011). Latent variable models and factor analysis: A unified approach, vol. 899. Wiley.

[4] B E L L O N I , A ., C H E N , D ., C H E R N O Z H U K O V, V. and H A N S E N , C . (2012). Sparse models and methods for optimal instruments with an application to eminent domain. Econometrica 80 2369-2429.
[5] B I C K E L , P. J ., R I T O V, Y. and T S Y B A K O V, A . B . (2009). Simultaneous analysis of Lasso and Dantzig selector. Annals of Statistics 37 1705-1732.

8

[6] B O U C H E R O N , S ., L U G O S I , G . and M A S S A R T, P. (2013). Concentration inequalities: A nonasymptotic theory of independence. Oxford University Press.
[7] C A I , T., L I U , W. and L U O , X . (2011). A constrained 1 minimization approach to sparse precision matrix estimation. Journal of the American Statistical Association 106 594-607.
[8] C A N D E S , E . and TA O , T. (2007). The Dantzig selector: Statistical estimation when p is much larger than n. Annals of Statistics 35 2313-2351.
[9] C H A G A N T Y, A . T. and L I A N G , P. (2013). Spectral experts for estimating mixtures of linear regressions. arXiv preprint arXiv:1306.3729 .
[10] C H A U D H U R I , K ., D A S G U P TA , S . and VAT TA N I , A . (2009). Learning mixtures of Gaussians using the k-means algorithm. arXiv preprint arXiv:0912.0086 .
[11] D A S G U P TA , S . and S C H U L M A N , L . (2007). A probabilistic analysis of EM for mixtures of separated, spherical Gaussians. Journal of Machine Learning Research 8 203-226.
[12] D E M P S T E R , A . P., L A I R D , N . M . and R U B I N , D . B . (1977). Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B (Statistical Methodology) 39 1-38.
[13] J AVA N M A R D , A . and M O N TA N A R I , A . (2014). Confidence intervals and hypothesis testing for high-dimensional regression. Journal of Machine Learning Research 15 2869-2909.
[14] K H A L I L I , A . and C H E N , J . (2007). Variables selection in finite mixture of regression models. Journal of the American Statistical Association 102 1025-1038.
[15] K N I G H T, K . and F U , W. (2000). Asymptotics for Lasso-type estimators. Annals of Statistics 28 1356-1378.
[16] L E E , J . D ., S U N , D . L ., S U N , Y. and TAY L O R , J . E . (2013). Exact inference after model selection via the Lasso. arXiv preprint arXiv:1311.6238 .
[17] L O C K H A R T, R ., TAY L O R , J ., T I B S H I R A N I , R . J . and T I B S H I R A N I , R . (2014). A significance test for the Lasso. Annals of Statistics 42 413-468.
[18] M C L A C H L A N , G . and K R I S H N A N , T. (2007). The EM algorithm and extensions, vol. 382. Wiley. [19] M E I N S H A U S E N , N . and B U H L M A N N , P. (2010). Stability selection. Journal of the Royal Statistical
Society: Series B (Statistical Methodology) 72 417-473. [20] M E I N S H A U S E N , N ., M E I E R , L . and B U H L M A N N , P. (2009). p-values for high-dimensional
regression. Journal of the American Statistical Association 104 1671-1681. [21] N E S T E R O V, Y. (2004). Introductory lectures on convex optimization:A basic course, vol. 87. Springer. [22] N I C K L , R . and VA N D E G E E R , S . (2013). Confidence sets in sparse regression. Annals of Statistics
41 2852-2876. [23] S T A D L E R , N ., B U H L M A N N , P. and VA N D E G E E R , S . (2010). 1-penalization for mixture
regression models. TEST 19 209-256. [24] TAY L O R , J ., L O C K H A R T, R ., T I B S H I R A N I , R . J . and T I B S H I R A N I , R . (2014). Post-selection
adaptive inference for least angle regression and the Lasso. arXiv preprint arXiv:1401.3889 . [25] T S E N G , P. (2004). An analysis of the EM algorithm and entropy-like proximal point methods. Mathe-
matics of Operations Research 29 27-44. [26] VA N D E G E E R , S ., B U H L M A N N , P., R I T O V, Y. and D E Z E U R E , R . (2014). On asymptotically
optimal confidence regions and tests for high-dimensional models. Annals of Statistics 42 1166-1202. [27] VA N D E R VA A R T, A . W. (2000). Asymptotic statistics, vol. 3. Cambridge University Press. [28] V E R S H Y N I N , R . (2010). Introduction to the non-asymptotic analysis of random matrices. arXiv
preprint arXiv:1011.3027 . [29] WA S S E R M A N , L . and R O E D E R , K . (2009). High-dimensional variable selection. Annals of Statistics
37 2178-2201. [30] W U , C . F. J . (1983). On the convergence properties of the EM algorithm. Annals of Statistics 11
95-103. [31] Y I , X ., C A R A M A N I S , C . and S A N G H AV I , S . (2013). Alternating minimization for mixed linear
regression. arXiv preprint arXiv:1310.3745 . [32] Z H A N G , C . - H . and Z H A N G , S . S . (2014). Confidence intervals for low dimensional parameters in
high dimensional linear models. Journal of the Royal Statistical Society: Series B (Statistical Methodology) 76 217-242.
9

