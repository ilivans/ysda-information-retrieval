Estimating Jaccard Index with Missing Observations: A Matrix Calibration Approach
Wenye Li Macao Polytechnic Institute
Macao SAR, China wyli@ipm.edu.mo
Abstract
The Jaccard index is a standard statistics for comparing the pairwise similarity between data samples. This paper investigates the problem of estimating a Jaccard index matrix when there are missing observations in data samples. Starting from a Jaccard index matrix approximated from the incomplete data, our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints, through a simple alternating projection algorithm. Compared with conventional approaches that estimate the similarity matrix based on the imputed data, our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the Frobenius norm than the un-calibrated matrix (except in special cases they are identical). We carried out a series of empirical experiments and the results confirmed our theoretical justification. The evaluation also reported significantly improved results in real learning tasks on benchmark datasets.
1 Introduction
A critical task in data analysis is to determine how similar two data samples are. The applications arise in many science and engineering disciplines. For example, in statistical and computing sciences, similarity analysis lays a foundation for cluster analysis, pattern classification, image analysis and recommender systems [15, 8, 17].
A variety of similarity models have been established for different types of data. When data samples can be represented as algebraic vectors, popular choices include cosine similarity model, linear kernel model, and so on [24, 25]. When each vector element takes a value of zero or one, the Jaccard index model is routinely applied, which measures the similarity by the ratio of the number of unique elements common to two samples against the total number of unique elements in either of them [14, 23].
Despite the wide applications, the Jaccard index model faces a non-trivial challenge when data samples are not fully observed. As a treatment, imputation approaches may be applied, which replace the missing observations with substituted values and then calculate the Jaccard index based on the imputed data. Unfortunately, with a large portion of missing observations, imputing data samples often becomes un-reliable or even infeasible, as evidenced in our evaluation.
Instead of trying to fill in the missing values, this paper investigates a completely different approach based on matrix calibration. Starting from an approximate Jaccard index matrix that is estimated from incomplete samples, the proposed method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints. The calibration procedure is carried out with a simple yet flexible alternating projection algorithm.
1

The proposed method has a strong theoretical advantage. The calibrated matrix is guaranteed to be better than, or at least identical to (in special cases), the un-calibrated matrix in terms of a shorter Frobenius distance to the true Jaccard index matrix, which was verified empirically as well. Besides, our evaluation of the method also reported improved results in learning applications, and the improvement was especially significant with a high portion of missing values.
A note on notation. Throughout the discussion, a data sample, Ai (1  i  n), is treated as a set of features. Let F = {f1, * * * , fd} be the set of all possible features. Without causing ambiguity, Ai also represents a binary-valued vector. If the j-th (1  j  d) element of vector Ai is one, it means fj  Ai (feature fj belongs to sample Ai); if the element is zero, fj  Ai; if the element is marked as missing, it remains unknown whether feature fj belongs to sample Ai or not.

2 Background

2.1 The Jaccard index

The Jaccard index is a commonly used statistical indicator for measuring the pairwise similarity [14, 23]. For two nonempty and finite sets Ai and Aj, it is defined to be the ratio of the number of elements in their intersection against the number of elements in their union:

Jij

=

|Ai |Ai

 

Aj | Aj |

where |*| denotes the cardinality of a set.

The Jaccard index has a value of 0 when the two sets have no elements in common, 1 when they have exactly the same elements, and strictly between 0 and 1 otherwise. The two sets are more similar (have more common elements) when the value gets closer to 1.

For n Jij

sets
n

A1, * . The

i,j=1

* * , An matrix

(n  2), the Jaccard index is symmetric and all diagonal

matrix is elements

defined as an n of the matrix are

x 1.

n

matrix

J

=

2.2 Handling missing observations
When data samples are fully observed, the accurate Jaccard index can be obtained trivially by enumerating the intersection and the union between each pair of samples if both the number of samples and the number of features are small. For samples with a large number of features, the index can often be approximated by MinHash and related methods [5, 18], which avoid the explicit counting of the intersection and the union of the two sets.
When data samples are not fully observed, however, obtaining the accurate Jaccard index generally becomes infeasible. One naive approximation is to ignore the features with missing values. Only those features that have no missing values in all samples are used to calculate the Jaccard index. Obviously, for a large dataset with missing-at-random features, it is very likely that this method will throw away all features and therefore does not work at all.
The mainstream work tries to replace the missing observations with substituted values, and then calculates the Jaccard index based on the imputed data. Several simple approaches, including zero, median and k-nearest neighbors (kNN) methods, are popularly used. A missing element is set to zero, often implying the corresponding feature does not exist in a sample. It can also be set to the median value (or the mean value) of the feature over all samples, or sometimes over a number of nearest neighboring instances.
A more systematical imputation framework is based on the classical expectation maximization (EM) algorithm [6], which generalizes maximum likelihood estimation to the case of incomplete data. Assuming the existence of un-observed latent variables, the algorithm alternates between the expectation step and the maximization step, and finds maximum likelihood or maximum a posterior estimates of the un-observed variables. In practice, the imputation is often carried out through iterating between learning a mixture of clusters of the filled data and re-filling missing values using cluster means, weighted by the posterior probability that a cluster generates the samples [11].

2

3 Solution

Our work investigates the Jaccard index matrix estimation problem for incomplete data. Instead of throwing away the un-observed features or imputing the missing values, a completely different solution based on matrix calibration is designed.

3.1 Initial approximation

For a sample Ai, denote by Oi+ the set of features that are known to be in Ai, and denote by Oi- the set of features that are known to be not in Ai. Let Oi = Oi+  Oi-. If Oi = F , Ai is fully observed without missing values; otherwise, Ai is not fully observed with missing values. The complement of Oi with respect to F , denoted by Oi, gives Ai's unknown features and missing values.

For two samples Ai and Aj with missing values, we approximate their Jaccard index by:

Ji0j =

Oi+  Oj  Oj+  Oi Oi+  Oj  Oj+  Oi

=

Oi+  Oj+ Oi+  Oj  Oj+  Oi

Here we assume that each sample has at least one observed feature. It is obvious that Ji0j is equal to the ground truth Jij if the samples are fully observed.

There exists an interval [ij, ij] that the true value Jij lies in, where

1,  ij = |Oi+Oj+| ,  Oi-Oj-

if i = j otherwise

and

1, if i = j



ij

=

Oi- Oj-
 | | ,OiOj Oi+Oj+

. otherwise

The lower bound ij is obtained from the extreme case of setting the missing values in a way that the
two sets have the fewest features in their intersection while having the most features in their union.
On the contrary, the upper bound ij is obtained from the other extreme. When the samples are fully observed, the interval shrinks to a single point ij = ij = Jij.

3.2 Matrix calibration

Denote by J =

Jij

n ij=1

the

true

Jaccard

index

matrix

for

a

set

of

data

samples

{A1,

*

**

, An},

we have [2]:

Theorem 1. For a given set of data samples, its Jaccard index matrix J is positive semi-definite.

For data samples with missing values, the matrix J0 =

Ji0j

n ij=1

often loses positive semi-

definiteness. Nevertheless, it can be calibrated to ensure the property by seeking an n x n matrix

J = {Jij}nij=1 to minimize:

L0 (J ) =

J - J0

2 F

subject to the constraints:

J 0, and, ij  Jij  ij (1  i, j  n)

where J 0 requires J to be positive semi-definite and * F denotes the Frobenius norm of a

matrix and

J

2 F

=

ij Ji2j .

Let Mn be the set of n x n symmetric matrices. The feasible region defined by the constraints, denoted by R, is a nonempty closed and convex subset of Mn. Following standard results in optimization theory [20, 3, 10], the problem of minimizing L0 (J) is convex. Denote by PR the
projection onto R. Its unique solution is given by the projection of J0 onto R: JR0 = PR J0 .

For JR0 , we have:

3

Theorem 2.

J  - JR0

2 F



J - J0

2 F

.

The

equality

holds

iff

J0



R,

i.e.,

J0

=

JR0 .

Proof. Define an inner product on Mn that induces the Frobenius norm:

X, Y = trace XT Y , for X, Y  Mn.

Then

J - J0

2 F

=

J  - JR0 - J 0 - JR0

2 F

=

J  - JR0

2 F

+

J 0 - JR0

2 F

-

2

J  - JR0 , J 0 - JR0



J  - JR0

2 F

-2

J  - JR0 , J 0 - JR0



J  - JR0

2 F

The second "" holds due to the Kolmogrov's criterion, which states that the projection of J0 onto R, JR0 , is unique and characterized by:

JR0  R, and J - JR0 , J 0 - JR0  0 for all J  R.

The equality holds iff

J 0 - JR0

2 F

= 0 and

J  - JR0 , J 0 - JR0

= 0, i.e., J 0 = JR0 .

This key observation shows that projecting J0 onto the feasible region R will produce an improved estimate towards J, although this ground truth matrix remains unknown to us.

3.3 Projection onto subsets

Based on the results in Section 3.2, we are to seek a minimizer to L0 (J) to improve the estimate J0. Define two nonempty closed and convex subsets of Mn:
S = {X|X  Mn, X 0}
and T = {X|X  Mn, ij  Xij  ij (1  i, j  n)} .

Obviously R = S  T . Now our minimization problem becomes finding the projection of J0 onto the intersection of two sets S and T with respect to the Frobenius norm. This can be done by studying the projection onto the two sets individually. Denote by PS the projection onto S, and PT the projection onto T . For projection onto T , a straightforward result based on the Kolmogrov's criterion is:
Theorem 3. For a given matrix X  Mn, its projection onto T , XT = PT (X), is given by

Xij , (XT )ij = ij ,
ij ,

if ij  Xij  ij

if Xij < ij

.

if Xij > ij

For projection onto S, a well known result is the following [12, 16, 13]:

Theorem 4. For X  Mn and its singular value decomposition X = U V T where  = diag (1, * * * , n), the projection of X onto S is given by: XS = PS (X) = U V T where  = diag (1, * * * , n) and

i =

i, 0,

if i  0 . otherwise

The matrix XS = PS (X) gives the positive semi-definite matrix that most closely approximates X with respect to the Frobenius norm.

4

3.4 Dykstra's algorithm

To study the orthogonal projection onto the intersection of subspaces, a classical result is von Neumann's alternating projection algorithm. Let H be a Hilbert space with two closed subspaces C1 and C2. The orthogonal projection onto the intersection C1  C2 can be obtained by the product of the two projections PC1 PC2 when the two projections commute (PC1 PC2 = PC2 PC1 ). When they do not commute, the work shows that for each x0  H, the projection of x0 onto the intersection
can be obtained by the limit point of a sequence of projections onto each subspace respectively: limk (PC2 PC1 )k x0 = PC1C2 x0 . The algorithm generalizes to any finite number of subspaces and projections onto them.

Unfortunately, different from the application in [19], in our problem both S and T are not subspaces but subsets, and von Neumann's convergence result does not apply. The limit point of the generated sequence may converge to non-optimal points.

To handle the difficulty, Dykstra extended von Neumann's work and proposed an algorithm that

works with subsets [9]. Consider the case of C =

r i=1

Ci

where

C

is

nonempty

and

each

Ci

is

a closed and convex subset in H. Assume that for any x  H, obtaining PC (x) is hard, while

obtaining each PCi (x) is easy. Starting from x0  H, Dykstra's algorithm produces two sequences,

the iterates xki and the increments Iik . The two sequences are generated by:

xk0 = xrk-1 xki = PCi xki-1 - Iik-1 Iik = xki - xki-1 - Iik-1

where i = 1, * * * , r and k = 1, 2, * * * . The initial values are given by x0r = x0, Ii0 = 0. The sequence of xki converges to the optimal solution with a theoretical guarantee [9, 10].
Theorem 5. Let C1, * * * , Cr be closed and convex subsets of a Hilbert space H such that C =
r
Ck = . For any i = 1, * * * , r and any x0  H, the sequence xki converges strongly to
k=1
x0C = PC x0 (i.e. xki - x0C  0 as k  ).
The convergent rate of Dykstra's algorithm for polyhedral sets is linear [7], which coincides with the convergence rate of von Neumann's alternating projection method.

3.5 An iterative method
Based on the discussion in Section 3.4, we have a simple approach, shown in Algorithm 1, that finds the projection of an initial matrix J0 onto the nonempty set R = S  T . Here the projections onto S and T are given by the two theorems in Section 3.3. The algorithm stops when Jk falls into the feasible region or when a maximal number of iterations is achieved. For practical implementation, a more robust stopping criterion can be adopted [1].

3.6 Related work
It is a known study in mathematical optimization field to find a positive semi-definite matrix that is closest to a given matrix. A number of methods have been proposed recently. The idea of alternating projection method was firstly applied in a financial application [13]. The problem can also be phrased as a semi-definite programming (SDP) model [13] and be solved via the interior-point method. In the work of [21] and [4], the quasi-Newton method and the projected gradient method to the Lagrangian dual of the original problem were applied, which reported faster results than the SDP formulation. An even faster Newton's method was developed in [22] by investigating the dual problem, which is unconstrained with a twice continuously differentiable objective function and has a quadratically convergent solution.
5

Algorithm 1 Projection onto R = S  T Require: Initial matrix J0
k=0 JT0 = J 0 IS0 = 0 IT0 = 0 while NOT CONVERGENT do
JSk+1 = PS JTk - ISk ISk+1 = JSk+1 - JTk - ISk JTk+1 = PT JSk+1 - ITk ITk+1 = JTk+1 - JSk+1 - ITk k = k+1 end while return J k = JTk
4 Evaluation
To evaluate the performance of the proposed method, four benchmark datasets were used in our experiments.
* MNIST: a grayscale image database of handwritten digits ("0" to "9"). After binarization, each image is represented as a 784-dimensional 0-1 vector.
* USPS: another grayscale image database of handwritten digits. After binarization, each image is represented as a 256-dimensional 0-1 vector.
* PROTEIN: a bioinformatics database with three classes of instances. Each instance is represented as a sparse 357-dimensional 0-1 vector.
* WEBSPAM: a dataset with both spam and non-spam web pages. Each page is represented as a 0-1 vector. The data are highly sparse. On average one vector has about 4, 000 non-zero values out of more than 16 million features.
Our experiments have two objectives. One is to verify the effectiveness of the proposed method in estimating the Jaccard index matrix by measuring the derivation of the calibrated matrix from the ground truth in Frobenius norm. The other is to evaluate the performance of the calibrated matrix in general learning applications. The comparison is made against the popular imputation approaches listed in Section 2.2, including the zero, kNN and EM 1 approaches. (As the median approach gave very similar performance as the zero approach, its results were not reported separately.)
4.1 Jaccard index matrix estimation
The experiment was carried out under various settings. For each dataset, we experimented with 1, 000 and 10, 000 samples respectively. For each sample, different portions (from 10% to 90%) of feature values were marked as missing, which was assumed to be "missing at random" and all features had the same probability of being marked.
As mentioned in Section 3, for the proposed calibration approach, an initial Jaccard index matrix was firstly built based on the incomplete data. Then the matrix was calibrated to meet the positive semi-definite requirement and the lower and upper bounds requirement. While for the imputation approaches, the Jaccard index matrix was calculated directly from the imputed data.
Note that for the kNN approach, we iterated different k from 1 to 5 and the best result was collected, which actually overestimated its performance. Under some settings, the results of the EM approach were not available due to its prohibitive computational requirement to our platform.
The results are presented through the comparison of mean square deviations from the ground truth of the Jaccard index matrix J. For an n x n estimated matrix J, its mean square deviation from
1ftp://ftp.cs.toronto.edu/pub/zoubin/old/EMcode.tar.Z
6

Mean Square Deviation (log-scale)

Mean Square Deviation (1,000 Samples)

.FBO4RVBSF%FWJBUJPO (1,000 Samples)

Mean Square Deviation (1,000 Samples)

.FBO4RVBSF%FWJBUJPO (1,000 Samples)

ZERO/MEDIAN

ZERO/MEDIAN

ZERO/MEDIAN

ZERO/MEDIAN

10-1 kNN 10-1 kNN 10-1 kNN EM EM EM

kNN NO_CALIBRATION

NO_CALIBRATION CALIBRATION

NO_CALIBRATION CALIBRATION

NO_CALIBRATION CALIBRATION

10-2

CALIBRATION

10-2 10-2 10-2 10-3

0HDQ6TXDUH'HYLDWLRQ (log-scale)

Mean Square Deviation (log-scale)

0HDQ6TXDUH'HYLDWLRQ (log-scale)

10-3 10-3 10-3 10-4

10-4 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Ratio of Observed Features

1

(a) MNIST

10-4 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Ratio of Observed Features

1

(b) USPS

10-4 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Ratio of Observed Features

1

(c) PROTEIN

10-5
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Ratio of Observed Features
(d) WEBSPAM

Mean Square Deviation (10,000 Samples)
ZERO/MEDIAN 10-1 kNN
EM NO_CALIBRATION CALIBRATION

.FBO4RVBSF%FWJBUJPO (10,000 Samples)
ZERO/MEDIAN 10-1 kNN
EM NO_CALIBRATION CALIBRATION

Mean Square Deviation (10,000 Samples)
ZERO/MEDIAN 10-1 kNN
NO_CALIBRATION CALIBRATION

.FBO4RVBSF%FWJBUJPO (10,000 Samples)
ZERO/MEDIAN kNN NO_CALIBRATION 10-2 CALIBRATION

10-2 10-2 10-2 10-3

0HDQ6TXDUH'HYLDWLRQ (log-scale)

Mean Square Deviation (log-scale)

0HDQ6TXDUH'HYLDWLRQ (log-scale)

10-3 10-3 10-3 10-4

10-4 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Ratio of Observed Features

1

(e) MNIST

10-4 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Ratio of Observed Features

1

(f) USPS

10-4 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Ratio of Observed Features

1

(g) PROTEIN

10-5
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Ratio of Observed Features
(h) WEBSPAM

Mean Square Deviation (log-scale)

Figure 1: Mean square deviations from the ground truth on benchmark datasets by different methods.
Horizontal: percentages of observed values (from 10% to 90%); Vertical: mean square deviations in log-scale. (a)-(d): 1, 000 samples; (e)-(f): 10, 000 samples. (For better visualization effect of the results shown in color, the reader is referred to the soft copy of this paper.)

J is defined as the square Frobenius distance between the two matrices, divided by the number of

elements, i.e.,

( )n
ij=1

Jij -Jij

2

n2

.

In

addition

to

the

comparison

with

the

popular

approaches,

the

mean

square deviation between the un-calibrated matrix J0 and J, shown as NO CALIBRATION, is

also reported as a baseline.

Figure 1 shows the results. It can be seen that the calibrated matrices reported the smallest derivation from the ground truth in nearly all experiments. The improvement is especially significant when the ratio of observed features is low (the missing ratio is high). It is guaranteed to be no worse than the un-calibrated matrix. As evidenced in the results, for all the imputation approaches, there is no such a guarantee.

4.2 Supervised learning
Knowing the improved results in reducing the deviation from the ground truth matrix, we would like to further investigate whether this improvement indeed benefits practical applications, specifically in supervised learning.
We applied the calibrated results in nearest neighbor classification tasks. Given a training set of labeled samples, we tried to predict the labels of the samples in the testing set. For each testing sample, its label was determined by the label of the sample in the training set that had the largest Jaccard index value with it.
Similarly the experiment was carried out with 1, 000/10, 000 samples and different portions of missing values from 10% to 90% respectively. In each run, 90% of the samples were randomly chosen as the training set and the remaining 10% were used as the testing set. The mean and standard deviation of the classification errors in 1, 000 runs were reported. As a reference, the results from the ground truth matrix J, shown as FULLY OBSERVED, were also included.
Figure 2 shows the results. Again the matrix calibration method reported evidently improved results over the imputation approaches in most experiments. The improvement verified the benefits brought by the reduced deviation from the true Jaccard index matrix, and therefore justified the usefulness of the proposed method in learning applications.
7

Classification Error

Classification Error (1,000 Samples)
1 FULLY_OBSERVED ZERO/MEDIAN
0.9 kNN EM
0.8 NO_CALIBRATION CALIBRATION
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Ratio of Observed Features
(a) MNIST

Classification Error Classification Error

Classification Error (1,000 Samples)
1 FULLY_OBSERVED ZERO/MEDIAN
0.9 kNN EM
0.8 NO_CALIBRATION CALIBRATION
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Ratio of Observed Features
(b) USPS

Classification Error (1,000 Samples)
0.8 FULLY_OBSERVED ZERO/MEDIAN kNN
0.75 EM NO_CALIBRATION CALIBRATION
0.7
0.65
0.6
0.55
0.5 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Ratio of Observed Features
(c) PROTEIN

Classification Error

Classification Error (1,000 Samples)
0.11
FULLY_OBSERVED
ZERO/MEDIAN 0.1 kNN
NO_CALIBRATION
0.09 CALIBRATION

0.08

0.07

0.06

0.05

0.04

0.03

0.02

0.01 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Ratio of Observed Features

1

(d) WEBSPAM

Classification Error (10,000 Samples)
1 FULLY_OBSERVED ZERO/MEDIAN
0.9 kNN EM
0.8 NO_CALIBRATION CALIBRATION
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Ratio of Observed Features
(e) MNIST

Classification Error Classification Error

Classification Error (10,000 Samples)
1 FULLY_OBSERVED ZERO/MEDIAN
0.9 kNN EM
0.8 NO_CALIBRATION CALIBRATION
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Ratio of Observed Features
(f) USPS

Classification Error (10,000 Samples)
FULLY_OBSERVED ZERO/MEDIAN kNN 0.65 NO_CALIBRATION CALIBRATION
0.6
0.55
0.5
0.45
0.4 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Ratio of Observed Features
(g) PROTEIN

Classification Error

Classification Error (10,000 Samples)
0.07 FULLY_OBSERVED ZERO/MEDIAN kNN
0.06 NO_CALIBRATION CALIBRATION
0.05

0.04

0.03

0.02

0.01 0

0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 Ratio of Observed Features

1

(h) WEBSPAM

Classification Error

Figure 2: Classification errors on benchmark datasets by different methods. Horizontal: percentage of observed values (from 10% to 90%); Vertical: classification errors. (a)-(d): 1, 000 samples; (e)(f): 10, 000 samples. (For better visualization effect of the results shown in color, the reader is
referred to the soft copy of this paper.)

5 Discussion and conclusion
The Jaccard index measures the pairwise similarity between data samples, which is routinely used in real applications. Unfortunately in practice, it is non-trivial to estimate the Jaccard index matrix for incomplete data samples. This paper investigates the problem, and proposes a matrix calibration approach in a way that is completely different from the existing methods. Instead of throwing away the unknown features or imputing the missing values, the proposed approach calibrates any approximate Jaccard index matrix by ensuring the positive semi-definite requirement on the matrix. It is theoretically shown and empirically verified that the approach indeed brings about improvement in practical problems.
One point that is not particularly addressed in this paper is the computational complexity issue. We adopted a simple alternating projection procedure based on Dykstra's algorithm. The computational complexity of the algorithm heavily depends on the successive matrix decompositions. It is expensive when the size of the matrix becomes large. Calibrating a Jaccard index matrix for 1, 000 samples can be finished in seconds of time on our platform, while calibrating a matrix for 10, 000 samples quickly increases to more than an hour. Further investigations for faster solutions are thus necessary for scalability.
Actually, there is a simple divide-and-conquer heuristic to calibrate a large matrix. Firstly divide the matrix into small sub-matrices. Then calibrate each sub-matrix to meet the constraints. Finally merge the results. Although the heuristic may not give the optimal result, it also guarantees to produce a matrix better than or identical to the un-calibrated matrix. The heuristic runs with high parallel efficiency and easily scales to very large matrices. The detailed discussion is omitted here due to the space limit.
Acknowledgments
The work is supported by The Science and Technology Development Fund (Project No. 006/2014/A), Macao SAR, China.
8

References
[1] E.G. Birgin and M. Raydan. Robust stopping criteria for Dykstra's algorithm. SIAM Journal on Scientific Computing, 26(4):1405-1414, 2005.
[2] M. Bouchard, A.L. Jousselme, and P.E. Dore. A proof for the positive definiteness of the Jaccard index matrix. International Journal of Approximate Reasoning, 54(5):615-626, 2013.
[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, New York, NY, USA, 2004.
[4] S. Boyd and L. Xiao. Least-squares covariance matrix adjustment. SIAM Journal on Matrix Analysis and Applications, 27(2):532-546, 2005.
[5] A.Z. Broder, M. Charikar, A.M. Frieze, and M. Mitzenmacher. Min-wise independent permutations. In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, pages 327-336. ACM, 1998.
[6] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39:1-38, 1977.
[7] F. Deutsch. Best Approximation in Inner Product Spaces. Springer, New York, NY, USA, 2001. [8] R.O. Duda and P.E. Hart. Pattern Classification. John Wiley and Sons, Hoboken, NJ, USA, 2000. [9] R.L. Dykstra. An algorithm for restricted least squares regression. Journal of the American Statistical
Association, 78(384):837-842, 1983. [10] R. Escalante and M. Raydan. Alternating Projection Methods. SIAM, Philadelphia, PA, USA, 2011. [11] Z. Ghahramani and M.I. Jordan. Supervised learning from incomplete data via an EM approach. In
Advances in Neural Information Processing Systems, volume 6, pages 120-127. Morgan Kaufmann, 1994. [12] G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press, Baltimore, MD,
USA, 1996. [13] N.J. Higham. Computing the nearest correlation matrix - a problem from finance. IMA Journal of Nu-
merical Analysis, 22:329-343, 2002. [14] P. Jaccard. The distribution of the flora in the alpine zone. New Phytologist, 11(2):37-50, 1912. [15] A.K. Jain, M.N. Murty, and P.J. Flynn. Data clustering: A review. ACM Computing Surveys, 31(3):264-
323, 1999. [16] D.L. Knol and J.M.F. ten Berge. Least-squares approximation of an improper correlation matrix by a
proper one. Psychometrika, 54(1):53-61, 1989. [17] J. Leskovec, A. Rajaraman, and J. Ullman. Mining of Massive Datasets. Cambridge University Press,
New York, NY, USA, 2014. [18] P. Li and A.C. Konig. Theory and applications of b-bit minwise hashing. Communications of the ACM,
54(8):101-109, 2011. [19] W. Li, K.H. Lee, and K.S. Leung. Large-scale RLSC learning without agony. In Proceedings of the 24th
International Conference on Machine Learning, pages 529-536. ACM, 2007. [20] D.G. Luenberger. Optimization by Vector Space Methods. John Wiley & Sons, New York, NY, USA,
1969. [21] J. Malick. A dual approach to semidefinite least-squares problems. SIAM Journal on Matrix Analysis and
Applications, 26(1):272-284, 2004. [22] H. Qi and D. Sun. A quadratically convergent newton method for computing the nearest correlation
matrix. SIAM Journal on Matrix Analysis and Applications, 28(2):360-385, 2006. [23] D.J. Rogers and T.T. Tanimoto. A computer program for classifying plants. Science, 132(3434):1115-
1118, 1960. [24] G. Salton, A. Wong, and C.S. Yang. A vector space model for automatic indexing. Communications of
the ACM, 18(11):613-620, 1975. [25] B. Scholkopf and A.J. Smola. Learning With Kernels, Support Vector Machines, Regularization, Opti-
mization, and Beyond. The MIT Press, Cambridge, MA, USA, 2001.
9

